import os
import numpy as np
import matplotlib.pyplot as plt
from tqdm import tqdm
import multiprocessing as mp
from matplotlib.colors import LinearSegmentedColormap
from scipy import stats
from scipy.signal import medfilt
import time
import pickle
from functools import partial
import argparse
import math
from matplotlib import rcParams
from scipy.optimize import minimize
from scipy.stats import poisson
import sys
from itertools import product
import warnings
import csv
from glob import glob

# Import other modules from the project
from src.model_with_a_minimal_v3 import ThresholdDynamicsModel
from src.detection_utils import detect_jumps_improved
from src.model_v3_fixed import ThresholdDynamicsModelV3Fixed
from src.config import MAX_ITER, CONVERGENCE_TOL, BOUNDARY_FRACTION

# Set detailed logging output
import logging
logging.basicConfig(level=logging.INFO)


# Set English font
plt.rcParams['font.family'] = ['DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False



def solve_steady_state_for_r(model, r_m, threshold_params=None):
    """
    ä¸ºå•ä¸ªr_må€¼æ±‚è§£ç¨³æ€ï¼Œç”¨äºå¹¶è¡Œè®¡ç®—
    
    å‚æ•°:
        model: ThresholdDynamicsModelå®ä¾‹æˆ–å‚æ•°å­—å…¸
        r_m: ç§»é™¤æ¯”ä¾‹
        threshold_params: é˜ˆå€¼å‚æ•°(å¯é€‰ï¼Œå½“modelä¸ºå‚æ•°å­—å…¸æ—¶éœ€æä¾›)
        
    è¿”å›:
        åŒ…å«ç¨³æ€è§£çš„å­—å…¸
    """
    # å¦‚æœä¼ å…¥çš„æ˜¯å‚æ•°å­—å…¸è€Œä¸æ˜¯æ¨¡å‹å®ä¾‹ï¼Œåˆ™åˆ›å»ºæ¨¡å‹
    if not isinstance(model, ThresholdDynamicsModel):
        network_params = model
        model = ThresholdDynamicsModelV3Fixed(network_params, threshold_params)
    
    try:
        # æ±‚è§£è‡ªæ´½æ–¹ç¨‹
        result = model.solve_self_consistent(
            removal_ratios={'mainstream': r_m},
            max_iter=MAX_ITER,
            tol=CONVERGENCE_TOL
        )
        
        if not result['converged']:
            raise ValueError(f"ç¨³æ€åœ¨r_m={r_m}å¤„æœªæ”¶æ•›")
        
        # è¿”å›ç»“æœ(æ·»åŠ r_må€¼ä»¥ä¾¿åç»­å¤„ç†)
        result['r_m'] = r_m
        return result
    except Exception as e:
        # ä¸è¿”å›é»˜è®¤å€¼ï¼Œç›´æ¥æŠ›å‡ºå¼‚å¸¸
        raise ValueError(f"åœ¨r_m={r_m}å¤„æ±‚è§£ç¨³æ€å¤±è´¥: {str(e)}")

def _solve_steady_state_wrapper(params):
    """ç”¨äºå¤šè¿›ç¨‹çš„åŒ…è£…å‡½æ•°"""
    net_params, r, thresh_params = params
    return solve_steady_state_for_r(net_params, r, thresh_params)

def compute_single_point_r_combination(args):
    """
    è®¡ç®—å•ä¸ª(phi, theta, r)ç»„åˆçš„ç¨³æ€ï¼Œç”¨äºå±•å¹³å¹¶è¡Œè®¡ç®—
    
    å‚æ•°:
        args: åŒ…å«(phi, theta, r_m, network_params, base_threshold_params)çš„å…ƒç»„
        
    è¿”å›:
        åŒ…å«è®¡ç®—ç»“æœçš„å­—å…¸
    """
    phi, theta, r_m, network_params, base_threshold_params = args
    
    try:
        # æ„å»ºé˜ˆå€¼å‚æ•°
        if base_threshold_params is None:
            threshold_params = {
                'theta': theta,
                'phi': phi
            }
        else:
            threshold_params = base_threshold_params.copy()
            threshold_params['theta'] = theta
            threshold_params['phi'] = phi
        
        # åˆ›å»ºæ¨¡å‹
        model = ThresholdDynamicsModelV3Fixed(network_params, threshold_params)
        
        # æ±‚è§£è‡ªæ´½æ–¹ç¨‹
        result = model.solve_self_consistent(
            removal_ratios={'mainstream': r_m},
            max_iter=MAX_ITER,
            tol=CONVERGENCE_TOL
        )
        
        if not result['converged']:
            raise ValueError(f"ç¨³æ€åœ¨r_m={r_m}å¤„æœªæ”¶æ•›")
        
        # è¿”å›ç»“æœ
        return {
            'phi': phi,
            'theta': theta,
            'r_m': r_m,
            'success': True,
            'X_H': result['X_H'],
            'X_M': result['X_M'],
            'X_L': result['X_L'],
            'p_risk': result['p_risk'],
            'p_risk_m': result['p_risk_m'],
            'p_risk_w': result['p_risk_w']
        }
        
    except Exception as e:
        return {
            'phi': phi,
            'theta': theta,
            'r_m': r_m,
            'success': False,
            'error': str(e)
        }

def compute_single_point_r_combination_with_kappa(args):
    """
    è®¡ç®—å•ä¸ª(phi, theta, kappa, r)ç»„åˆçš„ç¨³æ€ï¼Œç”¨äºå±•å¹³å¹¶è¡Œè®¡ç®—
    
    å‚æ•°:
        args: åŒ…å«(phi, theta, r_m, kappa_network_params, base_threshold_params, kappa, init_states)çš„å…ƒç»„
        
    è¿”å›:
        åŒ…å«è®¡ç®—ç»“æœçš„å­—å…¸
    """
    # å…¼å®¹æ—§æ ¼å¼ï¼ˆä¸åŒ…å«init_statesï¼‰å’Œæ–°æ ¼å¼ï¼ˆåŒ…å«init_statesï¼‰
    if len(args) == 6:
        # æ—§æ ¼å¼ï¼šæ²¡æœ‰init_states
        phi, theta, r_m, kappa_network_params, base_threshold_params, kappa = args
        init_states = None
    elif len(args) == 7:
        # æ–°æ ¼å¼ï¼šåŒ…å«init_states
        phi, theta, r_m, kappa_network_params, base_threshold_params, kappa, init_states = args
    else:
        raise ValueError(f"Invalid argument format: expected 6 or 7 arguments, got {len(args)}")
    
    try:
        # æ„å»ºé˜ˆå€¼å‚æ•°
        if base_threshold_params is None:
            threshold_params = {
                'theta': theta,
                'phi': phi
            }
        else:
            threshold_params = base_threshold_params.copy()
            threshold_params['theta'] = theta
            threshold_params['phi'] = phi
        
        # åˆ›å»ºæ¨¡å‹
        model = ThresholdDynamicsModelV3Fixed(kappa_network_params, threshold_params)
        
        # å‡†å¤‡solve_self_consistentçš„å‚æ•°
        solve_params = {
            'removal_ratios': {'mainstream': r_m},
            'max_iter': MAX_ITER,
            'tol': CONVERGENCE_TOL
        }
        
        # å¦‚æœæä¾›äº†init_statesï¼Œåˆ™ä¼ é€’ç»™æ±‚è§£å™¨
        if init_states is not None:
            solve_params['init_states'] = init_states
        
        # æ±‚è§£è‡ªæ´½æ–¹ç¨‹
        result = model.solve_self_consistent(**solve_params)
        
        if not result['converged']:
            raise ValueError(f"ç¨³æ€åœ¨r_m={r_m}å¤„æœªæ”¶æ•›")
        
        # è¿”å›ç»“æœ
        return {
            'phi': phi,
            'theta': theta,
            'kappa': kappa,
            'r_m': r_m,
            'success': True,
            'X_H': result['X_H'],
            'X_M': result['X_M'],
            'X_L': result['X_L'],
            'p_risk': result['p_risk'],
            'p_risk_m': result['p_risk_m'],
            'p_risk_w': result['p_risk_w']
        }
        
    except Exception as e:
        return {
            'phi': phi,
            'theta': theta,
            'kappa': kappa,
            'r_m': r_m,
            'success': False,
            'error': str(e)
        }

def process_parameter_point_flattened(phi, theta, r_values, network_params, 
                                    base_threshold_params, save_dir, 
                                    abs_jump_threshold, removal_type, 
                                    all_results_dict, init_states=None):
    """
    å¤„ç†å•ä¸ªå‚æ•°ç‚¹ï¼Œä½¿ç”¨å±•å¹³çš„é¢„è®¡ç®—ç»“æœ
    
    å‚æ•°:
        phi, theta: å‚æ•°å€¼
        r_values: rå€¼æ•°ç»„
        network_params: ç½‘ç»œå‚æ•°
        base_threshold_params: é˜ˆå€¼å‚æ•°æ¨¡æ¿
        save_dir: ä¿å­˜ç›®å½•
        abs_jump_threshold: è·³å˜é˜ˆå€¼
        removal_type: ç§»é™¤ç±»å‹
        all_results_dict: é¢„è®¡ç®—çš„æ‰€æœ‰ç»“æœå­—å…¸ {(phi, theta, r): result}
        init_states: åˆå§‹çŠ¶æ€å­—å…¸ï¼Œé»˜è®¤None
        
    è¿”å›:
        å¤„ç†ç»“æœå­—å…¸
    """
    try:
        # è®¡ç®—kappaå€¼
        kappa = network_params.get('k_out_mainstream', 60) + network_params.get('k_out_wemedia', 60)
        
        # åˆ›å»ºå‚æ•°ç‚¹å¯¹åº”çš„ç›®å½•
        kappa_int = int(round(kappa))
        phi_int = int(round(phi * 100))
        theta_int = int(round(theta * 100))
        
        point_dir = os.path.join(save_dir, f"kappa{kappa_int:03d}_phi{phi_int:03d}_theta{theta_int:03d}")
        os.makedirs(point_dir, exist_ok=True)
        
        # æ„å»ºé˜ˆå€¼å‚æ•°
        if base_threshold_params is None:
            threshold_params = {
                'theta': theta,
                'phi': phi
            }
        else:
            threshold_params = base_threshold_params.copy()
            threshold_params['theta'] = theta
            threshold_params['phi'] = phi
        
        print(f"  1. Extracting steady states from precomputed results ({len(r_values)} points)...")
        
        # ä»é¢„è®¡ç®—ç»“æœä¸­æå–ç¨³æ€å€¼
        X_H_values = []
        X_M_values = []
        X_L_values = []
        p_risk_values = []
        p_risk_m_values = []
        p_risk_w_values = []
        
        failed_points = []
        
        # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ è°ƒè¯•ä¿¡æ¯å’Œå®¹é”™æœºåˆ¶
        print(f"  ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥all_results_dicté”®å€¼åŒ¹é…...")
        print(f"    ç›®æ ‡å‚æ•°ï¼šphi={phi}, theta={theta}, kappa={kappa}")
        print(f"    å­—å…¸ä¸­æ€»é”®æ•°ï¼š{len(all_results_dict)}")
        
        # æ˜¾ç¤ºå­—å…¸ä¸­çš„å‰å‡ ä¸ªé”®ä½œä¸ºæ ·æœ¬
        sample_keys = list(all_results_dict.keys())[:3]
        for i, sample_key in enumerate(sample_keys):
            print(f"    æ ·æœ¬é”®{i+1}: {sample_key}")
        
        matched_count = 0
        for r in r_values:
            # ğŸ”§ ä¿®å¤æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜ï¼šä½¿ç”¨æ›´å¼ºçš„å®¹é”™åŒ¹é…
            result = None
            
            # ç¬¬ä¸€æ­¥ï¼šå°è¯•ç›´æ¥é”®åŒ¹é…
            key = (phi, theta, kappa, r)
            if key in all_results_dict and all_results_dict[key]['success']:
                result = all_results_dict[key]
                matched_count += 1
            else:
                # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨å®¹é”™åŒ¹é…å¯»æ‰¾æœ€ä½³åŒ¹é…
                best_match = None
                best_r_diff = float('inf')
                
                for stored_key, stored_result in all_results_dict.items():
                    stored_phi, stored_theta, stored_kappa, stored_r = stored_key
                    if (abs(stored_phi - phi) < 1e-10 and 
                        abs(stored_theta - theta) < 1e-10 and 
                        abs(stored_kappa - kappa) < 1e-10 and 
                        stored_result['success']):
                        
                        r_diff = abs(stored_r - r)
                        if r_diff < best_r_diff:
                            best_r_diff = r_diff
                            best_match = stored_result
                
                # å¦‚æœæ‰¾åˆ°äº†è¶³å¤Ÿæ¥è¿‘çš„åŒ¹é…ï¼ˆå·®å¼‚å°äº1e-6ï¼‰
                if best_match is not None and best_r_diff < 1e-6:
                    result = best_match
                    matched_count += 1
                else:
                    # è°ƒè¯•è¾“å‡ºï¼šæ˜¾ç¤ºæœ€æ¥è¿‘çš„é”®
                    closest_r_diff = float('inf')
                    closest_key = None
                    for stored_key in all_results_dict.keys():
                        stored_phi, stored_theta, stored_kappa, stored_r = stored_key
                        if (abs(stored_phi - phi) < 1e-10 and 
                            abs(stored_theta - theta) < 1e-10 and 
                            abs(stored_kappa - kappa) < 1e-10):
                            r_diff = abs(stored_r - r)
                            if r_diff < closest_r_diff:
                                closest_r_diff = r_diff
                                closest_key = stored_key
                    
                    print(f"    âŒ æœªæ‰¾åˆ°r={r:.6f}çš„åŒ¹é…é”®")
                    if closest_key:
                        print(f"       æœ€æ¥è¿‘çš„é”®: {closest_key}, rå·®å¼‚={closest_r_diff:.2e}")
                    
                failed_points.append(r)
                # ä½¿ç”¨NaNå¡«å……ç¼ºå¤±å€¼
                X_H_values.append(np.nan)
                X_M_values.append(np.nan)
                X_L_values.append(np.nan)
                p_risk_values.append(np.nan)
                p_risk_m_values.append(np.nan)
                p_risk_w_values.append(np.nan)
                continue
            
            # æˆåŠŸæ‰¾åˆ°åŒ¹é…ç»“æœ
            X_H_values.append(result['X_H'])
            X_M_values.append(result['X_M'])
            X_L_values.append(result['X_L'])
            p_risk_values.append(result['p_risk'])
            p_risk_m_values.append(result['p_risk_m'])
            p_risk_w_values.append(result['p_risk_w'])
        
        print(f"  ğŸ“Š é”®å€¼åŒ¹é…ç»“æœï¼šæˆåŠŸåŒ¹é… {matched_count}/{len(r_values)} ä¸ªrå€¼")
        
        if failed_points:
            print(f"  âš ï¸ Warning: {len(failed_points)} r values failed to find matches")
            print(f"      Failed r values: {failed_points[:5]}..." if len(failed_points) > 5 else f"      Failed r values: {failed_points}")
        else:
            print(f"  âœ… æ‰€æœ‰rå€¼éƒ½æˆåŠŸåŒ¹é…åˆ°é¢„è®¡ç®—ç»“æœ")
        
        # åºå‚é‡è·³å˜åˆ†æ
        print(f"  2. Analyzing order parameter jumps...")
        
        # ç§»é™¤NaNå€¼è¿›è¡Œè·³å˜åˆ†æ
        valid_mask = ~np.isnan(X_H_values)
        if np.sum(valid_mask) < 5:
            return {
                'status': 'failed',
                'phi': phi,
                'theta': theta,
                'kappa': kappa,
                'error': 'Insufficient valid data points for analysis'
            }
        
        valid_r_values = np.array(r_values)[valid_mask]
        valid_X_H = np.array(X_H_values)[valid_mask]
        
        # ä½¿ç”¨æ”¹è¿›çš„è·³å˜æ£€æµ‹
        jumps, r_jumps, jump_detected, z_score = detect_jumps_improved(valid_r_values, valid_X_H)
        
        # æå–è·³å˜ä¿¡æ¯
        if jumps and r_jumps:
            max_jump_position = r_jumps[0]  # ç¬¬ä¸€ä¸ªï¼ˆæœ€å¤§çš„ï¼‰è·³å˜ä½ç½®
            max_jump_size = jumps[0]        # ç¬¬ä¸€ä¸ªï¼ˆæœ€å¤§çš„ï¼‰è·³å˜å¤§å°
        else:
            max_jump_position = None
            max_jump_size = 0.0
        
        if jump_detected:
            print(f"  Significant jump detected: r_c = {max_jump_position:.4f}, jump size = {max_jump_size:.4f}, z-score = {z_score:.2f}")
        elif max_jump_position is not None:
            print(f"  Jump detected but not significant: r_c = {max_jump_position:.4f}, jump size = {max_jump_size:.4f}, z-score = {z_score:.2f}")
        else:
            print(f"  No jump detected")
        
        # å…³è”é•¿åº¦è®¡ç®— - ç‰©ç†ä¸Šéœ€è¦æ‰°åŠ¨åˆ†æï¼Œå¿…é¡»é‡æ–°è®¡ç®—
        print(f"  3. Computing correlation length (requires perturbation analysis)...")
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = ThresholdDynamicsModelV3Fixed(network_params, threshold_params)
        
        try:
            # å¹¶è¡Œè®¡ç®—å…³è”é•¿åº¦ï¼ˆç‰©ç†ä¸Šéœ€è¦æ‰°åŠ¨åˆ†æï¼Œä¸èƒ½åŸºäºé¢„è®¡ç®—ç¨³æ€å€¼ï¼‰
            corr_result = model.calculate_correlation_length_robust(
                r_values, 
                removal_type=removal_type, 
                n_processes=max(1, mp.cpu_count() - 1),  # è‡ªåŠ¨é€‰æ‹©åˆç†çš„è¿›ç¨‹æ•°
                external_threshold_params=threshold_params,
                state_name='X_H',
                enable_diagnostics=False,
                enable_power_law=True,
                init_states=init_states  # ä¼ é€’åˆå§‹çŠ¶æ€
            )
            
            # å¤„ç†è¿”å›å€¼
            if len(corr_result) >= 4:
                corr_lengths, critical_r, raw_data, power_law_results = corr_result
                print(f"   Successfully retrieved correlation length data")
            else:
                raise ValueError(f"Correlation length calculation returned abnormal result")
                
        except RuntimeError as e:
            if "è®¡ç®—ç»“æœé•¿åº¦ä¸åŒ¹é…" in str(e):
                print(f"   âš ï¸ éƒ¨åˆ†rå€¼è®¡ç®—å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å®¹é”™å¤„ç†...")
                # è¿‡æ»¤æ‰å®¹æ˜“å¤±è´¥çš„é«˜rå€¼ï¼Œé‡æ–°è®¡ç®—
                r_filtered = r_values[r_values <= 0.9]  # ç§»é™¤r>0.9çš„ç‚¹
                if len(r_filtered) < 10:
                    # å¦‚æœè¿‡æ»¤åç‚¹æ•°å¤ªå°‘ï¼ŒæŠ›å‡ºåŸå§‹å¼‚å¸¸
                    raise e
                
                print(f"   ä½¿ç”¨è¿‡æ»¤åçš„rå€¼èŒƒå›´: [{r_filtered[0]:.3f}, {r_filtered[-1]:.3f}] ({len(r_filtered)}ä¸ªç‚¹)")
                
                corr_result = model.calculate_correlation_length_robust(
                    r_filtered, 
                    removal_type=removal_type, 
                    n_processes=max(1, mp.cpu_count() - 1),
                    external_threshold_params=threshold_params,
                    state_name='X_H',
                    enable_diagnostics=False,
                    enable_power_law=True,
                    init_states=init_states
                )
                
                if len(corr_result) >= 4:
                    corr_lengths_filtered, critical_r, raw_data, power_law_results = corr_result
                    print(f"   âœ… å®¹é”™è®¡ç®—æˆåŠŸï¼Œä½¿ç”¨{len(r_filtered)}ä¸ªæœ‰æ•ˆç‚¹")
                    
                    # æ‰©å±•ç»“æœåˆ°åŸå§‹r_valuesé•¿åº¦ï¼Œç¼ºå¤±å€¼ç”¨NaNå¡«å……
                    corr_lengths = np.full(len(r_values), np.nan)
                    for i, r in enumerate(r_values):
                        if r in r_filtered:
                            filtered_idx = np.where(r_filtered == r)[0][0]
                            corr_lengths[i] = corr_lengths_filtered[filtered_idx]
                    
                else:
                    raise ValueError(f"å®¹é”™å¤„ç†åä»ç„¶å¤±è´¥")
            else:
                # å…¶ä»–ç±»å‹çš„RuntimeErrorï¼Œç›´æ¥æŠ›å‡º
                raise e
        
        # Quality assessment
        quality_info = assess_correlation_quality_v3(
            r_values, corr_lengths, critical_r, power_law_results
        )
        
        max_corr = np.max(corr_lengths)
        max_corr_idx = np.argmax(corr_lengths)
        max_corr_position = r_values[max_corr_idx]
        
        # ä»å¹‚å¾‹ç»“æœæå–Î½å€¼
        nu_value = None
        if power_law_results and 'correlation_length_scaling' in power_law_results:
            scaling = power_law_results['correlation_length_scaling']
            if 'nu' in scaling:
                nu_value = scaling['nu']
        
        print(f"  Quality assessment: {quality_info['overall_score']}/{quality_info['max_score']} ({quality_info['quality_level']})")
        
        # 4. ç¡®å®šç›¸å˜ç±»å‹
        transition_type = determine_transition_type_v3(
            jump_detected, quality_info, max_jump_position, critical_r,
            X_H_values=X_H_values, r_values=r_values, power_law_results=power_law_results
        )
        
        # 5. ä¿å­˜ç»“æœ
        point_result = {
            'phi': phi,
            'theta': theta,
            'r_c': critical_r if transition_type in ['second_order', 'possible_second_order'] else max_jump_position,
            'max_corr': max_corr,
            'max_corr_position': max_corr_position,
            'critical_r': critical_r,
            'nu': nu_value,
            'has_jump': jump_detected,
            'jump_position': max_jump_position if jump_detected else None,
            'jump_size': max_jump_size if jump_detected else 0,
            'transition_type': transition_type,
            'quality_info': quality_info,
            'correlation_lengths': corr_lengths,
            'r_values': r_values,
            'X_H_values': X_H_values,
            'X_M_values': X_M_values,
            'X_L_values': X_L_values,
            'p_risk_values': p_risk_values,
            'p_risk_m_values': p_risk_m_values,
            'p_risk_w_values': p_risk_w_values,
            'power_law_results': power_law_results,
            'failed_points': failed_points
        }
        
        # ä¿å­˜ç»“æœ
        result_file = os.path.join(point_dir, 'result.pkl')
        with open(result_file, 'wb') as f:
            pickle.dump(point_result, f)
        
        # ç”Ÿæˆå¯è§†åŒ–
        generate_visualization_v3(point_result, point_dir, phi)
        
        # ä¿®å¤æ ¼å¼åŒ–é”™è¯¯ï¼šæ·»åŠ å¯¹Noneå€¼çš„å¤„ç†
        r_c_value = point_result['r_c']
        r_c_str = f"{r_c_value:.4f}" if r_c_value is not None else "None"
        print(f"  âœ… Analysis completed: {transition_type}, r_c={r_c_str}")
        
        return {
            'status': 'success',
            'phi': phi,
            'theta': theta,
            'result': point_result
        }
        
    except Exception as e:
        print(f"  âŒ Analysis failed: {str(e)}")
        import traceback
        traceback.print_exc()
        
        return {
            'status': 'failed',
            'phi': phi,
            'theta': theta,
            'error': str(e)
        }

def process_parameter_point_flattened_with_kappa(phi, theta, kappa, r_values, kappa_network_params, 
                                                base_threshold_params, save_dir, 
                                                abs_jump_threshold, removal_type, 
                                                all_results_dict, init_states=None):
    """
    å¤„ç†å•ä¸ªå‚æ•°ç‚¹ï¼Œä½¿ç”¨å±•å¹³çš„é¢„è®¡ç®—ç»“æœ - æ”¯æŒkappaå‚æ•°
    
    å‚æ•°:
        phi, theta, kappa: å‚æ•°å€¼
        r_values: rå€¼æ•°ç»„
        kappa_network_params: ç½‘ç»œå‚æ•°ï¼ˆå·²æ ¹æ®kappaè°ƒæ•´ï¼‰
        base_threshold_params: é˜ˆå€¼å‚æ•°æ¨¡æ¿
        save_dir: ä¿å­˜ç›®å½•
        abs_jump_threshold: è·³å˜é˜ˆå€¼
        removal_type: ç§»é™¤ç±»å‹
        all_results_dict: é¢„è®¡ç®—çš„æ‰€æœ‰ç»“æœå­—å…¸ {(phi, theta, kappa, r): result}
        init_states: åˆå§‹çŠ¶æ€å­—å…¸ï¼Œå¯é€‰
        
    è¿”å›:
        å¤„ç†ç»“æœå­—å…¸
    """
    try:
        # åˆ›å»ºå‚æ•°ç‚¹å¯¹åº”çš„ç›®å½•
        kappa_int = int(round(kappa))
        phi_int = int(round(phi * 100))
        theta_int = int(round(theta * 100))
        
        point_dir = os.path.join(save_dir, f"kappa{kappa_int:03d}_phi{phi_int:03d}_theta{theta_int:03d}")
        os.makedirs(point_dir, exist_ok=True)
        
        # æ„å»ºé˜ˆå€¼å‚æ•°
        if base_threshold_params is None:
            threshold_params = {
                'theta': theta,
                'phi': phi
            }
        else:
            threshold_params = base_threshold_params.copy()
            threshold_params['theta'] = theta
            threshold_params['phi'] = phi
        
        print(f"  1. Extracting steady states from precomputed results ({len(r_values)} points)...")
        
        # ä»é¢„è®¡ç®—ç»“æœä¸­æå–ç¨³æ€å€¼
        X_H_values = []
        X_M_values = []
        X_L_values = []
        p_risk_values = []
        p_risk_m_values = []
        p_risk_w_values = []
        
        failed_points = []
        
        # ğŸ”§ ä¿®å¤ï¼šæ·»åŠ è°ƒè¯•ä¿¡æ¯å’Œå®¹é”™æœºåˆ¶
        print(f"  ğŸ” è°ƒè¯•ï¼šæ£€æŸ¥all_results_dicté”®å€¼åŒ¹é…...")
        print(f"    ç›®æ ‡å‚æ•°ï¼šphi={phi}, theta={theta}, kappa={kappa}")
        print(f"    å­—å…¸ä¸­æ€»é”®æ•°ï¼š{len(all_results_dict)}")
        
        # æ˜¾ç¤ºå­—å…¸ä¸­çš„å‰å‡ ä¸ªé”®ä½œä¸ºæ ·æœ¬
        sample_keys = list(all_results_dict.keys())[:3]
        for i, sample_key in enumerate(sample_keys):
            print(f"    æ ·æœ¬é”®{i+1}: {sample_key}")
        
        matched_count = 0
        for r in r_values:
            # ğŸ”§ ä¿®å¤æµ®ç‚¹æ•°ç²¾åº¦é—®é¢˜ï¼šä½¿ç”¨æ›´å¼ºçš„å®¹é”™åŒ¹é…
            result = None
            
            # ç¬¬ä¸€æ­¥ï¼šå°è¯•ç›´æ¥é”®åŒ¹é…
            key = (phi, theta, kappa, r)
            if key in all_results_dict and all_results_dict[key]['success']:
                result = all_results_dict[key]
                matched_count += 1
            else:
                # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨å®¹é”™åŒ¹é…å¯»æ‰¾æœ€ä½³åŒ¹é…
                best_match = None
                best_r_diff = float('inf')
                
                for stored_key, stored_result in all_results_dict.items():
                    stored_phi, stored_theta, stored_kappa, stored_r = stored_key
                    if (abs(stored_phi - phi) < 1e-10 and 
                        abs(stored_theta - theta) < 1e-10 and 
                        abs(stored_kappa - kappa) < 1e-10 and 
                        stored_result['success']):
                        
                        r_diff = abs(stored_r - r)
                        if r_diff < best_r_diff:
                            best_r_diff = r_diff
                            best_match = stored_result
                
                # å¦‚æœæ‰¾åˆ°äº†è¶³å¤Ÿæ¥è¿‘çš„åŒ¹é…ï¼ˆå·®å¼‚å°äº1e-6ï¼‰
                if best_match is not None and best_r_diff < 1e-6:
                    result = best_match
                    matched_count += 1
                else:
                    # è°ƒè¯•è¾“å‡ºï¼šæ˜¾ç¤ºæœ€æ¥è¿‘çš„é”®
                    closest_r_diff = float('inf')
                    closest_key = None
                    for stored_key in all_results_dict.keys():
                        stored_phi, stored_theta, stored_kappa, stored_r = stored_key
                        if (abs(stored_phi - phi) < 1e-10 and 
                            abs(stored_theta - theta) < 1e-10 and 
                            abs(stored_kappa - kappa) < 1e-10):
                            r_diff = abs(stored_r - r)
                            if r_diff < closest_r_diff:
                                closest_r_diff = r_diff
                                closest_key = stored_key
                    
                    print(f"    âŒ æœªæ‰¾åˆ°r={r:.6f}çš„åŒ¹é…é”®")
                    if closest_key:
                        print(f"       æœ€æ¥è¿‘çš„é”®: {closest_key}, rå·®å¼‚={closest_r_diff:.2e}")
                    
                failed_points.append(r)
                # ä½¿ç”¨NaNå¡«å……ç¼ºå¤±å€¼
                X_H_values.append(np.nan)
                X_M_values.append(np.nan)
                X_L_values.append(np.nan)
                p_risk_values.append(np.nan)
                p_risk_m_values.append(np.nan)
                p_risk_w_values.append(np.nan)
                continue
            
            # æˆåŠŸæ‰¾åˆ°åŒ¹é…ç»“æœ
            X_H_values.append(result['X_H'])
            X_M_values.append(result['X_M'])
            X_L_values.append(result['X_L'])
            p_risk_values.append(result['p_risk'])
            p_risk_m_values.append(result['p_risk_m'])
            p_risk_w_values.append(result['p_risk_w'])
        
        print(f"  ğŸ“Š é”®å€¼åŒ¹é…ç»“æœï¼šæˆåŠŸåŒ¹é… {matched_count}/{len(r_values)} ä¸ªrå€¼")
        
        if failed_points:
            print(f"  âš ï¸ Warning: {len(failed_points)} r values failed to find matches")
            print(f"      Failed r values: {failed_points[:5]}..." if len(failed_points) > 5 else f"      Failed r values: {failed_points}")
        else:
            print(f"  âœ… æ‰€æœ‰rå€¼éƒ½æˆåŠŸåŒ¹é…åˆ°é¢„è®¡ç®—ç»“æœ")
        
        # åºå‚é‡è·³å˜åˆ†æ
        print(f"  2. Analyzing order parameter jumps...")
        
        # ç§»é™¤NaNå€¼è¿›è¡Œè·³å˜åˆ†æ
        valid_mask = ~np.isnan(X_H_values)
        if np.sum(valid_mask) < 5:
            return {
                'status': 'failed',
                'phi': phi,
                'theta': theta,
                'kappa': kappa,
                'error': 'Insufficient valid data points for analysis'
            }
        
        valid_r_values = np.array(r_values)[valid_mask]
        valid_X_H = np.array(X_H_values)[valid_mask]
        
        # ä½¿ç”¨æ”¹è¿›çš„è·³å˜æ£€æµ‹
        jumps, r_jumps, jump_detected, z_score = detect_jumps_improved(valid_r_values, valid_X_H)
        
        # æå–è·³å˜ä¿¡æ¯
        if jumps and r_jumps:
            max_jump_position = r_jumps[0]  # ç¬¬ä¸€ä¸ªï¼ˆæœ€å¤§çš„ï¼‰è·³å˜ä½ç½®
            max_jump_size = jumps[0]        # ç¬¬ä¸€ä¸ªï¼ˆæœ€å¤§çš„ï¼‰è·³å˜å¤§å°
        else:
            max_jump_position = None
            max_jump_size = 0.0
        
        if jump_detected:
            print(f"  Significant jump detected: r_c = {max_jump_position:.4f}, jump size = {max_jump_size:.4f}, z-score = {z_score:.2f}")
        elif max_jump_position is not None:
            print(f"  Jump detected but not significant: r_c = {max_jump_position:.4f}, jump size = {max_jump_size:.4f}, z-score = {z_score:.2f}")
        else:
            print(f"  No jump detected")
        
        # å…³è”é•¿åº¦è®¡ç®— - ç‰©ç†ä¸Šéœ€è¦æ‰°åŠ¨åˆ†æï¼Œå¿…é¡»é‡æ–°è®¡ç®—
        # æ³¨é‡Šæ‰ç¬¬äºŒå±‚å…³è”é•¿åº¦è®¡ç®—ï¼Œåªä¿ç•™ç¬¬ä¸€å±‚ç¨³æ€è®¡ç®—
        print(f"  3. Computing correlation length (requires perturbation analysis)...")
        print(f"  âš ï¸  å…³è”é•¿åº¦è®¡ç®—å·²æ³¨é‡Šæ‰ï¼Œåªè¿›è¡Œç¬¬ä¸€å±‚ç¨³æ€è®¡ç®—")
        
        # åˆ›å»ºæ¨¡å‹å®ä¾‹
        model = ThresholdDynamicsModelV3Fixed(kappa_network_params, threshold_params)
        
        # æ³¨é‡Šæ‰å…³è”é•¿åº¦è®¡ç®—éƒ¨åˆ†
        """
        try:
            # å¹¶è¡Œè®¡ç®—å…³è”é•¿åº¦ï¼ˆç‰©ç†ä¸Šéœ€è¦æ‰°åŠ¨åˆ†æï¼Œä¸èƒ½åŸºäºé¢„è®¡ç®—ç¨³æ€å€¼ï¼‰
            corr_result = model.calculate_correlation_length_robust(
                r_values, 
                removal_type=removal_type, 
                n_processes=max(1, mp.cpu_count() - 1),  # è‡ªåŠ¨é€‰æ‹©åˆç†çš„è¿›ç¨‹æ•°
                external_threshold_params=threshold_params,
                state_name='X_H',
                enable_diagnostics=False,
                enable_power_law=True,
                init_states=init_states  # ä¼ é€’åˆå§‹çŠ¶æ€
            )
            
            # å¤„ç†è¿”å›å€¼
            if len(corr_result) >= 4:
                corr_lengths, critical_r, raw_data, power_law_results = corr_result
                print(f"   Successfully retrieved correlation length data")
            else:
                raise ValueError(f"Correlation length calculation returned abnormal result")
                
        except RuntimeError as e:
            if "è®¡ç®—ç»“æœé•¿åº¦ä¸åŒ¹é…" in str(e):
                print(f"   âš ï¸ éƒ¨åˆ†rå€¼è®¡ç®—å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨å®¹é”™å¤„ç†...")
                # è¿‡æ»¤æ‰å®¹æ˜“å¤±è´¥çš„é«˜rå€¼ï¼Œé‡æ–°è®¡ç®—
                r_filtered = r_values[r_values <= 0.9]  # ç§»é™¤r>0.9çš„ç‚¹
                if len(r_filtered) < 10:
                    # å¦‚æœè¿‡æ»¤åç‚¹æ•°å¤ªå°‘ï¼ŒæŠ›å‡ºåŸå§‹å¼‚å¸¸
                    raise e
                
                print(f"   ä½¿ç”¨è¿‡æ»¤åçš„rå€¼èŒƒå›´: [{r_filtered[0]:.3f}, {r_filtered[-1]:.3f}] ({len(r_filtered)}ä¸ªç‚¹)")
                
                corr_result = model.calculate_correlation_length_robust(
                    r_filtered, 
                    removal_type=removal_type, 
                    n_processes=max(1, mp.cpu_count() - 1),
                    external_threshold_params=threshold_params,
                    state_name='X_H',
                    enable_diagnostics=False,
                    enable_power_law=True,
                    init_states=init_states
                )
                
                if len(corr_result) >= 4:
                    corr_lengths_filtered, critical_r, raw_data, power_law_results = corr_result
                    print(f"   âœ… å®¹é”™è®¡ç®—æˆåŠŸï¼Œä½¿ç”¨{len(r_filtered)}ä¸ªæœ‰æ•ˆç‚¹")
                    
                    # æ‰©å±•ç»“æœåˆ°åŸå§‹r_valuesé•¿åº¦ï¼Œç¼ºå¤±å€¼ç”¨NaNå¡«å……
                    corr_lengths = np.full(len(r_values), np.nan)
                    for i, r in enumerate(r_values):
                        if r in r_filtered:
                            filtered_idx = np.where(r_filtered == r)[0][0]
                            corr_lengths[i] = corr_lengths_filtered[filtered_idx]
                    
                else:
                    raise ValueError(f"å®¹é”™å¤„ç†åä»ç„¶å¤±è´¥")
            else:
                # å…¶ä»–ç±»å‹çš„RuntimeErrorï¼Œç›´æ¥æŠ›å‡º
                raise e
        
        # Quality assessmentï¼ˆç§»åˆ°ifè¯­å¥å¤–é¢ï¼‰
        quality_info = assess_correlation_quality_v3(
            r_values, corr_lengths, critical_r, power_law_results
        )
        
        max_corr = np.max(corr_lengths)
        max_corr_idx = np.argmax(corr_lengths)
        max_corr_position = r_values[max_corr_idx]
        
        # ä»å¹‚å¾‹ç»“æœæå–Î½å€¼
        nu_value = None
        if power_law_results and 'correlation_length_scaling' in power_law_results:
            scaling = power_law_results['correlation_length_scaling']
            if 'nu' in scaling:
                nu_value = scaling['nu']
        
        print(f"  Quality assessment: {quality_info['overall_score']}/{quality_info['max_score']} ({quality_info['quality_level']})")
        """
        
        # ç®€åŒ–ç‰ˆæœ¬ï¼šåªè¿›è¡Œç¬¬ä¸€å±‚ç¨³æ€è®¡ç®—ï¼Œä¸è®¡ç®—å…³è”é•¿åº¦
        corr_lengths = np.full(len(r_values), np.nan)  # å¡«å……NaN
        critical_r = None
        power_law_results = None
        quality_info = {'overall_score': 0, 'max_score': 10, 'quality_level': 'no_data'}
        max_corr = np.nan
        max_corr_position = np.nan
        nu_value = None
        
        print(f"  âœ… ç¬¬ä¸€å±‚ç¨³æ€è®¡ç®—å®Œæˆï¼Œå…³è”é•¿åº¦è®¡ç®—å·²è·³è¿‡")
        
        # 4. ç¡®å®šç›¸å˜ç±»å‹ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼ŒåªåŸºäºè·³å˜æ£€æµ‹ï¼‰
        # ç”±äºå…³è”é•¿åº¦è®¡ç®—è¢«æ³¨é‡Šæ‰ï¼Œç›¸å˜ç±»å‹åˆ¤æ–­ä¹Ÿç›¸åº”ç®€åŒ–
        if jump_detected:
            transition_type = 'first_order'  # æœ‰è·³å˜å°±æ˜¯ä¸€çº§ç›¸å˜
        else:
            transition_type = 'continuous'   # æ— è·³å˜å°±æ˜¯è¿ç»­ç›¸å˜
        
        print(f"  ç®€åŒ–ç›¸å˜ç±»å‹åˆ¤æ–­: {transition_type}")
        
        # æ³¨é‡Šæ‰åŸæ¥çš„å¤æ‚ç›¸å˜ç±»å‹åˆ¤æ–­
        """
        transition_type = determine_transition_type_v3(
            jump_detected, quality_info, max_jump_position, critical_r,
            X_H_values=X_H_values, r_values=r_values, power_law_results=power_law_results
        )
        """
        
        # 5. ä¿å­˜ç»“æœï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œå…³è”é•¿åº¦ç›¸å…³å€¼ä¸ºNaN/Noneï¼‰
        point_result = {
            'phi': phi,
            'theta': theta,
            'kappa': kappa,  # æ·»åŠ kappaä¿¡æ¯
            'r_c': max_jump_position if jump_detected else None,  # ç®€åŒ–ï¼šåªåŸºäºè·³å˜ä½ç½®
            'max_corr': max_corr,  # NaN
            'max_corr_position': max_corr_position,  # NaN
            'critical_r': critical_r,  # None
            'nu': nu_value,  # None
            'has_jump': jump_detected,
            'jump_position': max_jump_position if jump_detected else None,
            'jump_size': max_jump_size if jump_detected else 0,
            'transition_type': transition_type,
            'quality_info': quality_info,  # ç®€åŒ–ç‰ˆæœ¬
            'correlation_lengths': corr_lengths,  # å…¨ä¸ºNaN
            'r_values': r_values,
            'X_H_values': X_H_values,
            'X_M_values': X_M_values,
            'X_L_values': X_L_values,
            'p_risk_values': p_risk_values,
            'p_risk_m_values': p_risk_m_values,
            'p_risk_w_values': p_risk_w_values,
            'power_law_results': power_law_results,  # None
            'failed_points': failed_points
        }
        
        # ä¿å­˜ç»“æœ
        result_file = os.path.join(point_dir, 'result.pkl')
        with open(result_file, 'wb') as f:
            pickle.dump(point_result, f)
        
        # ç”Ÿæˆå¯è§†åŒ–
        generate_visualization_v3_with_kappa(point_result, point_dir, phi, kappa)
        
        # ä¿®å¤æ ¼å¼åŒ–é”™è¯¯ï¼šæ·»åŠ å¯¹Noneå€¼çš„å¤„ç†
        r_c_value = point_result['r_c']
        r_c_str = f"{r_c_value:.4f}" if r_c_value is not None else "None"
        print(f"  âœ… Analysis completed: {transition_type}, r_c={r_c_str}")
        
        return {
            'status': 'success',
            'phi': phi,
            'theta': theta,
            'kappa': kappa,
            'result': point_result
        }
        
    except Exception as e:
        print(f"  âŒ Analysis failed: {str(e)}")
        import traceback
        traceback.print_exc()
        
        return {
            'status': 'failed',
            'phi': phi,
            'theta': theta,
            'kappa': kappa,
            'error': str(e)
        }

def quick_scan_critical_points_v3_optimized(phi_range, theta_range, removal_range, kappa_range=None,
                                  network_params=None, base_threshold_params=None, n_processes=None, 
                                  save_dir=None, abs_jump_threshold=0.2, trivial_threshold=0.1, 
                                  skip_existing=True, removal_type='mainstream',
                                           parallel_param_points=True, init_states=None,
                                           power_law_params=None):
    """
    V3ç‰ˆæœ¬çš„ä¸´ç•Œç‚¹æ‰«æå‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒç¨³æ€ç»“æœç¼“å­˜å’Œå¹‚å¾‹åˆ†å¸ƒå‚æ•°
    
    æ–°å¢åŠŸèƒ½ï¼š
    - ç¨³æ€ç»“æœè‡ªåŠ¨ç¼“å­˜å’ŒåŠ è½½
    - é¿å…é‡å¤è®¡ç®—å·²æœ‰çš„ç¨³æ€å€¼
    - æ”¯æŒä¸­æ–­åç»§ç»­è®¡ç®—
    - æ”¯æŒå¹‚å¾‹åˆ†å¸ƒå‚æ•°é…ç½®
    
    å‚æ•°:
        phi_range: phiå€¼èŒƒå›´ [start, end, step] æˆ– phiå€¼åˆ—è¡¨
        theta_range: thetaå€¼èŒƒå›´ [start, end, step] æˆ– thetaå€¼åˆ—è¡¨
        removal_range: ç§»é™¤æ¯”ä¾‹èŒƒå›´ [start, end, step]
        kappa_range: kappaå€¼èŒƒå›´ [start, end, step] æˆ– kappaå€¼åˆ—è¡¨ï¼Œé»˜è®¤Noneä½¿ç”¨network_paramsä¸­çš„å€¼
        network_params: ç½‘ç»œå‚æ•°(å¯é€‰)
        base_threshold_params: åŸºç¡€é˜ˆå€¼å‚æ•°(å¯é€‰)
        n_processes: å¹¶è¡Œè¿›ç¨‹æ•°(å¯é€‰)
        save_dir: ç»“æœä¿å­˜ç›®å½•(å¯é€‰)
        abs_jump_threshold: ç»å¯¹è·³è·ƒé˜ˆå€¼ï¼Œé»˜è®¤0.2
        trivial_threshold: å¾®å°è·³è·ƒé˜ˆå€¼ï¼Œé»˜è®¤0.1
        skip_existing: æ˜¯å¦è·³è¿‡å·²æœ‰ç»“æœï¼Œé»˜è®¤True
        removal_type: ç§»é™¤ç±»å‹ï¼Œé»˜è®¤'mainstream'
        parallel_param_points: æ˜¯å¦ä½¿ç”¨å¹¶è¡Œè®¡ç®—ï¼Œé»˜è®¤True
        init_states: åˆå§‹çŠ¶æ€å­—å…¸ï¼Œé»˜è®¤None
        power_law_params: å¹‚å¾‹åˆ†å¸ƒå‚æ•°å­—å…¸ï¼ŒåŒ…å«gamma_pref, k_min_prefç­‰(å¯é€‰)
    
    è¿”å›:
        è¯¦ç»†çš„æ‰«æç»“æœå­—å…¸
    """
    print(f"V3 ä¼˜åŒ–ç‰ˆæœ¬æ‰«æ - æ”¯æŒç¨³æ€ç»“æœç¼“å­˜å’Œå¹‚å¾‹åˆ†å¸ƒå‚æ•°...")
    
    # è®¾ç½®ä¿å­˜ç›®å½•
    if save_dir is None:
        import tempfile
        save_dir = tempfile.mkdtemp(prefix=f"v3_optimized_scan_")
    else:
        os.makedirs(save_dir, exist_ok=True)
    
    print(f"Results will be saved in: {save_dir}")
    
    if network_params is None:
        raise ValueError("Must provide network parameters")
    
    # å¤„ç†å¹‚å¾‹åˆ†å¸ƒå‚æ•°
    if power_law_params is not None:
        # éªŒè¯å¹‚å¾‹åˆ†å¸ƒå‚æ•°
        gamma_pref = power_law_params.get('gamma_pref')
        k_min_pref = power_law_params.get('k_min_pref', 1)
        max_k = power_law_params.get('max_k', network_params.get('max_k', 200))
        
        if gamma_pref is not None:
            if not isinstance(gamma_pref, (int, float)) or gamma_pref <= 0:
                raise ValueError(f"å¹‚å¾‹æŒ‡æ•° gamma_pref å¿…é¡»ä¸ºæ­£æ•°ï¼Œå½“å‰å€¼: {gamma_pref}")
            if gamma_pref <= 1:
                raise ValueError(f"å¹‚å¾‹æŒ‡æ•° gamma_pref å¿…é¡»å¤§äº1ä»¥ç¡®ä¿åˆ†å¸ƒå¯å½’ä¸€åŒ–ï¼Œå½“å‰å€¼: {gamma_pref}")
            
            if not isinstance(k_min_pref, int) or k_min_pref < 1:
                raise ValueError(f"å¹‚å¾‹åˆ†å¸ƒä¸‹ç•Œ k_min_pref å¿…é¡»ä¸ºæ­£æ•´æ•°ï¼Œå½“å‰å€¼: {k_min_pref}")
            
            if not isinstance(max_k, int) or max_k <= k_min_pref:
                raise ValueError(f"å¹‚å¾‹åˆ†å¸ƒä¸Šç•Œ max_k å¿…é¡»ä¸ºæ­£æ•´æ•°ä¸”å¤§äºä¸‹ç•Œï¼Œå½“å‰å€¼: {max_k}")
            
            # æ›´æ–°ç½‘ç»œå‚æ•°
            network_params = network_params.copy()
            network_params['gamma_pref'] = gamma_pref
            network_params['k_min_pref'] = k_min_pref
            network_params['max_k'] = max_k
            
            print(f"é…ç½®å¹‚å¾‹åˆ†å¸ƒå‚æ•°: Î³={gamma_pref}, k_min={k_min_pref}, k_max={max_k}")
        else:
            print("æœªé…ç½®å¹‚å¾‹åˆ†å¸ƒå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤åˆ†å¸ƒ")
    else:
        print("æœªæä¾›å¹‚å¾‹åˆ†å¸ƒå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤åˆ†å¸ƒ")
    
    # ç”Ÿæˆå‚æ•°å€¼æ•°ç»„ï¼ˆå¤ç”¨åŸæœ‰é€»è¾‘ï¼‰
    if isinstance(phi_range, list) or isinstance(phi_range, tuple):
        if len(phi_range) == 3 and all(isinstance(x, (int, float)) for x in phi_range):
            phi_start, phi_end, phi_step = phi_range
            phi_values = np.arange(phi_start, phi_end + phi_step/2, phi_step)
        else:
            phi_values = np.array(phi_range)
    else:
        phi_values = np.array([phi_range])
    
    if isinstance(theta_range, list) or isinstance(theta_range, tuple):
        if len(theta_range) == 3 and all(isinstance(x, (int, float)) for x in theta_range):
            theta_start, theta_end, theta_step = theta_range
            theta_values = np.arange(theta_start, theta_end + theta_step/2, theta_step)
        else:
            theta_values = np.array(theta_range)
    else:
        theta_values = np.array([theta_range])
    
    if kappa_range is None:
        default_kappa = network_params.get('k_out_mainstream', 60) + network_params.get('k_out_wemedia', 60)
        kappa_values = np.array([default_kappa])
        print(f"Using default kappa: {default_kappa}")
    else:
        if isinstance(kappa_range, list) or isinstance(kappa_range, tuple):
            if len(kappa_range) == 3 and all(isinstance(x, (int, float)) for x in kappa_range):
                kappa_start, kappa_end, kappa_step = kappa_range
                kappa_values = np.arange(kappa_start, kappa_end + kappa_step/2, kappa_step)
            else:
                kappa_values = np.array(kappa_range)
        else:
            kappa_values = np.array([kappa_range])
        print(f"Scanning kappa values: {kappa_values}")
    
    r_start, r_end, r_step = removal_range
    r_values = np.arange(r_start, r_end + r_step/2, r_step)
    
    # åˆ›å»ºç»“æœå­˜å‚¨
    results = {
        'phi_values': phi_values,
        'theta_values': theta_values,
        'kappa_values': kappa_values,
        'r_values': r_values,
        'critical_points': [],
        'init_states': init_states,
        'power_law_params': power_law_params
    }
    
    # é…ç½®è¿›ç¨‹æ•°
    if n_processes is None:
        n_processes = max(1, mp.cpu_count() - 1)
    n_processes = min(n_processes, 112)
    print(f"Using {n_processes} processes for analysis")
    
    # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç‚¹ï¼ˆé¢„å…ˆè¿‡æ»¤çº¦æŸæ¡ä»¶ï¼‰
    param_points = []
    invalid_points = []
    
    for phi in phi_values:
        for theta in theta_values:
            for kappa in kappa_values:
                # æ£€æŸ¥çº¦æŸæ¡ä»¶ï¼štheta > phi
                if theta > phi:
                    param_points.append((phi, theta, kappa))
                else:
                    invalid_points.append((phi, theta, kappa))
    
    print(f"Total parameter points to process: {len(param_points)}")
    if invalid_points:
        print(f"âš ï¸  è¿‡æ»¤æ‰ {len(invalid_points)} ä¸ªä¸ç¬¦åˆçº¦æŸçš„ç‚¹ (theta <= phi)")
        print(f"   æœ‰æ•ˆå‚æ•°ç‚¹: {len(param_points)}")
        print(f"   æ— æ•ˆå‚æ•°ç‚¹ç¤ºä¾‹: {invalid_points[:3]}...")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç°æœ‰ç»“æœ
    if skip_existing:
        existing_results = []
        for phi, theta, kappa in param_points:
            result_file = os.path.join(save_dir, f"phi_{phi:.3f}_theta_{theta:.3f}_kappa_{kappa:.0f}_result.pkl")
            if os.path.exists(result_file):
                existing_results.append((phi, theta, kappa))
        
        if existing_results:
            print(f"Found {len(existing_results)} existing results, skipping...")
            param_points = [(phi, theta, kappa) for phi, theta, kappa in param_points 
                          if (phi, theta, kappa) not in existing_results]
            print(f"Remaining points to process: {len(param_points)}")
    
    if not param_points:
        print("No new parameter points to process")
        return results
    
    # æ£€æŸ¥ç¨³æ€ç»“æœç¼“å­˜
    steady_state_file = os.path.join(save_dir, "steady_state_cache.pkl")
    existing_steady_results = {}
    
    if os.path.exists(steady_state_file):
        try:
            with open(steady_state_file, 'rb') as f:
                existing_steady_results = pickle.load(f)
            print(f"ğŸ“– åŠ è½½ç¨³æ€ç»“æœç¼“å­˜: {len(existing_steady_results)} ä¸ªç»“æœ")
        except Exception as e:
            print(f"âŒ åŠ è½½ç¨³æ€ç»“æœç¼“å­˜å¤±è´¥: {str(e)}")
            existing_steady_results = {}
    else:
        print(f"ğŸ“ æœªæ‰¾åˆ°ç¨³æ€ç»“æœç¼“å­˜æ–‡ä»¶")
    
    # ç¬¬ä¸€æ­¥ï¼šè®¡ç®—ç¼ºå¤±çš„ç¨³æ€å€¼
    print(f"\nç¬¬ä¸€æ­¥ï¼šè®¡ç®—ç¼ºå¤±çš„ç¨³æ€å€¼...")
    
    flattened_tasks = []
    for phi, theta, kappa in param_points:
        kappa_network_params = network_params.copy()
        kappa_network_params['k_out_mainstream'] = kappa // 2
        kappa_network_params['k_out_wemedia'] = kappa // 2
        
        for r_m in r_values:
            key = (phi, theta, kappa, r_m)
            
            # æ£€æŸ¥æ˜¯å¦å·²ç»è®¡ç®—è¿‡
            if key in existing_steady_results and existing_steady_results[key]['success']:
                continue
            
            task = (phi, theta, r_m, kappa_network_params, base_threshold_params, kappa, init_states)
            flattened_tasks.append(task)
    
    total_steady_tasks = len(param_points) * len(r_values)
    print(f"éœ€è¦è®¡ç®—çš„ç¨³æ€ç‚¹: {len(flattened_tasks)}/{total_steady_tasks}")
    if total_steady_tasks > 0:
        print(f"ç¼“å­˜å‘½ä¸­ç‡: {(total_steady_tasks - len(flattened_tasks)) / total_steady_tasks * 100:.1f}%")
    
    # å¹¶è¡Œè®¡ç®—ç¼ºå¤±çš„ç¨³æ€å€¼
    new_steady_results = {}
    all_steady_results = []  # åˆå§‹åŒ–å˜é‡ï¼Œé˜²æ­¢UnboundLocalError
    
    if len(flattened_tasks) > 0:
        if parallel_param_points and len(flattened_tasks) > 1:
            print(f"Using {n_processes} processes to calculate steady state values...")
            
            with mp.Pool(processes=n_processes) as pool:
                all_steady_results = list(tqdm(
                    pool.map(compute_single_point_r_combination_with_kappa, flattened_tasks),
                    total=len(flattened_tasks),
                    desc="Calculating steady state values"
                ))
        else:
            print("Using serial mode to calculate steady state values...")
            all_steady_results = []
            for task in tqdm(flattened_tasks, desc="Calculating steady state values"):
                result = compute_single_point_r_combination_with_kappa(task)
                all_steady_results.append(result)
    
    # æ•´ç†æ–°è®¡ç®—çš„ç»“æœ
    for result in all_steady_results:
        key = (result['phi'], result['theta'], result['kappa'], result['r_m'])
        new_steady_results[key] = result
    
    # åˆå¹¶æ‰€æœ‰ç¨³æ€ç»“æœ
    all_steady_results_dict = existing_steady_results.copy()
    all_steady_results_dict.update(new_steady_results)
    
    # ğŸ”§ ä¿å­˜æ›´æ–°åçš„ç¨³æ€ç»“æœ
    if len(new_steady_results) > 0:
        try:
            with open(steady_state_file, 'wb') as f:
                pickle.dump(all_steady_results_dict, f)
            print(f"âœ… ç¨³æ€ç»“æœå·²ä¿å­˜åˆ°: {steady_state_file}")
            print(f"   æ€»è®¡ {len(all_steady_results_dict)} ä¸ªç¨³æ€ç»“æœï¼ˆæ–°å¢ {len(new_steady_results)} ä¸ªï¼‰")
        except Exception as e:
            print(f"âŒ ä¿å­˜ç¨³æ€ç»“æœå¤±è´¥: {str(e)}")
    
    # ç»Ÿè®¡æˆåŠŸç‡
    success_count = sum(1 for result in all_steady_results_dict.values() if result['success'])
    print(f"ç¨³æ€è®¡ç®—æ€»æˆåŠŸç‡: {success_count}/{len(all_steady_results_dict)} ({success_count/len(all_steady_results_dict)*100:.1f}%)")
    
    # ç¬¬äºŒæ­¥ï¼šå¤„ç†æ¯ä¸ªå‚æ•°ç‚¹ï¼ˆè®¡ç®—è·³å˜å’Œå…³è”é•¿åº¦ï¼‰
    print(f"\nç¬¬äºŒæ­¥ï¼šä¸²è¡Œå¤„ç†å„å‚æ•°ç‚¹çš„åˆ†æ...")
    
    point_results = []
    failed_cases = 0
    
    for i, (phi, theta, kappa) in enumerate(param_points):
        print(f"\nProcessing parameter point {i+1}/{len(param_points)}: (Îº, Ï†, Î¸) = ({kappa}, {phi:.2f}, {theta:.2f})")
        
        kappa_network_params = network_params.copy()
        kappa_network_params['k_out_mainstream'] = kappa // 2
        kappa_network_params['k_out_wemedia'] = kappa // 2
        
        result = process_parameter_point_flattened_with_kappa(
            phi, theta, kappa, r_values, kappa_network_params, 
            base_threshold_params, save_dir, 
            abs_jump_threshold, removal_type, 
            all_steady_results_dict, init_states
        )
        point_results.append(result)
    
    # å¤„ç†ç»“æœ
    for result in point_results:
        if result['status'] == 'success':
            results['critical_points'].append(result['result'])
        else:
            failed_cases += 1

    # è¾“å‡ºæ‘˜è¦
    print("\n===== ä¼˜åŒ–ç‰ˆæœ¬æ‰«æç»“æœæ‘˜è¦ =====")
    print(f"æ€»å‚æ•°ç‚¹: {len(param_points)}")
    print(f"æˆåŠŸæ¡ˆä¾‹: {len(results['critical_points'])}")
    print(f"å¤±è´¥æ¡ˆä¾‹: {failed_cases}")
    print(f"æˆåŠŸç‡: {len(results['critical_points'])/len(param_points)*100:.1f}%")
    
    # ä¿å­˜æœ€ç»ˆç»“æœ
    results_file = os.path.join(save_dir, "scan_results.pkl")
    try:
        with open(results_file, 'wb') as f:
            pickle.dump(results, f)
        print(f"âœ… æ‰«æç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    except Exception as e:
        print(f"âŒ ä¿å­˜æ‰«æç»“æœå¤±è´¥: {str(e)}")
    
    return results

def assess_correlation_quality_v3(r_values, correlation_lengths, critical_r, power_law_results=None):
    """
    V3ç‰ˆæœ¬çš„è´¨é‡è¯„ä¼°ï¼Œæ›´ä¸¥æ ¼çš„æ ‡å‡†
    """
    r_array = np.array(r_values)
    corr_array = np.array(correlation_lengths)
    
    quality_info = {
        'overall_score': 0,
        'max_score': 10,
        'issues': [],
        'strengths': []
    }
    
    # 1. å³°å€¼æ˜¾è‘—æ€§
    max_idx = np.argmax(corr_array)
    peak_value = corr_array[max_idx]
    
    if len(corr_array) > 10:
        boundary_indices = list(range(min(5, len(corr_array)//4))) + list(range(-min(5, len(corr_array)//4), 0))
        background_values = corr_array[boundary_indices]
        
        if len(background_values) > 0:
            background_mean = np.mean(background_values)
            background_std = np.std(background_values)
            
            if background_std > 0:
                z_score = (peak_value - background_mean) / background_std
                
                if z_score > 4.0:
                    quality_info['overall_score'] += 3
                    quality_info['strengths'].append(f"Highly significant peak: {z_score:.1f}Ïƒ")
                elif z_score > 3.0:
                    quality_info['overall_score'] += 2
                    quality_info['strengths'].append(f"Significant peak: {z_score:.1f}Ïƒ")
                elif z_score > 2.0:
                    quality_info['overall_score'] += 1
                    quality_info['strengths'].append(f"Moderate significance: {z_score:.1f}Ïƒ")
                else:
                    quality_info['issues'].append(f"Weak statistical significance: {z_score:.1f}Ïƒ")
    
    # 2. è¾¹ç•Œæ£€æŸ¥
    if critical_r is not None and (critical_r <= 0.08 or critical_r >= 0.92):
        quality_info['issues'].append(f"Boundary artifact: r_c={critical_r:.4f}")
    elif critical_r is not None:
        quality_info['overall_score'] += 2
        quality_info['strengths'].append("No boundary artifacts")
    else:
        # critical_rä¸ºNoneçš„æƒ…å†µ
        quality_info['issues'].append("Critical point is None")
    
    # 3. å¹‚å¾‹è´¨é‡
    if power_law_results and 'correlation_length_scaling' in power_law_results:
        scaling = power_law_results['correlation_length_scaling']
        if 'nu' in scaling and 'r_squared' in scaling:
            nu = scaling['nu']
            r_squared = scaling['r_squared']
            
            # å®‰å…¨å¤„ç†Noneå€¼
            if (r_squared is not None and nu is not None and 
                r_squared >= 0.8 and 0.3 <= nu <= 2.0):
                quality_info['overall_score'] += 3
                quality_info['strengths'].append(f"Excellent power law: Î½={nu:.3f}, RÂ²={r_squared:.3f}")
            elif r_squared is not None and nu is not None and r_squared >= 0.7:
                quality_info['overall_score'] += 2
                quality_info['strengths'].append(f"Good power law: Î½={nu:.3f}, RÂ²={r_squared:.3f}")
            elif r_squared is not None:
                quality_info['issues'].append(f"Poor power law fit: RÂ²={r_squared:.3f}")
            else:
                quality_info['issues'].append("Power law analysis failed (None values)")
    
    # 4. å¹³æ»‘åº¦
    if len(corr_array) >= 3:
        second_deriv = np.diff(corr_array, n=2)
        smoothness = np.sqrt(np.mean(second_deriv**2))
        
        if smoothness < 0.5:
            quality_info['overall_score'] += 2
            quality_info['strengths'].append(f"Excellent smoothness: {smoothness:.3f}")
        elif smoothness < 1.0:
            quality_info['overall_score'] += 1
        else:
            quality_info['issues'].append(f"Poor smoothness: {smoothness:.3f}")
    
    # ç»¼åˆè¯„ä¼°
    if quality_info['overall_score'] >= 8:
        quality_info['quality_level'] = "excellent"
    elif quality_info['overall_score'] >= 6:
        quality_info['quality_level'] = "good"
    elif quality_info['overall_score'] >= 4:
        quality_info['quality_level'] = "moderate"
    else:
        quality_info['quality_level'] = "poor"
    
    return quality_info

def determine_transition_type_v3(has_jump, quality_info, jump_position, critical_r, 
                                X_H_values=None, r_values=None, power_law_results=None):
    """
    V3ç‰ˆæœ¬çš„ç›¸å˜ç±»å‹åˆ¤æ–­ï¼ŒåŸºäºç‰©ç†ç‰¹å¾çš„ä¸¥æ ¼æ ‡å‡†
    
    åˆ¤æ–­é€»è¾‘ï¼š
    1. æœ‰è·³å˜ + ä¸´ç•Œç‚¹å‰å¹³å¦  = mixed_order (äºŒé˜¶ç›¸å˜ç‰¹å¾ + ä¸€é˜¶è·³è·ƒ)
    2. æœ‰è·³å˜ + ä¸´ç•Œç‚¹å‰ä¸Šå‡è¶‹åŠ¿ = first_order (çº¯ä¸€é˜¶ç›¸å˜)
    3. æ— è·³å˜ + é«˜è´¨é‡å¹‚å¾‹ = second_order (çº¯äºŒé˜¶ç›¸å˜)
    4. å…¶ä»–æƒ…å†µ = continuous æˆ–æ ¹æ®å…·ä½“æƒ…å†µåˆ¤æ–­
    """
    quality_level = quality_info.get('quality_level', 'poor')
    
    # æ£€æŸ¥å¹‚å¾‹åˆ†æè´¨é‡ - ä½¿ç”¨éå¸¸ä¸¥æ ¼çš„æ ‡å‡†
    has_good_power_law = False
    power_law_quality = "none"
    
    if power_law_results and 'correlation_length_scaling' in power_law_results:
        scaling = power_law_results['correlation_length_scaling']
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰é”™è¯¯
        if 'error' in scaling:
            power_law_quality = "failed"
            print(f"  å¹‚å¾‹åˆ†æå¤±è´¥: {scaling['error']}")
        elif 'nu' in scaling and 'r_squared' in scaling and scaling['nu'] is not None and scaling['r_squared'] is not None:
            r_squared = scaling['r_squared']
            nu = scaling['nu']
            fit_points = scaling.get('fit_points', 0)
            has_quality_issue = scaling.get('quality_issue', False)
            
            print(f"  å¹‚å¾‹åˆ†æç»“æœ: Î½={nu:.3f}, RÂ²={r_squared:.3f}, æ‹Ÿåˆç‚¹æ•°={fit_points}")
            
            # ç®€åŒ–çš„å¹‚å¾‹æ ‡å‡† - æ¸…æ™°çš„0.8é˜ˆå€¼
            if (r_squared >= 0.80 and 0.3 <= nu <= 2.5 and fit_points >= 4 and not has_quality_issue):
                has_good_power_law = True
                power_law_quality = "good"
                print(f"  âœ… æ£€æµ‹åˆ°é«˜è´¨é‡å¹‚å¾‹è¡Œä¸ºï¼Œæ”¯æŒäºŒé˜¶ç›¸å˜")
            else:
                power_law_quality = "poor"
                print(f"  âŒ å¹‚å¾‹è´¨é‡ä¸è¶³ï¼Œä¸æ”¯æŒäºŒé˜¶ç›¸å˜")
        else:
            power_law_quality = "incomplete"
            print(f"  âš ï¸ å¹‚å¾‹åˆ†æç»“æœä¸å®Œæ•´")
    
    if has_jump:
        # æœ‰è·³å˜çš„æƒ…å†µï¼šéœ€è¦åˆ†æä¸´ç•Œç‚¹å‰çš„è¡Œä¸º
        if X_H_values is not None and r_values is not None and critical_r is not None:
            # åˆ†æä¸´ç•Œç‚¹å‰çš„è¶‹åŠ¿
            r_array = np.array(r_values)
            X_H_array = np.array(X_H_values)
            
            # æ‰¾åˆ°ä¸´ç•Œç‚¹å‰çš„åŒºåŸŸï¼ˆä¸´ç•Œç‚¹å‰20%çš„æ•°æ®ï¼‰
            pre_critical_mask = r_array < critical_r
            if np.sum(pre_critical_mask) >= 3:
                pre_critical_r = r_array[pre_critical_mask]
                pre_critical_X_H = X_H_array[pre_critical_mask]
                
                # è®¡ç®—ä¸´ç•Œç‚¹å‰æœ€åä¸€æ®µçš„æ–œç‡ï¼ˆä½¿ç”¨æœ€å30%çš„æ•°æ®ï¼‰
                n_recent = max(3, int(len(pre_critical_r) * 0.3))
                recent_r = pre_critical_r[-n_recent:]
                recent_X_H = pre_critical_X_H[-n_recent:]
                
                # çº¿æ€§æ‹Ÿåˆè®¡ç®—æ–œç‡
                if len(recent_r) >= 3:
                    from scipy import stats
                    slope, intercept, r_value, p_value, std_err = stats.linregress(recent_r, recent_X_H)
                    
                    print(f"  ä¸´ç•Œç‚¹å‰è¶‹åŠ¿åˆ†æ: æ–œç‡={slope:.4f}, RÂ²={r_value**2:.3f}")
                    
                    # ä¿®æ­£åˆ¤æ–­é€»è¾‘ï¼šfirst_orderåœ¨ä¸´ç•Œç‚¹å‰æœ‰ä¸Šå‡ï¼Œmixed_orderåœ¨ä¸´ç•Œç‚¹å‰å¹³å¦
                    if abs(slope) < 0.05 and r_value**2 > 0.3:  # å¹³å¦è¶‹åŠ¿ï¼ˆå°æ–œç‡ï¼‰
                        if has_good_power_law:
                            print(f"  ğŸ“Š åˆ¤æ–­: å¹³å¦è¶‹åŠ¿ + é«˜è´¨é‡å¹‚å¾‹ â†’ mixed_order")
                            return 'mixed_order'  # æœ‰è·³å˜ + å¹³å¦è¶‹åŠ¿ + å¥½çš„å¹‚å¾‹ = mixed order
                        else:
                            print(f"  ğŸ“Š åˆ¤æ–­: å¹³å¦è¶‹åŠ¿ï¼Œå¹‚å¾‹è´¨é‡ä¸è¶³ â†’ first_order")
                            return 'first_order'  # å¹³å¦ä½†å¹‚å¾‹è´¨é‡ä¸å¤Ÿï¼Œä»æ˜¯ä¸€é˜¶
                    elif slope > 0.1 and r_value**2 > 0.5:  # æ˜æ˜¾ä¸Šå‡è¶‹åŠ¿
                        print(f"  ğŸ“Š åˆ¤æ–­: ä¸Šå‡è¶‹åŠ¿ + è·³å˜ â†’ first_order")
                        return 'first_order'  # æœ‰è·³å˜ + ä¸Šå‡è¶‹åŠ¿ = çº¯ä¸€é˜¶ç›¸å˜
                    else:  # ä¸‹é™æˆ–ä¸æ˜¾è‘—è¶‹åŠ¿
                        print(f"  ğŸ“Š åˆ¤æ–­: ä¸æ˜ç¡®è¶‹åŠ¿ â†’ first_order")
                        return 'first_order'  # é»˜è®¤ä¸ºä¸€é˜¶ç›¸å˜
        
        # æ— æ³•åˆ†æè¶‹åŠ¿æ—¶çš„fallback
        if has_good_power_law:
            return 'mixed_order'
        else:
            return 'first_order'
    
    else:
        # æ— è·³å˜çš„æƒ…å†µï¼šä¸»è¦çœ‹å¹‚å¾‹è´¨é‡
        if has_good_power_law:
            return 'second_order'  # æ— è·³å˜ + é«˜è´¨é‡å¹‚å¾‹ = äºŒé˜¶ç›¸å˜
        elif quality_level in ['excellent', 'good']:
            return 'possible_second_order'  # è´¨é‡è¿˜å¯ä»¥ä½†å¹‚å¾‹ä¸å¤Ÿå¥½
        else:
            return 'continuous'  # è´¨é‡å·®ï¼Œå¯èƒ½æ˜¯è¿ç»­å˜åŒ–

def generate_visualization_v3(result, save_dir, phi=None):
    """
    V3ç‰ˆæœ¬çš„å¯è§†åŒ–ï¼Œæ›´æ¸…æ™°çš„å±•ç¤º
    """
    try:
        theta = result['theta']
        phi = result.get('phi', phi)  # ä¼˜å…ˆä½¿ç”¨ç»“æœä¸­çš„phiå€¼
        r_values = result['r_values']
        X_H_values = result['X_H_values']
        X_M_values = result['X_M_values']
        X_L_values = result['X_L_values']
        correlation_lengths = result['correlation_lengths']
        critical_r = result['r_c']
        transition_type = result['transition_type']
        quality_info = result.get('quality_info', {})
        power_law_results = result.get('power_law_results')
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. ç¨³æ€å›¾
        ax1 = axes[0, 0]
        ax1.plot(r_values, X_H_values, 'bo-', markersize=4, linewidth=2, label='X_H')
        
        # å®‰å…¨å¤„ç†critical_rä¸ºNoneçš„æƒ…å†µ
        if critical_r is not None:
            ax1.axvline(x=critical_r, color='red', linestyle='--', linewidth=2, 
                       label=f'Critical r_c={critical_r:.4f}')
        else:
            # å¦‚æœcritical_rä¸ºNoneï¼Œä¸æ˜¾ç¤ºå‚ç›´çº¿ï¼Œä½†åœ¨å›¾ä¾‹ä¸­æ ‡æ³¨
            ax1.plot([], [], color='red', linestyle='--', linewidth=2, label='Critical r_c=None')
        
        if result.get('has_jump') and result.get('jump_position'):
            jump_pos = result['jump_position']
            ax1.axvline(x=jump_pos, color='orange', linestyle=':', linewidth=2,
                       label=f'Jump at r={jump_pos:.4f}')
        
        ax1.set_xlabel('Removal Ratio (r)')
        ax1.set_ylabel('High Arousal Proportion (X_H)')
        ax1.set_title(f'Steady State (Ï†={phi:.2f}, Î¸={theta:.2f})')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. å…³è”é•¿åº¦å›¾
        ax2 = axes[0, 1]
        ax2.plot(r_values, correlation_lengths, 'co-', markersize=4, linewidth=2)
        
        # å®‰å…¨å¤„ç†critical_rä¸ºNoneçš„æƒ…å†µ
        if critical_r is not None:
            ax2.axvline(x=critical_r, color='red', linestyle='--', linewidth=2)
        
        max_idx = np.argmax(correlation_lengths)
        max_corr = correlation_lengths[max_idx]
        max_r = r_values[max_idx]
        ax2.plot(max_r, max_corr, 'ro', markersize=8, 
                label=f'Max: Î¾={max_corr:.2f} at r={max_r:.4f}')
        
        ax2.set_xlabel('Removal Ratio (r)')
        ax2.set_ylabel('Correlation Length (Î¾)')
        ax2.set_title('Correlation Length')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. å¹‚å¾‹åˆ†æ
        ax3 = axes[1, 0]
        if power_law_results and 'correlation_length_scaling' in power_law_results and critical_r is not None:
            scaling = power_law_results['correlation_length_scaling']
            if 'nu' in scaling and 'r_squared' in scaling:
                nu = scaling['nu']
                r_squared = scaling['r_squared']
                
                # åœ¨ä¸´ç•Œç‚¹é™„è¿‘çš„æ•°æ®
                dr = np.abs(r_values - critical_r)
                mask = (dr > 0.001) & (correlation_lengths > 0.01)
                
                if np.sum(mask) > 0:
                    ax3.loglog(dr[mask], correlation_lengths[mask], 'o', 
                              color='blue', markersize=6)
                    
                    # æ‹Ÿåˆçº¿
                    dr_fit = np.logspace(np.log10(min(dr[mask])), 
                                        np.log10(max(dr[mask])), 100)
                    intercept = scaling.get('intercept', 0)
                    corr_fit = 10**(intercept - nu * np.log10(dr_fit))
                    ax3.loglog(dr_fit, corr_fit, 'r-', linewidth=2, 
                              label=f'Î¾ ~ |r-r_c|^(-{nu:.3f})\nRÂ² = {r_squared:.3f}')
                
                ax3.set_xlabel('|r - r_c|')
                ax3.set_ylabel('Correlation Length (Î¾)')
                ax3.set_title('Power Law Analysis')
                ax3.legend()
                ax3.grid(True, alpha=0.3, which='both')
        else:
            # å¦‚æœæ²¡æœ‰å¹‚å¾‹åˆ†ææˆ–critical_rä¸ºNoneï¼Œæ˜¾ç¤ºæç¤ºä¿¡æ¯
            reason = 'No power law analysis'
            if critical_r is None:
                reason = 'No power law analysis\n(critical_r is None)'
            ax3.text(0.5, 0.5, reason, 
                    ha='center', va='center', transform=ax3.transAxes,
                    fontsize=12)
        
        # 4. è´¨é‡ä¿¡æ¯
        ax4 = axes[1, 1]
        ax4.axis('off')
        
        # å®‰å…¨å¤„ç†critical_rä¸ºNoneçš„æƒ…å†µ
        critical_r_str = f"{critical_r:.4f}" if critical_r is not None else "None"
        
        quality_text = f"""Transition Type: {transition_type.upper()}
Quality Level: {quality_info.get('quality_level', 'unknown')}
Score: {quality_info.get('overall_score', 0)}/{quality_info.get('max_score', 10)}
Critical Point: r_c = {critical_r_str}

Strengths:
"""
        for strength in quality_info.get('strengths', [])[:3]:
            quality_text += f"â€¢ {strength}\n"
        
        quality_text += "\nIssues:\n"
        for issue in quality_info.get('issues', [])[:3]:
            quality_text += f"â€¢ {issue}\n"
        
        # Set color
        if transition_type == 'second_order':
            bg_color = 'lightgreen'
        elif transition_type in ['first_order', 'mixed_order']:
            bg_color = 'lightblue'
        else:
            bg_color = 'lightcoral'
        
        ax4.text(0.05, 0.95, quality_text, transform=ax4.transAxes, fontsize=10,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor=bg_color, alpha=0.8))
        
        plt.suptitle(f'V3 Analysis: Ï†={phi:.2f}, Î¸={theta:.2f} - {transition_type.upper()}', 
                    fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, f'v3_analysis_phi{phi:.2f}_theta{theta:.2f}.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"    Visualization charts saved")
        
    except Exception as e:
        print(f"    Error generating visualization: {str(e)}")
        import traceback
        traceback.print_exc()

def generate_visualization_v3_with_kappa(result, save_dir, phi, kappa):
    """
    V3ç‰ˆæœ¬çš„å¯è§†åŒ–ï¼Œæ›´æ¸…æ™°çš„å±•ç¤º
    """
    try:
        theta = result['theta']
        phi = result.get('phi', phi)  # ä¼˜å…ˆä½¿ç”¨ç»“æœä¸­çš„phiå€¼
        r_values = result['r_values']
        X_H_values = result['X_H_values']
        X_M_values = result['X_M_values']
        X_L_values = result['X_L_values']
        correlation_lengths = result['correlation_lengths']
        critical_r = result['r_c']
        transition_type = result['transition_type']
        quality_info = result.get('quality_info', {})
        power_law_results = result.get('power_law_results')
        
        # åˆ›å»ºå›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(12, 10))
        
        # 1. ç¨³æ€å›¾
        ax1 = axes[0, 0]
        ax1.plot(r_values, X_H_values, 'bo-', markersize=4, linewidth=2, label='X_H')
        
        # å®‰å…¨å¤„ç†critical_rä¸ºNoneçš„æƒ…å†µ
        if critical_r is not None:
            ax1.axvline(x=critical_r, color='red', linestyle='--', linewidth=2, 
                       label=f'Critical r_c={critical_r:.4f}')
        else:
            # å¦‚æœcritical_rä¸ºNoneï¼Œä¸æ˜¾ç¤ºå‚ç›´çº¿ï¼Œä½†åœ¨å›¾ä¾‹ä¸­æ ‡æ³¨
            ax1.plot([], [], color='red', linestyle='--', linewidth=2, label='Critical r_c=None')
        
        if result.get('has_jump') and result.get('jump_position'):
            jump_pos = result['jump_position']
            ax1.axvline(x=jump_pos, color='orange', linestyle=':', linewidth=2,
                       label=f'Jump at r={jump_pos:.4f}')
        
        ax1.set_xlabel('Removal Ratio (r)')
        ax1.set_ylabel('High Arousal Proportion (X_H)')
        ax1.set_title(f'Steady State (Ï†={phi:.2f}, Î¸={theta:.2f})')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # 2. å…³è”é•¿åº¦å›¾
        ax2 = axes[0, 1]
        ax2.plot(r_values, correlation_lengths, 'co-', markersize=4, linewidth=2)
        
        # å®‰å…¨å¤„ç†critical_rä¸ºNoneçš„æƒ…å†µ
        if critical_r is not None:
            ax2.axvline(x=critical_r, color='red', linestyle='--', linewidth=2)
        
        max_idx = np.argmax(correlation_lengths)
        max_corr = correlation_lengths[max_idx]
        max_r = r_values[max_idx]
        ax2.plot(max_r, max_corr, 'ro', markersize=8, 
                label=f'Max: Î¾={max_corr:.2f} at r={max_r:.4f}')
        
        ax2.set_xlabel('Removal Ratio (r)')
        ax2.set_ylabel('Correlation Length (Î¾)')
        ax2.set_title('Correlation Length')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # 3. å¹‚å¾‹åˆ†æ
        ax3 = axes[1, 0]
        if power_law_results and 'correlation_length_scaling' in power_law_results and critical_r is not None:
            scaling = power_law_results['correlation_length_scaling']
            if 'nu' in scaling and 'r_squared' in scaling:
                nu = scaling['nu']
                r_squared = scaling['r_squared']
                
                # åœ¨ä¸´ç•Œç‚¹é™„è¿‘çš„æ•°æ®
                dr = np.abs(r_values - critical_r)
                mask = (dr > 0.001) & (correlation_lengths > 0.01)
                
                if np.sum(mask) > 0:
                    ax3.loglog(dr[mask], correlation_lengths[mask], 'o', 
                              color='blue', markersize=6)
                    
                    # æ‹Ÿåˆçº¿
                    dr_fit = np.logspace(np.log10(min(dr[mask])), 
                                        np.log10(max(dr[mask])), 100)
                    intercept = scaling.get('intercept', 0)
                    corr_fit = 10**(intercept - nu * np.log10(dr_fit))
                    ax3.loglog(dr_fit, corr_fit, 'r-', linewidth=2, 
                              label=f'Î¾ ~ |r-r_c|^(-{nu:.3f})\nRÂ² = {r_squared:.3f}')
                
                ax3.set_xlabel('|r - r_c|')
                ax3.set_ylabel('Correlation Length (Î¾)')
                ax3.set_title('Power Law Analysis')
                ax3.legend()
                ax3.grid(True, alpha=0.3, which='both')
        else:
            # å¦‚æœæ²¡æœ‰å¹‚å¾‹åˆ†ææˆ–critical_rä¸ºNoneï¼Œæ˜¾ç¤ºæç¤ºä¿¡æ¯
            reason = 'No power law analysis'
            if critical_r is None:
                reason = 'No power law analysis\n(critical_r is None)'
            ax3.text(0.5, 0.5, reason, 
                    ha='center', va='center', transform=ax3.transAxes,
                    fontsize=12)
        
        # 4. è´¨é‡ä¿¡æ¯
        ax4 = axes[1, 1]
        ax4.axis('off')
        
        # å®‰å…¨å¤„ç†critical_rä¸ºNoneçš„æƒ…å†µ
        critical_r_str = f"{critical_r:.4f}" if critical_r is not None else "None"
        
        quality_text = f"""Transition Type: {transition_type.upper()}
Quality Level: {quality_info.get('quality_level', 'unknown')}
Score: {quality_info.get('overall_score', 0)}/{quality_info.get('max_score', 10)}
Critical Point: r_c = {critical_r_str}

Strengths:
"""
        for strength in quality_info.get('strengths', [])[:3]:
            quality_text += f"â€¢ {strength}\n"
        
        quality_text += "\nIssues:\n"
        for issue in quality_info.get('issues', [])[:3]:
            quality_text += f"â€¢ {issue}\n"
        
        # Set color
        if transition_type == 'second_order':
            bg_color = 'lightgreen'
        elif transition_type in ['first_order', 'mixed_order']:
            bg_color = 'lightblue'
        else:
            bg_color = 'lightcoral'
        
        ax4.text(0.05, 0.95, quality_text, transform=ax4.transAxes, fontsize=10,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor=bg_color, alpha=0.8))
        
        plt.suptitle(f'V3 Analysis: Ï†={phi:.2f}, Î¸={theta:.2f} - {transition_type.upper()}', 
                    fontsize=14, fontweight='bold')
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, f'v3_analysis_phi{phi:.2f}_theta{theta:.2f}_kappa{kappa}.png'), 
                   dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"    Visualization charts saved")
        
    except Exception as e:
        print(f"    Error generating visualization: {str(e)}")
        import traceback
        traceback.print_exc()

def load_scan_results(results_file):
    """
    Load scan results file
    
    Parameters:
        results_file: Path to results file
        
    Returns:
        Scan results dictionary or None
    """
    try:
        with open(results_file, 'rb') as f:
            results = pickle.load(f)
        return results
    except Exception as e:
        print(f"Failed to load results file: {str(e)}")
        return None

def load_case_data_from_dir(base_dir, phi, theta, kappa=120):
    """
    Load data for specific parameter point from directory
    
    Parameters:
        base_dir: Base directory path
        phi: phi value
        theta: theta value
        kappa: kappa value (k_out_mainstream + k_out_wemedia), default 120
        
    Returns:
        Data dictionary or None
    """
    # Use integer naming to find directory
    kappa_int = int(round(kappa))
    phi_int = int(round(phi * 100))  # Convert 0.55 to 55
    theta_int = int(round(theta * 100))  # Convert 0.37 to 37
    point_dir = os.path.join(base_dir, f"kappa{kappa_int:03d}_phi{phi_int:03d}_theta{theta_int:03d}")
    result_file = os.path.join(point_dir, 'result.pkl')
    
    if not os.path.exists(result_file):
        # Try old phi_theta naming (backward compatibility)
        point_dir_old = os.path.join(base_dir, f"phi{phi_int:03d}_theta{theta_int:03d}")
        result_file_old = os.path.join(point_dir_old, 'result.pkl')
        
        if os.path.exists(result_file_old):
            result_file = result_file_old
        else:
            # Try even older float naming
            point_dir_float = os.path.join(base_dir, f"phi{phi:.2f}_theta{theta:.2f}")
            result_file_float = os.path.join(point_dir_float, 'result.pkl')
            
            if os.path.exists(result_file_float):
                result_file = result_file_float
            else:
                print(f"Results file does not exist: {result_file}")
                return None
    
    try:
        with open(result_file, 'rb') as f:
            result = pickle.load(f)
        
        # Ensure data integrity
        required_keys = ['r_values', 'X_H_values', 'correlation_lengths']
        if not all(key in result for key in required_keys):
            print(f"Results file data incomplete: {result_file}")
            return None
        
        return result
    except Exception as e:
        print(f"Failed to load results file: {result_file} - {str(e)}")
        return None

def create_comparison_plots(results, save_dir):
    """
    Create comparison plots
    
    Parameters:
        results: Scan results dictionary
        save_dir: Save directory
    """
    if 'critical_points' not in results or not results['critical_points']:
        print("No available data for visualization")
        return
    
    comparison_dir = os.path.join(save_dir, "comparison_plots")
    os.makedirs(comparison_dir, exist_ok=True)
    
    # Group by transition type
    transition_types = {}
    for point in results['critical_points']:
        t_type = point.get('transition_type', 'unknown')
        if t_type not in transition_types:
            transition_types[t_type] = []
        transition_types[t_type].append(point)
    
    # Create comparison plots for each transition type
    for t_type, type_data in transition_types.items():
        if len(type_data) < 2:
            continue
            
        # Order parameter comparison
        create_order_parameter_comparison(type_data, comparison_dir, t_type)
        
        # Correlation length comparison
        create_correlation_length_comparison(type_data, comparison_dir, t_type)
    
    # Create global comparison plots
    create_global_phase_diagram(results['critical_points'], comparison_dir)
    
    # Create parameter trend plots
    create_parameter_trends(results['critical_points'], comparison_dir)

def create_order_parameter_comparison(type_data, save_dir, transition_type):
    """Create order parameter comparison plot"""
    plt.figure(figsize=(12, 8))
    
    cmap = plt.get_cmap('tab10')
    
    for i, data in enumerate(type_data):
        phi = data['phi']
        theta = data['theta']
        r_values = data['r_values']
        X_H_values = data['X_H_values']
        critical_r = data.get('r_c')
        
        color = cmap(i % 10)
        
        # å®Œå…¨ç§»é™¤labelå‚æ•°ï¼Œé¿å…è‡ªåŠ¨åˆ›å»ºå›¾ä¾‹
        plt.plot(r_values, X_H_values, 'o-', color=color, 
                markersize=4, linewidth=2)
        
        if critical_r is not None:
            plt.axvline(x=critical_r, color=color, linestyle='--', alpha=0.7)
    
    plt.xlabel('Removal Ratio (r)')
    plt.ylabel('High Arousal Proportion (X_H)')
    plt.title(f'Order Parameter Comparison - {transition_type.replace("_", " ").title()}')
    # ç§»é™¤å›¾ä¾‹æ˜¾ç¤ºï¼Œå› ä¸ºcaseæ•°é‡å¤ªå¤š
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    filename = f"order_parameter_{transition_type}.png"
    plt.savefig(os.path.join(save_dir, filename), dpi=300)
    plt.close()
    
    print(f"Order parameter comparison plot saved: {filename}")

def create_correlation_length_comparison(type_data, save_dir, transition_type):
    """Create correlation length comparison plot"""
    plt.figure(figsize=(12, 8))
    
    cmap = plt.get_cmap('tab10')
    
    for i, data in enumerate(type_data):
        phi = data['phi']
        theta = data['theta']
        r_values = data['r_values']
        correlation_lengths = data['correlation_lengths']
        critical_r = data.get('r_c')
        nu = data.get('nu')
        
        color = cmap(i % 10)
        
        # å®Œå…¨ç§»é™¤labelå‚æ•°ï¼Œé¿å…è‡ªåŠ¨åˆ›å»ºå›¾ä¾‹
        plt.plot(r_values, correlation_lengths, 'o-', color=color, 
                markersize=4, linewidth=2)
        
        if critical_r is not None:
            plt.axvline(x=critical_r, color=color, linestyle='--', alpha=0.7)
    
    plt.xlabel('Removal Ratio (r)')
    plt.ylabel('Correlation Length (Î¾)')
    plt.title(f'Correlation Length Comparison - {transition_type.replace("_", " ").title()}')
    # ç§»é™¤å›¾ä¾‹æ˜¾ç¤ºï¼Œå› ä¸ºcaseæ•°é‡å¤ªå¤š
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    
    filename = f"correlation_length_{transition_type}.png"
    plt.savefig(os.path.join(save_dir, filename), dpi=300)
    plt.close()
    
    print(f"Correlation length comparison plot saved: {filename}")

def create_global_phase_diagram(all_data, save_dir):
    """Create global phase diagram with kappa support"""
    phi_values = []
    theta_values = []
    kappa_values = []
    critical_rs = []
    transition_types = []
    
    for data in all_data:
        phi_values.append(data['phi'])
        theta_values.append(data['theta'])
        kappa_values.append(data.get('kappa', 120))  # é»˜è®¤å€¼120
        # å®‰å…¨å¤„ç†Noneå€¼ï¼šå°†Noneè½¬æ¢ä¸ºnp.nan
        r_c = data.get('r_c', None)
        if r_c is None:
            critical_rs.append(np.nan)
        else:
            critical_rs.append(r_c)
        transition_types.append(data.get('transition_type', 'unknown'))
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªkappaå€¼
    unique_kappas = list(set(kappa_values))
    has_multiple_kappas = len(unique_kappas) > 1
    
    if has_multiple_kappas:
        # åˆ›å»ºåŒ…å«kappaç»´åº¦çš„å›¾è¡¨
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. æŒ‰kappaåˆ†ç»„çš„ç›¸å˜ç±»å‹åˆ†å¸ƒ
        ax1 = axes[0, 0]
        type_colors = {
            'second_order': 'red',
            'first_order': 'blue', 
            'mixed_order': 'purple',
            'possible_second_order': 'orange',
            'continuous': 'gray',
            'unknown': 'black'
        }
        
        for kappa in sorted(unique_kappas):
            kappa_indices = [i for i, k in enumerate(kappa_values) if k == kappa]
            if kappa_indices:
                phi_subset = [phi_values[i] for i in kappa_indices]
                theta_subset = [theta_values[i] for i in kappa_indices]
                types_subset = [transition_types[i] for i in kappa_indices]
                
                for t_type in set(types_subset):
                    type_indices = [i for i, t in enumerate(types_subset) if t == t_type]
                    if type_indices:
                        phi_type = [phi_subset[i] for i in type_indices]
                        theta_type = [theta_subset[i] for i in type_indices]
                        
                        ax1.scatter(phi_type, theta_type, 
                                   c=type_colors.get(t_type, 'black'),
                                   marker='o' if kappa == min(unique_kappas) else '^',
                                   s=50, alpha=0.7,
                                   label=f'Îº={kappa}, {t_type}' if len(phi_type) > 0 else None)
        
        ax1.set_xlabel('Ï† (Low Arousal Threshold)')
        ax1.set_ylabel('Î¸ (High Arousal Threshold)')
        ax1.set_title('Phase Diagram by Kappa - Transition Types')
        ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        ax1.grid(True, alpha=0.3)
        
        # 2. ä¸´ç•Œç‚¹åˆ†å¸ƒï¼ˆæŒ‰kappaåˆ†ç»„ï¼‰
        ax2 = axes[0, 1]
        valid_indices = []
        for i, r_c in enumerate(critical_rs):
            try:
                if r_c is not None and not np.isnan(r_c):
                    valid_indices.append(i)
            except (TypeError, ValueError):
                continue
        
        if valid_indices:
            phi_valid = [phi_values[i] for i in valid_indices]
            theta_valid = [theta_values[i] for i in valid_indices]
            kappa_valid = [kappa_values[i] for i in valid_indices]
            r_c_valid = [critical_rs[i] for i in valid_indices]
            
            # ä¸ºä¸åŒkappaä½¿ç”¨ä¸åŒæ ‡è®°
            kappa_markers = {k: marker for k, marker in zip(sorted(unique_kappas), ['o', '^', 's', 'D', 'v'])}
            
            for kappa in sorted(unique_kappas):
                kappa_indices = [i for i, k in enumerate(kappa_valid) if k == kappa]
                if kappa_indices:
                    phi_kappa = [phi_valid[i] for i in kappa_indices]
                    theta_kappa = [theta_valid[i] for i in kappa_indices]
                    r_c_kappa = [r_c_valid[i] for i in kappa_indices]
                    
                    scatter = ax2.scatter(phi_kappa, theta_kappa, c=r_c_kappa,
                                        marker=kappa_markers.get(kappa, 'o'),
                                        cmap='viridis', s=50, alpha=0.7,
                                        label=f'Îº={kappa}')
            
            # æ·»åŠ é¢œè‰²æ¡
            plt.colorbar(scatter, ax=ax2, label='Critical Point (r_c)')
            ax2.legend()
        else:
            ax2.text(0.5, 0.5, 'No valid critical points found', 
                    ha='center', va='center', transform=ax2.transAxes, fontsize=12)
        
        ax2.set_xlabel('Ï† (Low Arousal Threshold)')
        ax2.set_ylabel('Î¸ (High Arousal Threshold)')
        ax2.set_title('Phase Diagram by Kappa - Critical Points')
        ax2.grid(True, alpha=0.3)
        
        # 3. Kappa vs Critical Point
        ax3 = axes[1, 0]
        if valid_indices:
            kappa_valid = [kappa_values[i] for i in valid_indices]
            r_c_valid = [critical_rs[i] for i in valid_indices]
            
            ax3.scatter(kappa_valid, r_c_valid, alpha=0.7)
            ax3.set_xlabel('Îº (Connectivity)')
            ax3.set_ylabel('Critical Point (r_c)')
            ax3.set_title('Critical Point vs Connectivity')
            ax3.grid(True, alpha=0.3)
        else:
            ax3.text(0.5, 0.5, 'No valid critical points', 
                    ha='center', va='center', transform=ax3.transAxes)
            ax3.set_title('Critical Point vs Connectivity (No Data)')
        
        # 4. ç»Ÿè®¡ä¿¡æ¯
        ax4 = axes[1, 1]
        ax4.axis('off')
        
        # åˆ›å»ºç»Ÿè®¡è¡¨æ ¼
        stats_text = "Statistics by Kappa:\n\n"
        for kappa in sorted(unique_kappas):
            kappa_indices = [i for i, k in enumerate(kappa_values) if k == kappa]
            kappa_types = [transition_types[i] for i in kappa_indices]
            type_counts = {}
            for t in kappa_types:
                type_counts[t] = type_counts.get(t, 0) + 1
            
            stats_text += f"Îº = {kappa} ({len(kappa_indices)} points):\n"
            for t_type, count in type_counts.items():
                stats_text += f"  {t_type}: {count}\n"
            stats_text += "\n"
        
        ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=10,
                verticalalignment='top', fontfamily='monospace',
                bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
        
        plt.suptitle('Global Phase Diagram with Kappa Analysis', fontsize=16, fontweight='bold')
        
    else:
        # å•ä¸€kappaçš„æƒ…å†µï¼Œä½¿ç”¨åŸæ¥çš„å¸ƒå±€
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))
        
        # Transition type distribution
        type_colors = {
            'second_order': 'red',
            'first_order': 'blue', 
            'mixed_order': 'purple',
            'possible_second_order': 'orange',
            'continuous': 'gray',
            'unknown': 'black'
        }
        
        for t_type in set(transition_types):
            indices = [i for i, t in enumerate(transition_types) if t == t_type]
            if indices:
                phi_subset = [phi_values[i] for i in indices]
                theta_subset = [theta_values[i] for i in indices]
                ax1.scatter(phi_subset, theta_subset, 
                           c=type_colors.get(t_type, 'black'), 
                           label=t_type, s=50, alpha=0.7)
        
        ax1.set_xlabel('Ï† (Low Arousal Threshold)')
        ax1.set_ylabel('Î¸ (High Arousal Threshold)')
        ax1.set_title(f'Phase Diagram - Transition Types (Îº={unique_kappas[0]})')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Critical point distribution - å®‰å…¨å¤„ç†NaNå€¼
        valid_indices = []
        for i, r_c in enumerate(critical_rs):
            try:
                if r_c is not None and not np.isnan(r_c):
                    valid_indices.append(i)
            except (TypeError, ValueError):
                continue
        
        if valid_indices:
            phi_valid = [phi_values[i] for i in valid_indices]
            theta_valid = [theta_values[i] for i in valid_indices]
            r_c_valid = [critical_rs[i] for i in valid_indices]
            
            scatter = ax2.scatter(phi_valid, theta_valid, c=r_c_valid, 
                                cmap='viridis', s=50, alpha=0.7)
            plt.colorbar(scatter, ax=ax2, label='Critical Point (r_c)')
        else:
            ax2.text(0.5, 0.5, 'No valid critical points found', 
                    ha='center', va='center', transform=ax2.transAxes, fontsize=12)
        
        ax2.set_xlabel('Ï† (Low Arousal Threshold)')
        ax2.set_ylabel('Î¸ (High Arousal Threshold)')
        ax2.set_title(f'Phase Diagram - Critical Points (Îº={unique_kappas[0]})')
        ax2.grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, "global_phase_diagram.png"), dpi=300)
    plt.close()
    
    print("Global phase diagram saved: global_phase_diagram.png")

def create_parameter_trends(all_data, save_dir):
    """Create parameter trends plot"""
    phi_values = [data['phi'] for data in all_data]
    theta_values = [data['theta'] for data in all_data]
    
    # å®‰å…¨å¤„ç†Noneå€¼
    r_c_values = []
    nu_values = []
    
    for data in all_data:
        # å¤„ç†r_cå€¼
        r_c = data.get('r_c', None)
        if r_c is None:
            r_c_values.append(np.nan)
        else:
            r_c_values.append(r_c)
        
        # å¤„ç†nuå€¼
        nu = data.get('nu', None)
        if nu is None:
            nu_values.append(np.nan)
        else:
            nu_values.append(nu)
    
    fig, axes = plt.subplots(2, 2, figsize=(14, 10))
    
    # r_c vs phi - å®‰å…¨å¤„ç†NaNå€¼
    valid_indices = []
    for i, r_c in enumerate(r_c_values):
        try:
            if r_c is not None and not np.isnan(r_c):
                valid_indices.append(i)
        except (TypeError, ValueError):
            continue
    
    if valid_indices:
        phi_valid = [phi_values[i] for i in valid_indices]
        r_c_valid = [r_c_values[i] for i in valid_indices]
        axes[0,0].plot(phi_valid, r_c_valid, 'bo-')
        axes[0,0].set_xlabel('Ï†')
        axes[0,0].set_ylabel('Critical Point (r_c)')
        axes[0,0].set_title('Critical Point vs Ï†')
        axes[0,0].grid(True, alpha=0.3)
    else:
        axes[0,0].text(0.5, 0.5, 'No valid critical points', 
                      ha='center', va='center', transform=axes[0,0].transAxes)
        axes[0,0].set_title('Critical Point vs Ï† (No Data)')
    
    # Î½ vs phi - å®‰å…¨å¤„ç†NaNå€¼
    valid_indices = []
    for i, nu in enumerate(nu_values):
        try:
            if nu is not None and not np.isnan(nu):
                valid_indices.append(i)
        except (TypeError, ValueError):
            continue
    
    if valid_indices:
        phi_valid = [phi_values[i] for i in valid_indices]
        nu_valid = [nu_values[i] for i in valid_indices]
        axes[0,1].plot(phi_valid, nu_valid, 'ro-')
        axes[0,1].set_xlabel('Ï†')
        axes[0,1].set_ylabel('Critical Exponent (Î½)')
        axes[0,1].set_title('Critical Exponent vs Ï†')
        axes[0,1].grid(True, alpha=0.3)
    else:
        axes[0,1].text(0.5, 0.5, 'No valid critical exponents', 
                      ha='center', va='center', transform=axes[0,1].transAxes)
        axes[0,1].set_title('Critical Exponent vs Ï† (No Data)')
    
    # Transition type statistics
    transition_counts = {}
    for data in all_data:
        t_type = data.get('transition_type', 'unknown')
        transition_counts[t_type] = transition_counts.get(t_type, 0) + 1
    
    types = list(transition_counts.keys())
    counts = list(transition_counts.values())
    axes[1,0].bar(types, counts)
    axes[1,0].set_xlabel('Transition Type')
    axes[1,0].set_ylabel('Count')
    axes[1,0].set_title('Transition Type Distribution')
    axes[1,0].tick_params(axis='x', rotation=45)
    
    # Clear last subplot
    axes[1,1].axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, "parameter_trends.png"), dpi=300)
    plt.close()
    
    print("Parameter trends plot saved: parameter_trends.png")

def analyze_scan_results(results_file, create_plots=True):
    """
    Analyze scan results and generate visualization
    
    Parameters:
        results_file: Path to results file
        create_plots: Whether to create plots
    """
    print(f"Analyzing scan results: {results_file}")
    
    # Load results
    results = load_scan_results(results_file)
    if results is None:
        return
    
    # Output basic statistics
    critical_points = results.get('critical_points', [])
    print(f"Total parameter points: {len(critical_points)}")
    
    # Transition type statistics
    type_counts = {}
    for point in critical_points:
        t_type = point.get('transition_type', 'unknown')
        type_counts[t_type] = type_counts.get(t_type, 0) + 1
    
    print("\nTransition type statistics:")
    for t_type, count in type_counts.items():
        print(f"  {t_type}: {count}")
    
    # Valid data statistics
    valid_r_c = sum(1 for p in critical_points if p.get('r_c') is not None and not np.isnan(p.get('r_c', np.nan)))
    valid_nu = sum(1 for p in critical_points if p.get('nu') is not None and not np.isnan(p.get('nu', np.nan)))
    
    print(f"\nValid critical points: {valid_r_c}/{len(critical_points)}")
    print(f"Valid critical exponent points: {valid_nu}/{len(critical_points)}")
    
    # Create visualization
    if create_plots and critical_points:
        save_dir_from_file = os.path.dirname(results_file)
        create_comparison_plots(results, save_dir_from_file)
        print(f"\nVisualization charts saved to: {os.path.join(save_dir_from_file, 'comparison_plots')}")
        
    return results

def run_scan(phi_range, theta_range, r_range=(0.01, 0.99, 0.01), 
             kappa_range=None, save_dir=None, n_processes=None, skip_existing=True, 
             parallel_param_points=True, analyze_results=True, power_law_params=None,
             network_params=None, threshold_params=None):
    """
    åœ¨Jupyterä¸­è¿è¡Œæ‰«æçš„ä¾¿æ·å‡½æ•° - æ”¯æŒkappaæ‰«æå’Œå¹‚å¾‹åˆ†å¸ƒå‚æ•°ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰
    
    Parameters:
        phi_range: phièŒƒå›´ï¼Œå¯ä»¥æ˜¯[start, end, step]æˆ–å€¼åˆ—è¡¨
        theta_range: thetaèŒƒå›´ï¼Œå¯ä»¥æ˜¯[start, end, step]æˆ–å€¼åˆ—è¡¨  
        r_range: rèŒƒå›´ï¼Œé»˜è®¤(0.01, 0.99, 0.01)
        kappa_range: kappaèŒƒå›´ï¼Œå¯ä»¥æ˜¯[start, end, step]æˆ–å€¼åˆ—è¡¨ï¼Œé»˜è®¤Noneä½¿ç”¨120
        save_dir: ä¿å­˜ç›®å½•ï¼Œé»˜è®¤Noneä¼šè‡ªåŠ¨åˆ›å»º
        n_processes: è¿›ç¨‹æ•°ï¼Œé»˜è®¤Noneä¼šè‡ªåŠ¨è®¾ç½®
        skip_existing: æ˜¯å¦è·³è¿‡å·²æœ‰ç»“æœï¼Œé»˜è®¤True
        parallel_param_points: æ˜¯å¦å¹¶è¡Œå¤„ç†å‚æ•°ç‚¹ï¼Œé»˜è®¤True
        analyze_results: æ˜¯å¦è‡ªåŠ¨è¿›è¡Œå¯è§†åŒ–åˆ†æï¼Œé»˜è®¤True
        power_law_params: å¹‚å¾‹åˆ†å¸ƒå‚æ•°å­—å…¸ï¼ŒåŒ…å«gamma_pref, k_min_prefç­‰(å¯é€‰)
        network_params: ç½‘ç»œå‚æ•°å­—å…¸ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤å€¼
        threshold_params: é˜ˆå€¼å‚æ•°å­—å…¸ï¼Œå¦‚æœä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤å€¼
        
    Returns:
        æ‰«æç»“æœå­—å…¸
        
    Note:
        ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬ï¼Œæ”¯æŒç¨³æ€ç»“æœç¼“å­˜ï¼Œé¿å…é‡å¤è®¡ç®—
        kappa = k_out_mainstream + k_out_wemediaï¼Œä¼šå¹³å‡åˆ†é…
        ä¾‹å¦‚ï¼škappa=100 â†’ k_out_mainstream=50, k_out_wemedia=50
        æ”¯æŒå¹‚å¾‹åˆ†å¸ƒå‚æ•°é…ç½®ï¼Œå¦‚æœä¸æä¾›power_law_paramsåˆ™ä½¿ç”¨é»˜è®¤æ³Šæ¾åˆ†å¸ƒ
    """
    # ç½‘ç»œå‚æ•°ï¼ˆæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
    if network_params is None:
        network_params = {
            'n_mainstream': 1000,
            'n_wemedia': 1000,
            'n_public': 5000,
            'k_out_mainstream': 60,  # é»˜è®¤å€¼ï¼Œä¼šæ ¹æ®kappaè°ƒæ•´
            'k_out_wemedia': 60,     # é»˜è®¤å€¼ï¼Œä¼šæ ¹æ®kappaè°ƒæ•´
            'k_out_public': 10,
            'max_k': 200,            # åº¦åˆ†å¸ƒä¸Šç•Œ
            'use_original_like_dist': False  # ä¸ä½¿ç”¨åŸå§‹åˆ†å¸ƒ
        }
    
    # é˜ˆå€¼å‚æ•°ï¼ˆæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰ï¼Œå¦åˆ™ä½¿ç”¨é»˜è®¤å€¼ï¼‰
    if threshold_params is None:
        threshold_params = {
            'theta': 0.55,
            'phi': 0.2
        }
    
    print("ğŸš€ å¼€å§‹V3ä¼˜åŒ–ç‰ˆæœ¬ç›¸å˜ç‚¹æ‰«æ...")
    print("âœ¨ æ–°åŠŸèƒ½: ç¨³æ€ç»“æœè‡ªåŠ¨ç¼“å­˜ï¼Œæ”¯æŒä¸­æ–­åç»§ç»­è®¡ç®—")
    print("âœ¨ æ–°åŠŸèƒ½: æ”¯æŒå¹‚å¾‹åˆ†å¸ƒå‚æ•°é…ç½®")
    print(f"phièŒƒå›´: {phi_range}")
    print(f"thetaèŒƒå›´: {theta_range}")
    print(f"rèŒƒå›´: {r_range}")
    
    if kappa_range is not None:
        print(f"kappaèŒƒå›´: {kappa_range}")
        print("ğŸ“ kappaåˆ†é…è¯´æ˜: kappaä¼šå¹³å‡åˆ†é…ç»™ä¸»æµåª’ä½“å’Œè‡ªåª’ä½“")
        print("   ä¾‹å¦‚: kappa=100 â†’ k_out_mainstream=50, k_out_wemedia=50")
    else:
        default_kappa = network_params['k_out_mainstream'] + network_params['k_out_wemedia']
        print(f"ä½¿ç”¨é»˜è®¤kappa: {default_kappa}")
    
    # å¤„ç†å¹‚å¾‹åˆ†å¸ƒå‚æ•°
    if power_law_params is not None:
        print(f"å¹‚å¾‹åˆ†å¸ƒå‚æ•°: {power_law_params}")
        gamma_pref = power_law_params.get('gamma_pref')
        k_min_pref = power_law_params.get('k_min_pref', 1)
        max_k = power_law_params.get('max_k', 200)
        
        if gamma_pref is not None:
            print(f"  ä½¿ç”¨å¹‚å¾‹åˆ†å¸ƒ: Î³={gamma_pref}, k_min={k_min_pref}, k_max={max_k}")
        else:
            print("  æœªé…ç½®å¹‚å¾‹æŒ‡æ•°ï¼Œä½¿ç”¨é»˜è®¤åˆ†å¸ƒ")
    else:
        print("ä½¿ç”¨é»˜è®¤æ³Šæ¾åˆ†å¸ƒ")
    
    # ğŸ”§ ä½¿ç”¨ä¼˜åŒ–ç‰ˆæœ¬å‡½æ•°
    results = quick_scan_critical_points_v3_optimized(
        phi_range=phi_range,
        theta_range=theta_range,
        removal_range=r_range,
        kappa_range=kappa_range,
        network_params=network_params,
        base_threshold_params=threshold_params,
        power_law_params=power_law_params,  # æ–°å¢å¹‚å¾‹åˆ†å¸ƒå‚æ•°
        n_processes=n_processes,
        save_dir=save_dir,
        skip_existing=skip_existing,
        parallel_param_points=parallel_param_points
    )
    
    print("\nâœ… V3ä¼˜åŒ–ç‰ˆæœ¬æ‰«æå®Œæˆï¼")
    
    # è¾“å‡ºç»“æœæ‘˜è¦
    if 'critical_points' in results:
        type_counts = {}
        kappa_counts = {}
        
        for point in results['critical_points']:
            t_type = point.get('transition_type', 'unknown')
            type_counts[t_type] = type_counts.get(t_type, 0) + 1
            
            kappa = point.get('kappa', 'unknown')
            kappa_counts[kappa] = kappa_counts.get(kappa, 0) + 1
        
        print("\nğŸ“Š ç›¸å˜ç±»å‹ç»Ÿè®¡:")
        for t_type, count in type_counts.items():
            print(f"  {t_type}: {count}")
        
        # å¦‚æœæœ‰å¤šä¸ªkappaå€¼ï¼Œæ˜¾ç¤ºkappaåˆ†å¸ƒ
        if len(kappa_counts) > 1:
            print("\nğŸ“Š Kappaåˆ†å¸ƒç»Ÿè®¡:")
            for kappa, count in sorted(kappa_counts.items()):
                if kappa != 'unknown':
                    print(f"  Îº={kappa}: {count} ä¸ªå‚æ•°ç‚¹ (k_out_mainstream={kappa/2}, k_out_wemedia={kappa/2})")
            
            # æŒ‰kappaåˆ†ç»„çš„ç›¸å˜ç±»å‹ç»Ÿè®¡
            print("\nğŸ“Š æŒ‰Kappaåˆ†ç»„çš„ç›¸å˜ç±»å‹:")
            kappa_values = sorted([k for k in kappa_counts.keys() if k != 'unknown'])
            for kappa in kappa_values:
                kappa_points = [p for p in results['critical_points'] if p.get('kappa') == kappa]
                kappa_type_counts = {}
                for point in kappa_points:
                    t_type = point.get('transition_type', 'unknown')
                    kappa_type_counts[t_type] = kappa_type_counts.get(t_type, 0) + 1
                
                print(f"  Îº={kappa} (k_out={kappa/2}+{kappa/2}):")
                for t_type, count in kappa_type_counts.items():
                    print(f"    {t_type}: {count}")
        
        # æ˜¾ç¤ºåˆ†å¸ƒç±»å‹ä¿¡æ¯
        if power_law_params is not None:
            gamma_pref = power_law_params.get('gamma_pref')
            if gamma_pref is not None:
                print(f"\nğŸ“Š åˆ†å¸ƒç±»å‹: å¹‚å¾‹åˆ†å¸ƒ (Î³={gamma_pref})")
            else:
                print(f"\nğŸ“Š åˆ†å¸ƒç±»å‹: æ³Šæ¾åˆ†å¸ƒ")
        else:
            print(f"\nğŸ“Š åˆ†å¸ƒç±»å‹: æ³Šæ¾åˆ†å¸ƒ")
        
        # è‡ªåŠ¨è¿›è¡Œå¯è§†åŒ–åˆ†æ
        if analyze_results:
            save_dir_final = save_dir if save_dir else "."
            # æ ¹æ®æ˜¯å¦æœ‰kappaæ‰«æé€‰æ‹©ç»“æœæ–‡ä»¶å
            if kappa_range is not None:
                results_file = os.path.join(save_dir_final, "scan_results.pkl")
            else:
                results_file = os.path.join(save_dir_final, "scan_results.pkl")
                
            if os.path.exists(results_file):
                print(f"\nğŸ¨ è‡ªåŠ¨è¿›è¡Œå¯è§†åŒ–åˆ†æ...")
                analyze_scan_results(results_file, create_plots=True)
    
    return results
