#!/usr/bin/env python3
"""
å¹‚å¾‹åˆ†å¸ƒæ‰«æç»“æœç»Ÿè®¡åˆ†æ
åˆ†æå¯¹ç§°æ€§ã€æ‹Ÿåˆè´¨é‡ç­‰æŒ‡æ ‡ï¼Œå¹¶ç”Ÿæˆå¯è§†åŒ–
"""

import sys
sys.path.append('.')

import os
import pickle
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

# ==================== ğŸ“ ç¼“å­˜è·¯å¾„é…ç½®åŒºåŸŸ ====================
# ä½¿ç”¨ç»Ÿä¸€é…ç½®ç®¡ç†ç¼“å­˜è·¯å¾„

from src.config import get_cache_subdirs
cache_dirs = get_cache_subdirs('powerlaw')
cache_dir = cache_dirs['base']

print(f"ğŸ”´ å¹‚å¾‹åˆ†å¸ƒåˆ†æ - ç¼“å­˜ç›®å½•: {cache_dir}")

# åŠ¨æ€è®¾ç½®ç¼“å­˜è·¯å¾„åˆ°analyze_symmetryæ¨¡å—
import src.analyze_symmetry as aps

# é‡æ–°é…ç½®æ¨¡å—çš„ç¼“å­˜è·¯å¾„
aps.CACHE_DIR = cache_dir
aps.DATA_CACHE_DIR = cache_dirs['data']
aps.ANALYSIS_CACHE_DIR = cache_dirs['analysis']

# ç¡®ä¿ç›®å½•å­˜åœ¨
os.makedirs(aps.DATA_CACHE_DIR, exist_ok=True)
os.makedirs(aps.ANALYSIS_CACHE_DIR, exist_ok=True)

print(f"   å…³è”é•¿åº¦æ•°æ®: {aps.DATA_CACHE_DIR}")
print(f"   åˆ†æç»“æœ: {aps.ANALYSIS_CACHE_DIR}")
print("=" * 60)

# å¯¼å…¥åˆ†æå‡½æ•°ï¼ˆåœ¨è·¯å¾„é…ç½®ä¹‹åï¼‰
from src.analyze_symmetry import (
    analyze_power_law_symmetry,  # ç›´æ¥è°ƒç”¨ç»˜å›¾å‡½æ•°
    list_cached_results,
    load_correlation_data,  # åŠ è½½å…³è”é•¿åº¦æ•°æ®
    robust_pickle_load,  # ç»Ÿä¸€çš„pickleåŠ è½½å‡½æ•°
    collect_all_scan_results,  # ç»Ÿä¸€çš„æ‰«æç»“æœæ”¶é›†å‡½æ•°
    detect_multiple_peaks,      # ç»Ÿä¸€çš„å¤šå³°æ£€æµ‹å‡½æ•°
    analyze_peak_quality,       # ç»Ÿä¸€çš„å³°å€¼è´¨é‡åˆ†æå‡½æ•°
    filter_valid_results,       # ç»Ÿä¸€çš„ç»“æœè¿‡æ»¤å‡½æ•°
    analyze_results_statistics  # ç»Ÿä¸€çš„ç»“æœç»Ÿè®¡å‡½æ•°
)

# ç›´æ¥ä½¿ç”¨ç»Ÿä¸€çš„å‡½æ•°ï¼Œä¸å†é‡å¤å®šä¹‰

def robust_pickle_load(file_path):
    """
    æ›´robustçš„pickleåŠ è½½ï¼Œå¼ºåŠ›å¤„ç†numpyç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜
    """
    import sys
    import types
    
    # ç¬¬ä¸€æ¬¡å°è¯•ï¼šç›´æ¥åŠ è½½
    try:
        with open(file_path, 'rb') as f:
            return pickle.load(f)
    except Exception as e:
        if "numpy._core" in str(e):
            # ç¬¬äºŒæ¬¡å°è¯•ï¼šåˆ›å»ºnumpy._coreçš„å®Œæ•´mock
            try:
                import numpy
                
                # åˆ›å»º_coreæ¨¡å—ä½œä¸ºnumpy.coreçš„åˆ«å
                if not hasattr(numpy, '_core'):
                    numpy._core = numpy.core
                
                # æ³¨å…¥åˆ°sys.modulesä¸­
                if 'numpy._core' not in sys.modules:
                    sys.modules['numpy._core'] = numpy.core
                
                # ç¡®ä¿æ‰€æœ‰å¿…è¦çš„å­æ¨¡å—éƒ½å­˜åœ¨
                core_submodules = ['multiarray', 'umath', 'numeric', '_internal']
                for submod in core_submodules:
                    if hasattr(numpy.core, submod):
                        if not hasattr(numpy._core, submod):
                            setattr(numpy._core, submod, getattr(numpy.core, submod))
                        
                        # åŒæ—¶æ³¨å…¥sys.modules
                        sys_key = f'numpy._core.{submod}'
                        if sys_key not in sys.modules:
                            sys.modules[sys_key] = getattr(numpy.core, submod)
                
                with open(file_path, 'rb') as f:
                    return pickle.load(f)
                    
            except Exception as e2:
                # ç¬¬ä¸‰æ¬¡å°è¯•ï¼šæ›´æ¿€è¿›çš„monkey patching
                try:
                    # åˆ›å»ºä¸€ä¸ªå®Œæ•´çš„mock _coreæ¨¡å—
                    _core_mock = types.ModuleType('numpy._core')
                    
                    # å¤åˆ¶numpy.coreçš„æ‰€æœ‰å±æ€§åˆ°_core_mock
                    for attr_name in dir(numpy.core):
                        if not attr_name.startswith('__'):
                            attr_value = getattr(numpy.core, attr_name)
                            setattr(_core_mock, attr_name, attr_value)
                    
                    # ç¡®ä¿numpy._coreå­˜åœ¨
                    numpy._core = _core_mock
                    sys.modules['numpy._core'] = _core_mock
                    
                    # å¤„ç†å¯èƒ½çš„å­æ¨¡å—å¼•ç”¨
                    for attr_name in dir(numpy.core):
                        if hasattr(numpy.core, attr_name) and not attr_name.startswith('__'):
                            attr_value = getattr(numpy.core, attr_name)
                            if hasattr(attr_value, '__module__') and 'numpy.core' in str(attr_value.__module__):
                                sys_key = f'numpy._core.{attr_name}'
                                if sys_key not in sys.modules:
                                    sys.modules[sys_key] = attr_value
                    
                    with open(file_path, 'rb') as f:
                        return pickle.load(f)
                        
                except Exception as e3:
                    # æœ€åå°è¯•ï¼špickle protocolå…¼å®¹æ€§
                    try:
                        # å°è¯•ç”¨ä¸åŒçš„pickle protocol
                        with open(file_path, 'rb') as f:
                            # å…ˆè¯»å–æ–‡ä»¶å†…å®¹
                            content = f.read()
                        
                        # å°è¯•ä¿®å¤å†…å®¹ä¸­çš„æ¨¡å—å¼•ç”¨
                        content_str = content.decode('latin1') if isinstance(content, bytes) else str(content)
                        if 'numpy._core' in content_str:
                            content_str = content_str.replace('numpy._core', 'numpy.core')
                            content = content_str.encode('latin1')
                        
                        # é‡æ–°åŠ è½½
                        import io
                        content_io = io.BytesIO(content)
                        return pickle.load(content_io)
                        
                    except:
                        print(f"   ğŸ”§ å¼ºåŠ›ä¿®å¤å¤±è´¥: {file_path}")
                        raise e
        else:
            raise e

def collect_all_scan_results(distribution_filter='powerlaw'):
    """
    æ”¶é›†æ‰«æç»“æœ - ä¸“é—¨é’ˆå¯¹å¹‚å¾‹åˆ†å¸ƒä¼˜åŒ–
    åŒ…æ‹¬è€æ ¼å¼ç¼“å­˜å’Œæ–°åˆ†å±‚ç¼“å­˜ï¼Œä½¿ç”¨robuståŠ è½½æœºåˆ¶
    
    å‚æ•°:
    - distribution_filter: åˆ†å¸ƒç±»å‹è¿‡æ»¤å™¨
      - 'powerlaw': åªæ”¶é›†å¹‚å¾‹åˆ†å¸ƒç»“æœ (gamma_pref!=None) [é»˜è®¤]
      - 'poisson': åªæ”¶é›†æ³Šæ¾åˆ†å¸ƒç»“æœ (gamma_pref=None)
      - None: æ”¶é›†æ‰€æœ‰åˆ†å¸ƒç±»å‹çš„ç»“æœ
    
    è¿”å›:
    - results_list: ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«å®Œæ•´çš„åˆ†æç»“æœ
    """
    
    if distribution_filter == 'powerlaw':
        print("ğŸ”´ æ”¶é›†å¹‚å¾‹åˆ†å¸ƒæ‰«æç»“æœ...")
        print("   ğŸ”´ å¹‚å¾‹åˆ†å¸ƒå…³é”®å‚æ•°: Î³ (å¹‚å¾‹æŒ‡æ•°), k_min (æœ€å°æˆªæ–­åº¦), k_max (æœ€å¤§æˆªæ–­åº¦)")
        print("   ğŸ“Š æ¨¡å‹å‚æ•°: Ï† (å½±å“é˜ˆå€¼), Î¸ (æ„ŸçŸ¥é˜ˆå€¼)")
    elif distribution_filter == 'poisson':
        print("ğŸ”µ æ”¶é›†æ³Šæ¾åˆ†å¸ƒæ‰«æç»“æœ...")
        print("   ä¸»è¦å‚æ•°: Îº (kappa), Ï† (phi), Î¸ (theta)")
    elif distribution_filter is None:
        print("ğŸ“Š æ”¶é›†æ‰€æœ‰åˆ†å¸ƒç±»å‹çš„æ‰«æç»“æœ...")
    else:
        print(f"ğŸ“Š æ”¶é›† {distribution_filter} åˆ†å¸ƒæ‰«æç»“æœ...")
    print("=" * 50)
    
    results_list = []
    failed_files = []
    poisson_count = 0
    powerlaw_count = 0
    
    # === 1. æ‰«æè€æ ¼å¼ç¼“å­˜ (ç›´æ¥åœ¨CACHE_DIRä¸­) ===
    print("ğŸ” æ‰«æè€æ ¼å¼ç¼“å­˜...")
    old_cache_files = [f for f in os.listdir(aps.CACHE_DIR) if f.endswith('.pkl') and f.startswith('result_')]
    
    for cache_file in old_cache_files:
        cache_path = os.path.join(aps.CACHE_DIR, cache_file)
        try:
            result = robust_pickle_load(cache_path)
            
            # è¯†åˆ«åˆ†å¸ƒç±»å‹
            network_params = result.get('network_params', {})
            gamma_pref = network_params.get('gamma_pref')
            
            is_poisson = (gamma_pref is None)
            is_powerlaw = (gamma_pref is not None)
            
            # æ ¹æ®è¿‡æ»¤æ¡ä»¶åˆ¤æ–­æ˜¯å¦æ”¶é›†æ­¤ç»“æœ
            should_collect = False
            
            if distribution_filter is None:
                should_collect = True
            elif distribution_filter == 'poisson' and is_poisson:
                should_collect = True
            elif distribution_filter == 'powerlaw' and is_powerlaw:
                should_collect = True
                
            if should_collect:
                # ç¡®ä¿æœ‰å¿…è¦çš„å­—æ®µ
                if all(key in result for key in ['phi', 'theta', 'asymmetry']):
                    result['source'] = 'old_cache'
                    result['cache_file'] = cache_file
                    result['distribution_type'] = 'poisson' if is_poisson else 'powerlaw'
                    
                    # ä¸ºå¹‚å¾‹åˆ†å¸ƒæ·»åŠ ç‰¹æœ‰å‚æ•°æ ‡è¯†
                    if is_powerlaw:
                        result['k_min_pref'] = network_params.get('k_min_pref', 'unknown')
                        result['max_k'] = network_params.get('max_k', 'unknown')
                        result['gamma_pref'] = gamma_pref
                    
                    # å°è¯•ä»å¯¹åº”çš„correlation_dataä¸­æå–xi_peak (è€æ ¼å¼ç¼“å­˜ä¹Ÿéœ€è¦)
                    try:
                        phi = result['phi']
                        theta = result['theta'] 
                        kappa = result.get('kappa', 120)
                        correlation_data = load_correlation_data(
                            phi=phi, theta=theta, kappa=kappa,
                            gamma_pref=gamma_pref if is_powerlaw else None, 
                            k_min_pref=network_params.get('k_min_pref', 1) if is_powerlaw else 1,
                            max_k=network_params.get('max_k', 200) if is_powerlaw else 200,
                            r_range=(0.1, 0.9), peak_search_points=100
                        )
                        if correlation_data and 'xi_peak' in correlation_data:
                            result['xi_peak'] = correlation_data['xi_peak']
                    except:
                        pass
                    
                    results_list.append(result)
                    
                    if is_poisson:
                        poisson_count += 1
                    else:
                        powerlaw_count += 1
                    
        except Exception as e:
            failed_files.append(cache_file)
            # åªæ˜¾ç¤ºå‰3ä¸ªå¤±è´¥çš„è¯¦ç»†é”™è¯¯ï¼Œé¿å…åˆ·å±
            if len(failed_files) <= 3:
                print(f"âš ï¸ æ— æ³•åŠ è½½ {cache_file}: {e}")
    
    print(f"   è€æ ¼å¼ç¼“å­˜: æ³Šæ¾{poisson_count}ä¸ª + å¹‚å¾‹{powerlaw_count}ä¸ª = æ€»è®¡{poisson_count + powerlaw_count}ä¸ª")
    
    # === 2. æ‰«ææ–°åˆ†å±‚ç¼“å­˜ (ANALYSIS_CACHE_DIR) ===
    print("ğŸ” æ‰«ææ–°åˆ†å±‚ç¼“å­˜...")
    layered_success = 0
    layered_failed = 0
    layered_poisson = 0
    layered_powerlaw = 0
    
    if os.path.exists(aps.ANALYSIS_CACHE_DIR):
        analysis_files = [f for f in os.listdir(aps.ANALYSIS_CACHE_DIR) if f.endswith('.pkl')]
        
        for analysis_file in analysis_files:
            analysis_path = os.path.join(aps.ANALYSIS_CACHE_DIR, analysis_file)
            try:
                cache_data = robust_pickle_load(analysis_path)
                
                result = cache_data.get('analysis_result', {})
                
                # è¯†åˆ«åˆ†å¸ƒç±»å‹
                network_params = result.get('network_params', {})
                gamma_pref = network_params.get('gamma_pref')
                
                is_poisson = (gamma_pref is None)
                is_powerlaw = (gamma_pref is not None)
                
                # æ ¹æ®è¿‡æ»¤æ¡ä»¶åˆ¤æ–­æ˜¯å¦æ”¶é›†æ­¤ç»“æœ
                should_collect = False
                
                if distribution_filter is None:
                    should_collect = True
                elif distribution_filter == 'poisson' and is_poisson:
                    should_collect = True
                elif distribution_filter == 'powerlaw' and is_powerlaw:
                    should_collect = True
                
                if should_collect:
                    if all(key in result for key in ['phi', 'theta', 'asymmetry']):
                        result['source'] = 'layered_cache'
                        result['cache_file'] = analysis_file
                        result['analysis_params'] = cache_data.get('analysis_parameters', {})
                        result['distribution_type'] = 'poisson' if is_poisson else 'powerlaw'
                        
                        # ä¸ºå¹‚å¾‹åˆ†å¸ƒæ·»åŠ ç‰¹æœ‰å‚æ•°æ ‡è¯†
                        if is_powerlaw:
                            result['k_min_pref'] = network_params.get('k_min_pref', 'unknown')
                            result['max_k'] = network_params.get('max_k', 'unknown')
                            result['gamma_pref'] = gamma_pref
                        
                        # å°è¯•ä»å¯¹åº”çš„correlation_dataä¸­æå–xi_peak
                        try:
                            phi = result['phi']
                            theta = result['theta'] 
                            kappa = result.get('kappa', 120)  # å¹‚å¾‹åˆ†å¸ƒå¯èƒ½æ²¡æœ‰kappa
                            correlation_data = load_correlation_data(
                                phi=phi, theta=theta, kappa=kappa,
                                gamma_pref=gamma_pref, 
                                k_min_pref=network_params.get('k_min_pref', 1),
                                max_k=network_params.get('max_k', 200),
                                r_range=(0.1, 0.9), peak_search_points=100
                            )
                            if correlation_data and 'xi_peak' in correlation_data:
                                result['xi_peak'] = correlation_data['xi_peak']
                        except:
                            pass
                        
                        results_list.append(result)
                        layered_success += 1
                        
                        if is_poisson:
                            layered_poisson += 1
                        else:
                            layered_powerlaw += 1
                        
            except Exception as e:
                layered_failed += 1
                failed_files.append(analysis_file)
                # åªæ˜¾ç¤ºå‰3ä¸ªå¤±è´¥çš„è¯¦ç»†é”™è¯¯
                if layered_failed <= 3:
                    print(f"âš ï¸ æ— æ³•åŠ è½½ {analysis_file}: {e}")
    
    print(f"   æ–°åˆ†å±‚ç¼“å­˜: æ³Šæ¾{layered_poisson}ä¸ª + å¹‚å¾‹{layered_powerlaw}ä¸ª = æ€»è®¡{layered_success}ä¸ª")
    
    if failed_files:
        print(f"   âš ï¸ æ€»å…± {len(failed_files)} ä¸ªæ–‡ä»¶åŠ è½½å¤±è´¥ (é€šå¸¸æ˜¯numpyç‰ˆæœ¬å…¼å®¹æ€§é—®é¢˜)")
        if len(failed_files) > 6:
            print(f"      æ˜¾ç¤ºäº†å‰6ä¸ªé”™è¯¯ï¼Œå…¶ä½™ {len(failed_files)-6} ä¸ªç±»ä¼¼")
    
    # æœ€ç»ˆç»Ÿè®¡
    total_poisson = poisson_count + layered_poisson
    total_powerlaw = powerlaw_count + layered_powerlaw
    total_all = len(results_list)
    
    print(f"\nğŸ“ˆ æ”¶é›†ç»“æœç»Ÿè®¡:")
    print(f"   ğŸ”µ æ³Šæ¾åˆ†å¸ƒ: {total_poisson} ä¸ª")
    print(f"   ğŸ”´ å¹‚å¾‹åˆ†å¸ƒ: {total_powerlaw} ä¸ª")
    print(f"   âœ… æ€»è®¡: {total_all} ä¸ªç»“æœ")
    
    if distribution_filter:
        filter_names = {'poisson': 'æ³Šæ¾åˆ†å¸ƒ', 'powerlaw': 'å¹‚å¾‹åˆ†å¸ƒ'}
        filter_name = filter_names.get(distribution_filter, distribution_filter)
        print(f"   ğŸ¯ è¿‡æ»¤å™¨: ä»…æ”¶é›†{filter_name}ç»“æœ")
    
    return results_list

def collect_all_powerlaw_results():
    """
    ä¸»è¦å‡½æ•°ï¼šæ”¶é›†æ‰€æœ‰å¹‚å¾‹åˆ†å¸ƒç»“æœ
    """
    return collect_all_scan_results(distribution_filter='powerlaw')

def collect_all_poisson_results():
    """
    å…¼å®¹æ€§å‡½æ•°ï¼šæ”¶é›†æ‰€æœ‰æ³Šæ¾åˆ†å¸ƒç»“æœ
    """
    return collect_all_scan_results(distribution_filter='poisson')

def plot_from_cache_data(case):
    """
    ä»ç¼“å­˜æ•°æ®ç»˜åˆ¶ä¸“ä¸šçš„å¹‚å¾‹åˆ†æå›¾è¡¨ï¼Œå‘analyze_power_law_symmetryçœ‹é½
    
    å‚æ•°:
    - case: åŒ…å«å‚æ•°çš„å­—å…¸ {'phi', 'theta', 'kappa', 'gamma_pref', 'k_min_pref', 'max_k'}
    
    è¿”å›:
    - True: æˆåŠŸä»ç¼“å­˜ç»˜å›¾
    - False: ç¼“å­˜æ•°æ®ä¸è¶³ï¼Œéœ€è¦é‡æ–°è®¡ç®—
    """
    
    phi = case['phi']
    theta = case['theta'] 
    kappa = int(case.get('kappa', 120))  # å¹‚å¾‹åˆ†å¸ƒå¯èƒ½æ²¡æœ‰kappaæˆ–ä½¿ç”¨é»˜è®¤å€¼
    gamma_pref = case.get('gamma_pref')
    k_min_pref = case.get('k_min_pref', 1)
    max_k = case.get('max_k', 200)
    
    # å°è¯•åŠ è½½å…³è”é•¿åº¦æ•°æ® (ç¬¬1å±‚ç¼“å­˜)
    correlation_data = load_correlation_data(
        phi=phi, theta=theta, kappa=kappa,
        gamma_pref=gamma_pref,  # å¹‚å¾‹åˆ†å¸ƒ
        k_min_pref=k_min_pref,
        max_k=max_k,
        use_original_like_dist=case.get('use_original_like_dist', False),
        r_range=(0.1, 0.9), peak_search_points=100
    )
    
    if correlation_data is None:
        return False
    
    # æå–ç»˜å›¾æ•°æ®
    r_values = correlation_data.get('r_values')
    xi_values = correlation_data.get('xi_values') 
    r_peak = correlation_data.get('r_peak')
    xi_peak = correlation_data.get('xi_peak')
    
    if r_values is None or xi_values is None:
        return False
    
    print(f"    ğŸ“‚ ä½¿ç”¨ç¼“å­˜æ•°æ®: {len(r_values)}ä¸ªæ•°æ®ç‚¹, r_peak={r_peak:.4f}, Î¾_max={xi_peak:.2f}")
    
    # ğŸ” å¤šå³°æ£€æµ‹å’Œè´¨é‡åˆ†æ (ä½¿ç”¨æ›´æ•æ„Ÿçš„é˜ˆå€¼0.55)
    is_multi_peak, peak_info = detect_multiple_peaks(r_values, xi_values, r_peak, prominence_threshold=0.55)
    quality_score, quality_info = analyze_peak_quality(r_values, xi_values, r_peak, window_size=0.05)
    
    # è¾“å‡ºå³°å€¼è´¨é‡ä¿¡æ¯
    if is_multi_peak:
        problem_type = peak_info.get('problem_type', 'unknown')
        n_secondary = len(peak_info.get('secondary_peaks', []))
        print(f"    âš ï¸ å¤šå³°æ£€æµ‹: {problem_type} (æ€»å³°æ•°: {peak_info['n_peaks']}, æ¬¡å³°æ•°: {n_secondary})")
        
        # æ˜¾ç¤ºæ¬¡å³°è¯¦ç»†ä¿¡æ¯
        if n_secondary > 0 and 'secondary_peak_details' in peak_info:
            secondary_details = peak_info['secondary_peak_details']
            if secondary_details:
                detail_strs = []
                for detail in secondary_details:
                    pos = detail['position']
                    height = detail['height']
                    ratio = detail['ratio_to_main']
                    detail_strs.append(f"r={pos:.3f}(Î¾={height:.1f}, {ratio:.1%})")
                print(f"    ğŸ“ æ¬¡å³°è¯¦æƒ…: {detail_strs}")
                threshold = peak_info.get('detection_threshold', 0.6)
                print(f"    ğŸ” æ£€æµ‹é˜ˆå€¼: {threshold:.1%} (é™ä½å¯æ£€æµ‹æ›´å¤šæ¬¡å³°)")
    else:
        print(f"    âœ… å•å³°æ£€æµ‹: æ— æ˜¾è‘—æ¬¡å³° (æ€»å³°æ•°: {peak_info['n_peaks']}, ç±»å‹: {peak_info.get('problem_type', 'unknown')})")
    
    print(f"    ğŸ“Š å³°å€¼è´¨é‡: {quality_score:.3f} (å•è°ƒæ€§: L={quality_info['left_monotonic']}, R={quality_info['right_monotonic']}, å¹³æ»‘æ€§: {peak_info.get('local_smoothness', 'unknown')})")
    
    # ä¸“ä¸šçš„å¹‚å¾‹åˆ†æå‚æ•°ï¼ˆå‚è€ƒanalyze_power_law_symmetryï¼‰
    delta_r = 0.15  # åˆ†æçª—å£åŠå¾„
    exclusion_radius = 0.015  # æ’é™¤ä¸´ç•Œç‚¹é™„è¿‘çš„åŠå¾„
    n_points = 30  # æ¯ä¾§æ‹Ÿåˆç‚¹æ•°
    xi_bounds = (1, 1000)  # å…³è”é•¿åº¦è¿‡æ»¤èŒƒå›´
    
    # ç”Ÿæˆå·¦å³ä¸¤ä¾§çš„åˆ†æç‚¹
    r_left = np.linspace(r_peak - delta_r, r_peak - exclusion_radius, n_points)
    r_right = np.linspace(r_peak + exclusion_radius, r_peak + delta_r, n_points)
    
    # ç¡®ä¿rå€¼åœ¨åˆç†èŒƒå›´å†…
    r_left = r_left[r_left > 0.05]
    r_right = r_right[r_right < 0.95]
    
    # ä»åŸå§‹æ•°æ®ä¸­æ’å€¼å¾—åˆ°åˆ†æç‚¹çš„å…³è”é•¿åº¦
    from scipy.interpolate import interp1d
    
    try:
        # åˆ›å»ºæ’å€¼å‡½æ•°
        interp_func = interp1d(r_values, xi_values, kind='cubic', fill_value='extrapolate')
        
        # æ’å€¼å¾—åˆ°åˆ†æç‚¹çš„å…³è”é•¿åº¦
        xi_left = interp_func(r_left)
        xi_right = interp_func(r_right)
        
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        valid_left = ~np.isnan(xi_left) & (xi_left > 0)
        valid_right = ~np.isnan(xi_right) & (xi_right > 0)
        
        r_left_valid = r_left[valid_left]
        xi_left_valid = xi_left[valid_left]
        r_right_valid = r_right[valid_right]
        xi_right_valid = xi_right[valid_right]
        
        # è®¡ç®—è·ç¦»ä¸´ç•Œç‚¹çš„è·ç¦»
        dr_left = np.abs(r_left_valid - r_peak)
        dr_right = np.abs(r_right_valid - r_peak)
        
        # å¹‚å¾‹æ‹Ÿåˆå‡½æ•°
        def fit_power_law(dr, xi):
            from scipy import stats
            xi_min, xi_max = xi_bounds
            mask = (xi < xi_max) & (xi > xi_min) & (dr > 0)
            if np.sum(mask) < 3:
                return None, None, None
            
            log_dr = np.log10(dr[mask])
            log_xi = np.log10(xi[mask])
            
            valid_log_mask = np.isfinite(log_dr) & np.isfinite(log_xi)
            if np.sum(valid_log_mask) < 3:
                return None, None, None
                
            log_dr = log_dr[valid_log_mask]
            log_xi = log_xi[valid_log_mask]
            
            slope, intercept, r_value, p_value, std_err = stats.linregress(log_dr, log_xi)
            nu = -slope  # ä¸´ç•ŒæŒ‡æ•°
            
            return nu, r_value**2, len(log_dr)
        
        # æ‹Ÿåˆå·¦å³ä¸¤ä¾§
        nu_left, r2_left, n_left = fit_power_law(dr_left, xi_left_valid)
        nu_right, r2_right, n_right = fit_power_law(dr_right, xi_right_valid)
        
        # å®‰å…¨çš„æ ¼å¼åŒ–è¾“å‡º
        left_nu_str = f"{nu_left:.3f}" if nu_left is not None else "N/A"
        left_r2_str = f"{r2_left:.3f}" if r2_left is not None else "N/A"
        right_nu_str = f"{nu_right:.3f}" if nu_right is not None else "N/A"
        right_r2_str = f"{r2_right:.3f}" if r2_right is not None else "N/A"
        
        print(f"    ğŸ”¬ å¹‚å¾‹æ‹Ÿåˆ: å·¦Î½={left_nu_str}(RÂ²={left_r2_str}), å³Î½={right_nu_str}(RÂ²={right_r2_str})")
        
    except Exception as e:
        print(f"    âš ï¸ å¹‚å¾‹åˆ†æå¤±è´¥: {e}")
        nu_left = nu_right = r2_left = r2_right = None
        dr_left = dr_right = np.array([])
        xi_left_valid = xi_right_valid = np.array([])
    
    # ç»˜åˆ¶ä¸“ä¸šçš„3ä¸ªå­å›¾
    fig, axes = plt.subplots(1, 3, figsize=(15, 5))
    
    # å·¦å›¾: å®Œæ•´å…³è”é•¿åº¦æ›²çº¿
    axes[0].plot(r_values, xi_values, 'b-o', markersize=4, alpha=0.7, label='Correlation length')
    axes[0].axvline(r_peak, color='red', linestyle='--', alpha=0.8, label=f'r_peak={r_peak:.4f}')
    
    # æ ‡å‡ºæ’é™¤åŒºåŸŸ
    axes[0].axvspan(r_peak - exclusion_radius, r_peak + exclusion_radius, 
                    alpha=0.2, color='red', label=f'Exclusion (Â±{exclusion_radius:.3f})')
    
    # æ ‡æ³¨æ¬¡å³°ä½ç½®
    secondary_peaks = peak_info.get('secondary_peaks', [])
    if is_multi_peak and len(secondary_peaks) > 0:
        for peak_idx in secondary_peaks:
            # æ‰¾åˆ°è¿™ä¸ªå³°å€¼åœ¨æ•°ç»„ä¸­çš„ä½ç½®
            if peak_idx < len(r_values):
                r_secondary = r_values[peak_idx]
                xi_secondary = xi_values[peak_idx]
                axes[0].axvline(r_secondary, color='orange', linestyle=':', alpha=0.8)
                axes[0].plot(r_secondary, xi_secondary, 'o', color='orange', markersize=8, alpha=0.8)
        axes[0].axvline(np.nan, color='orange', linestyle=':', label=f'Secondary peaks ({len(secondary_peaks)})')
    
    axes[0].set_xlabel('Removal Ratio r', fontweight='bold')
    axes[0].set_ylabel('Correlation Length Î¾', fontweight='bold')
    
    # æ ‡é¢˜åŒ…å«å¤šå³°ä¿¡æ¯
    if is_multi_peak:
        title = f'Correlation Length Curve (Multi-peak âš ï¸)'
    else:
        title = f'Correlation Length Curve (Single-peak âœ“)'
    axes[0].set_title(title, fontweight='bold')
    axes[0].legend(fontsize=8)
    axes[0].grid(True, alpha=0.3)
    
    # ä¸­å›¾: çº¿æ€§å°ºåº¦æ‹Ÿåˆåˆ†æ
    if len(dr_left) > 0:
        axes[1].plot(dr_left, xi_left_valid, 'bo', label='Left side', markersize=6, alpha=0.8)
    if len(dr_right) > 0:
        axes[1].plot(dr_right, xi_right_valid, 'ro', label='Right side', markersize=6, alpha=0.8)
    
    axes[1].set_xlabel('|r - r_peak|', fontweight='bold')
    axes[1].set_ylabel('Correlation Length Î¾', fontweight='bold')
    axes[1].set_title('Linear Scale Analysis', fontweight='bold')
    axes[1].legend(fontsize=8)
    axes[1].grid(True, alpha=0.3)
    
    # å³å›¾: å¯¹æ•°å°ºåº¦å¹‚å¾‹æ‹Ÿåˆ
    if len(dr_left) > 0:
        axes[2].loglog(dr_left, xi_left_valid, 'bo', label='Left side', markersize=6, alpha=0.8)
    if len(dr_right) > 0:
        axes[2].loglog(dr_right, xi_right_valid, 'ro', label='Right side', markersize=6, alpha=0.8)
    
    # æ·»åŠ æ‹Ÿåˆçº¿
    if nu_left is not None and r2_left is not None and len(dr_left) > 0:
        dr_fit_range = np.logspace(np.log10(dr_left.min()), np.log10(dr_left.max()), 50)
        xi_fit_left = dr_fit_range**(-nu_left) * (xi_left_valid[0] / dr_left[0]**(-nu_left))
        axes[2].loglog(dr_fit_range, xi_fit_left, 'b--', alpha=0.8, linewidth=2,
                      label=f'Left: Î½={nu_left:.3f} (RÂ²={r2_left:.3f})')
    
    if nu_right is not None and r2_right is not None and len(dr_right) > 0:
        dr_fit_range = np.logspace(np.log10(dr_right.min()), np.log10(dr_right.max()), 50)
        xi_fit_right = dr_fit_range**(-nu_right) * (xi_right_valid[0] / dr_right[0]**(-nu_right))
        axes[2].loglog(dr_fit_range, xi_fit_right, 'r--', alpha=0.8, linewidth=2,
                      label=f'Right: Î½={nu_right:.3f} (RÂ²={r2_right:.3f})')
    
    axes[2].set_xlabel('|r - r_peak|', fontweight='bold')
    axes[2].set_ylabel('Correlation Length Î¾', fontweight='bold')
    axes[2].set_title('Power-law Scaling', fontweight='bold')
    axes[2].legend(fontsize=8)
    axes[2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.show()
    
    # è¾“å‡ºæœ€ç»ˆç»“æœæ€»ç»“
    peak_status = "å¤šå³°âš ï¸" if is_multi_peak else "å•å³°âœ“"
    quality_status = "é«˜è´¨é‡âœ“" if quality_score > 0.6 else "ä½è´¨é‡âš ï¸"
    
    # æ˜¾ç¤ºå¹‚å¾‹åˆ†å¸ƒå‚æ•°
    if gamma_pref is not None:
        print(f"    âœ… å¹‚å¾‹åˆ†å¸ƒåˆ†æå›¾è¡¨å·²ç”Ÿæˆ - Ï†={phi:.3f}, Î¸={theta:.3f}, Î³={gamma_pref:.2f}, kâˆˆ[{k_min_pref},{max_k}]")
    else:
        print(f"    âœ… ä¸“ä¸šåˆ†æå›¾è¡¨å·²ç”Ÿæˆ - Ï†={phi:.3f}, Î¸={theta:.3f}, Îº={kappa}")
    print(f"    ğŸ“ˆ ç»¼åˆè¯„ä¼°: {peak_status}, {quality_status} (è´¨é‡åˆ†: {quality_score:.3f})")
    return True

def detect_multiple_peaks(r_values, xi_values, main_peak_r, prominence_threshold=0.6):
    """
    æç®€åŒå³°æ£€æµ‹ - åªæ£€æµ‹æ˜æ˜¾çš„åŒå³°ç»“æ„
    
    åŸåˆ™ï¼š
    1. ä¸ä½¿ç”¨å…³è”é•¿åº¦é˜ˆå€¼
    2. åªæ¥å—æ°å¥½2ä¸ªå³°çš„æƒ…å†µ
    3. åŸºäºç®€å•çš„å±€éƒ¨æœ€å¤§å€¼æ£€æµ‹
    
    å‚æ•°:
    - r_values: removal ratioæ•°ç»„
    - xi_values: å…³è”é•¿åº¦æ•°ç»„  
    - main_peak_r: ä¸»å³°ä½ç½®
    - prominence_threshold: æœªä½¿ç”¨ï¼Œä¿æŒæ¥å£å…¼å®¹
    
    è¿”å›:
    - is_double_peak: æ˜¯å¦ä¸ºåŒå³°
    - peak_info: å³°å€¼ä¿¡æ¯å­—å…¸
    """
    import numpy as np
    
    # === ç®€å•çš„å±€éƒ¨æœ€å¤§å€¼æ£€æµ‹ ===
    peaks = []
    peak_heights = []
    
    # å¯»æ‰¾å±€éƒ¨æœ€å¤§å€¼ï¼šæ¯”å·¦å³é‚»å±…éƒ½é«˜
    for i in range(1, len(xi_values) - 1):
        if xi_values[i] > xi_values[i-1] and xi_values[i] > xi_values[i+1]:
            peaks.append(i)
            peak_heights.append(xi_values[i])
    
    peaks = np.array(peaks)
    peak_heights = np.array(peak_heights)
    
    # === ä¸¥æ ¼çš„åŒå³°åˆ¤æ–­ + å™ªå£°è¿‡æ»¤ ===
    is_double_peak = False
    
    if len(peaks) == 2:
        # æ£€æŸ¥æ¬¡å³°æ˜¯å¦ä¸ºå¹³å°å™ªå£°
        peak_heights_sorted = sorted(peak_heights, reverse=True)
        main_peak_height = peak_heights_sorted[0]
        secondary_peak_height = peak_heights_sorted[1]
        
        # è¿‡æ»¤æ¡ä»¶ï¼šæ¬¡å³°ä¸èƒ½æ˜¯å¹³å°å™ªå£°ï¼ˆÎ¾=1.0é™„è¿‘ï¼‰
        if secondary_peak_height > 1.2:  # æ¬¡å³°å¿…é¡»å¤§äº1.5
            is_double_peak = True
    
    # æ„å»ºç®€æ´ä¿¡æ¯
    peak_info = {
        'n_peaks': len(peaks),
        'is_double_peak': is_double_peak,
        'peak_positions': r_values[peaks].tolist() if len(peaks) > 0 else [],
        'peak_heights': peak_heights.tolist() if len(peaks) > 0 else [],
        'method': 'simple_local_maxima_with_noise_filter',
        'filtered_reason': 'secondary_peak_too_low' if len(peaks) == 2 and not is_double_peak else None
    }
    
    return is_double_peak, peak_info

def analyze_peak_quality(r_values, xi_values, main_peak_r, window_size=0.05):
    """
    åˆ†æä¸»å³°å‘¨å›´çš„è´¨é‡ï¼ˆå¹³æ»‘åº¦ã€å•è°ƒæ€§ï¼‰
    
    å‚æ•°:
    - r_values, xi_values: æ•°æ®
    - main_peak_r: ä¸»å³°ä½ç½®  
    - window_size: åˆ†æçª—å£å¤§å°
    
    è¿”å›:
    - quality_score: è´¨é‡è¯„åˆ† (0-1, è¶Šé«˜è¶Šå¥½)
    - quality_info: è¯¦ç»†è´¨é‡ä¿¡æ¯
    """
    
    # æ‰¾åˆ°ä¸»å³°å‘¨å›´çš„çª—å£
    main_peak_idx = np.argmin(np.abs(r_values - main_peak_r))
    
    # å®šä¹‰å·¦å³çª—å£
    left_mask = (r_values >= main_peak_r - window_size) & (r_values < main_peak_r)
    right_mask = (r_values > main_peak_r) & (r_values <= main_peak_r + window_size)
    
    quality_info = {
        'left_monotonic': False,
        'right_monotonic': False,  
        'left_smoothness': 0.0,
        'right_smoothness': 0.0,
        'peak_sharpness': 0.0
    }
    
    quality_score = 0.0
    
    if np.sum(left_mask) > 2 and np.sum(right_mask) > 2:
        # æ£€æŸ¥å·¦ä¾§å•è°ƒæ€§ï¼ˆåº”è¯¥å•è°ƒé€’å¢åˆ°å³°å€¼ï¼‰
        left_xi = xi_values[left_mask]
        left_r = r_values[left_mask]
        if len(left_xi) > 1:
            left_diffs = np.diff(left_xi)
            increasing_ratio = np.sum(left_diffs > 0) / len(left_diffs)
            quality_info['left_monotonic'] = increasing_ratio > 0.7
            
        # æ£€æŸ¥å³ä¾§å•è°ƒæ€§ï¼ˆåº”è¯¥å•è°ƒé€’å‡ç¦»å¼€å³°å€¼ï¼‰  
        right_xi = xi_values[right_mask]
        right_r = r_values[right_mask]
        if len(right_xi) > 1:
            right_diffs = np.diff(right_xi)
            decreasing_ratio = np.sum(right_diffs < 0) / len(right_diffs)
            quality_info['right_monotonic'] = decreasing_ratio > 0.7
            
        # è®¡ç®—å¹³æ»‘åº¦ï¼ˆäºŒé˜¶å¯¼æ•°çš„å˜åŒ–ï¼‰
        if len(left_xi) > 2:
            left_second_diff = np.diff(left_xi, n=2)
            quality_info['left_smoothness'] = 1.0 / (1.0 + np.std(left_second_diff))
            
        if len(right_xi) > 2:
            right_second_diff = np.diff(right_xi, n=2)
            quality_info['right_smoothness'] = 1.0 / (1.0 + np.std(right_second_diff))
        
        # å³°å€¼å°–é”åº¦ï¼ˆå³°å€¼ç›¸å¯¹äºä¸¤ä¾§çš„çªå‡ºç¨‹åº¦ï¼‰
        peak_height = xi_values[main_peak_idx]
        left_avg = np.mean(left_xi) if len(left_xi) > 0 else peak_height
        right_avg = np.mean(right_xi) if len(right_xi) > 0 else peak_height
        side_avg = (left_avg + right_avg) / 2
        if side_avg > 0:
            quality_info['peak_sharpness'] = (peak_height - side_avg) / peak_height
        
        # ç»¼åˆè´¨é‡è¯„åˆ†
        quality_score = (
            0.3 * (1.0 if quality_info['left_monotonic'] else 0.0) +
            0.3 * (1.0 if quality_info['right_monotonic'] else 0.0) +
            0.2 * min(quality_info['left_smoothness'], 1.0) +
            0.2 * min(quality_info['right_smoothness'], 1.0)
        )
    
    return quality_score, quality_info

def filter_valid_results(results_list):
    """
    è¿‡æ»¤å‡ºæœ‰æ•ˆçš„ç»“æœï¼Œæ’é™¤ä¸åˆç†çš„æ•°æ®
    
    å‚æ•°:
    - results_list: åŸå§‹ç»“æœåˆ—è¡¨
    
    è¿”å›:
    - filtered_list: è¿‡æ»¤åçš„ç»“æœåˆ—è¡¨
    - filter_stats: è¿‡æ»¤ç»Ÿè®¡ä¿¡æ¯
    """
    
    print("\nğŸ” æ•°æ®è´¨é‡ç­›é€‰...")
    print("=" * 50)
    
    original_count = len(results_list)
    filtered_list = []
    filter_reasons = {
        'missing_fields': 0,
        'infinite_asymmetry': 0,
        'negative_r2': 0,
        'unreasonable_nu': 0,
        'invalid_peak': 0,
        'parameter_constraint': 0
    }
    
    for result in results_list:
        # 1. æ£€æŸ¥å¿…è¦å­—æ®µ
        required_fields = ['phi', 'theta', 'kappa', 'asymmetry', 'nu_avg', 'r2_left', 'r2_right']
        if not all(result.get(field) is not None for field in required_fields):
            filter_reasons['missing_fields'] += 1
            continue
        
        # 2. æ£€æŸ¥ä¸å¯¹ç§°æ€§æ˜¯å¦ä¸ºæ— ç©·å¤§æˆ–NaN
        asymmetry = result.get('asymmetry')
        if asymmetry is None or np.isinf(asymmetry) or np.isnan(asymmetry):
            filter_reasons['infinite_asymmetry'] += 1
            continue
        
        # 3. æ£€æŸ¥RÂ²æ˜¯å¦åˆç†
        r2_left = result.get('r2_left')
        r2_right = result.get('r2_right')
        if r2_left is None or r2_right is None or r2_left < 0 or r2_right < 0:
            filter_reasons['negative_r2'] += 1
            continue
        
        # 4. æ£€æŸ¥ä¸´ç•ŒæŒ‡æ•°æ˜¯å¦åˆç† (é€šå¸¸åœ¨0.1-2.0ä¹‹é—´)
        nu_avg = result.get('nu_avg')
        if nu_avg is None or nu_avg <= 0 or nu_avg > 3.0:
            filter_reasons['unreasonable_nu'] += 1
            continue
        
        # 5. æ£€æŸ¥å³°å€¼ä½ç½®æ˜¯å¦åˆç†
        r_peak = result.get('r_peak')
        if r_peak is None or r_peak <= 0 or r_peak >= 1:
            filter_reasons['invalid_peak'] += 1
            continue
        
        # 6. æ£€æŸ¥å‚æ•°çº¦æŸ (theta > phi)
        phi = result.get('phi')
        theta = result.get('theta')
        if phi is None or theta is None or theta <= phi:
            filter_reasons['parameter_constraint'] += 1
            continue
        
        # é€šè¿‡æ‰€æœ‰æ£€æŸ¥
        filtered_list.append(result)
    
    filtered_count = len(filtered_list)
    removed_count = original_count - filtered_count
    
    print(f"ğŸ“Š ç­›é€‰ç»“æœ:")
    print(f"   åŸå§‹æ ·æœ¬æ•°: {original_count}")
    print(f"   æœ‰æ•ˆæ ·æœ¬æ•°: {filtered_count}")
    print(f"   ç§»é™¤æ ·æœ¬æ•°: {removed_count} ({removed_count/original_count*100:.1f}%)")
    
    if removed_count > 0:
        print(f"\nâŒ ç§»é™¤åŸå› ç»Ÿè®¡:")
        for reason, count in filter_reasons.items():
            if count > 0:
                reason_names = {
                    'missing_fields': 'ç¼ºå°‘å¿…è¦å­—æ®µ',
                    'infinite_asymmetry': 'ä¸å¯¹ç§°æ€§ä¸ºæ— ç©·å¤§/NaN',
                    'negative_r2': 'RÂ²ä¸ºè´Ÿæ•°/ç¼ºå¤±',
                    'unreasonable_nu': 'ä¸´ç•ŒæŒ‡æ•°ä¸åˆç†',
                    'invalid_peak': 'å³°å€¼ä½ç½®ä¸åˆç†',
                    'parameter_constraint': 'å‚æ•°çº¦æŸè¿å'
                }
                print(f"   {reason_names.get(reason, reason)}: {count}")
    
    return filtered_list, filter_reasons

def analyze_results_statistics(results_list):
    """
    ç»Ÿè®¡åˆ†ææ”¶é›†åˆ°çš„ç»“æœ
    
    å‚æ•°:
    - results_list: ç»“æœåˆ—è¡¨
    
    è¿”å›:
    - stats_df: ç»Ÿè®¡ç»“æœDataFrame
    """
    
    print("\nğŸ“ˆ ç»Ÿè®¡åˆ†æ...")
    print("=" * 50)
    
    # å…ˆè¿›è¡Œæ•°æ®è´¨é‡ç­›é€‰
    filtered_results, filter_stats = filter_valid_results(results_list)
    
    # è½¬æ¢ä¸ºDataFrameä¾¿äºåˆ†æ
    data_rows = []
    
    for result in filtered_results:
        row = {
            'phi': result.get('phi'),
            'theta': result.get('theta'), 
            'asymmetry': result.get('asymmetry'),
            'nu_avg': result.get('nu_avg'),
            'nu_left': result.get('nu_left'),
            'nu_right': result.get('nu_right'),
            'r2_left': result.get('r2_left'),
            'r2_right': result.get('r2_right'),
            'r_peak': result.get('r_peak'),
            'xi_peak': result.get('xi_peak'),  # æ·»åŠ å…³è”é•¿åº¦å³°å€¼
            'source': result.get('source'),
            'cache_file': result.get('cache_file'),
            'distribution_type': result.get('distribution_type', 'unknown')
        }
        
        # æ·»åŠ åˆ†å¸ƒç‰¹æœ‰çš„å‚æ•°
        if result.get('distribution_type') == 'poisson':
            row['kappa'] = result.get('kappa')
            row['k_min_pref'] = None
            row['max_k'] = None
            row['gamma_pref'] = None
        elif result.get('distribution_type') == 'powerlaw':
            row['kappa'] = result.get('kappa', 120)  # å¹‚å¾‹åˆ†å¸ƒå¯èƒ½æœ‰å›ºå®šçš„kappa
            row['k_min_pref'] = result.get('k_min_pref')
            row['max_k'] = result.get('max_k')
            row['gamma_pref'] = result.get('gamma_pref')
        else:
            # å…¼å®¹è€æ•°æ®
            row['kappa'] = result.get('kappa')
            row['k_min_pref'] = result.get('k_min_pref')
            row['max_k'] = result.get('max_k')
            row['gamma_pref'] = result.get('gamma_pref')
        
        # è®¡ç®—å¹³å‡RÂ²
        if row['r2_left'] is not None and row['r2_right'] is not None:
            row['r2_avg'] = (row['r2_left'] + row['r2_right']) / 2
        else:
            row['r2_avg'] = None
            
        data_rows.append(row)
    
    df = pd.DataFrame(data_rows)
    
    # åŸºæœ¬ç»Ÿè®¡
    print(f"\nğŸ“Š æœ‰æ•ˆæ•°æ®ç»Ÿè®¡:")
    print(f"   æœ‰æ•ˆæ ·æœ¬æ•°: {len(df)}")
    print(f"   æœ‰æ•ˆasymmetry: {df['asymmetry'].notna().sum()}")
    print(f"   æœ‰æ•ˆRÂ²: {df['r2_avg'].notna().sum()}")
    print(f"   æœ‰æ•ˆå…³è”é•¿åº¦å³°å€¼: {df['r_peak'].notna().sum()}")
    
    if len(df) > 0:
        # å¯¹ç§°æ€§ç»Ÿè®¡
        valid_asymmetry = df['asymmetry'].dropna()
        if len(valid_asymmetry) > 0:
            print(f"\nğŸ¯ å¯¹ç§°æ€§ç»Ÿè®¡:")
            print(f"   æœ€å°ä¸å¯¹ç§°æ€§: {valid_asymmetry.min():.1%}")
            print(f"   æœ€å¤§ä¸å¯¹ç§°æ€§: {valid_asymmetry.max():.1%}")
            print(f"   å¹³å‡ä¸å¯¹ç§°æ€§: {valid_asymmetry.mean():.1%}")
            print(f"   ä¸­ä½ä¸å¯¹ç§°æ€§: {valid_asymmetry.median():.1%}")
            
            # å¯¹ç§°æ€§åˆ†å¸ƒ
            excellent = (valid_asymmetry < 0.05).sum()
            good = ((valid_asymmetry >= 0.05) & (valid_asymmetry < 0.10)).sum()
            moderate = ((valid_asymmetry >= 0.10) & (valid_asymmetry < 0.20)).sum()
            poor = (valid_asymmetry >= 0.20).sum()
            
            print(f"   å¯¹ç§°æ€§åˆ†å¸ƒ:")
            print(f"     ä¼˜ç§€ (<5%): {excellent} ({excellent/len(valid_asymmetry)*100:.1f}%)")
            print(f"     è‰¯å¥½ (5%-10%): {good} ({good/len(valid_asymmetry)*100:.1f}%)")
            print(f"     ä¸­ç­‰ (10%-20%): {moderate} ({moderate/len(valid_asymmetry)*100:.1f}%)")
            print(f"     è¾ƒå·® (â‰¥20%): {poor} ({poor/len(valid_asymmetry)*100:.1f}%)")
        
        # RÂ²ç»Ÿè®¡
        valid_r2 = df['r2_avg'].dropna()
        if len(valid_r2) > 0:
            print(f"\nğŸ“ æ‹Ÿåˆè´¨é‡ç»Ÿè®¡:")
            print(f"   æœ€é«˜å¹³å‡RÂ²: {valid_r2.max():.3f}")
            print(f"   æœ€ä½å¹³å‡RÂ²: {valid_r2.min():.3f}")
            print(f"   å¹³å‡RÂ²: {valid_r2.mean():.3f}")
            print(f"   ä¸­ä½RÂ²: {valid_r2.median():.3f}")
            
            # RÂ²åˆ†å¸ƒ
            excellent_r2 = (valid_r2 >= 0.95).sum()
            good_r2 = ((valid_r2 >= 0.90) & (valid_r2 < 0.95)).sum()
            moderate_r2 = ((valid_r2 >= 0.80) & (valid_r2 < 0.90)).sum()
            poor_r2 = (valid_r2 < 0.80).sum()
            
            print(f"   æ‹Ÿåˆè´¨é‡åˆ†å¸ƒ:")
            print(f"     ä¼˜ç§€ (â‰¥0.95): {excellent_r2} ({excellent_r2/len(valid_r2)*100:.1f}%)")
            print(f"     è‰¯å¥½ (0.90-0.95): {good_r2} ({good_r2/len(valid_r2)*100:.1f}%)")
            print(f"     ä¸­ç­‰ (0.80-0.90): {moderate_r2} ({moderate_r2/len(valid_r2)*100:.1f}%)")
            print(f"     è¾ƒå·® (<0.80): {poor_r2} ({poor_r2/len(valid_r2)*100:.1f}%)")
        
        # ä¸´ç•ŒæŒ‡æ•°ç»Ÿè®¡
        valid_nu = df['nu_avg'].dropna()
        if len(valid_nu) > 0:
            print(f"\nâš¡ ä¸´ç•ŒæŒ‡æ•°ç»Ÿè®¡:")
            print(f"   æœ€å¤§Î½: {valid_nu.max():.3f}")
            print(f"   æœ€å°Î½: {valid_nu.min():.3f}")
            print(f"   å¹³å‡Î½: {valid_nu.mean():.3f}")
            print(f"   ä¸­ä½Î½: {valid_nu.median():.3f}")
            
            # ä¸ç†è®ºå€¼æ¯”è¾ƒ
            mean_field_close = ((valid_nu >= 0.4) & (valid_nu <= 0.6)).sum()
            ising_3d_close = ((valid_nu >= 0.55) & (valid_nu <= 0.70)).sum()
            ising_2d_close = ((valid_nu >= 0.9) & (valid_nu <= 1.1)).sum()
            
            print(f"   ç†è®ºå€¼æ¥è¿‘åº¦:")
            print(f"     æ¥è¿‘å¹³å‡åœº (0.4-0.6): {mean_field_close} ({mean_field_close/len(valid_nu)*100:.1f}%)")
            print(f"     æ¥è¿‘3D Ising (0.55-0.70): {ising_3d_close} ({ising_3d_close/len(valid_nu)*100:.1f}%)")
            print(f"     æ¥è¿‘2D Ising (0.9-1.1): {ising_2d_close} ({ising_2d_close/len(valid_nu)*100:.1f}%)")
        
        # å…³è”é•¿åº¦å³°å€¼ä½ç½®ç»Ÿè®¡ (r_peak)
        valid_r_peak = df['r_peak'].dropna()
        if len(valid_r_peak) > 0:
            print(f"\nğŸ“ å…³è”é•¿åº¦å³°å€¼ä½ç½®ç»Ÿè®¡ (r_peak):")
            print(f"   æœ€å¤§ä½ç½®: {valid_r_peak.max():.3f}")
            print(f"   æœ€å°ä½ç½®: {valid_r_peak.min():.3f}")
            print(f"   å¹³å‡ä½ç½®: {valid_r_peak.mean():.3f}")
            print(f"   ä¸­ä½ä½ç½®: {valid_r_peak.median():.3f}")
            
            # å³°å€¼ä½ç½®åˆ†å¸ƒ
            low_r = (valid_r_peak < 0.3).sum()
            mid_r = ((valid_r_peak >= 0.3) & (valid_r_peak < 0.7)).sum()
            high_r = (valid_r_peak >= 0.7).sum()
            
            print(f"   å³°å€¼ä½ç½®åˆ†å¸ƒ:")
            print(f"     ä½removalåŒº (<0.3): {low_r} ({low_r/len(valid_r_peak)*100:.1f}%)")
            print(f"     ä¸­removalåŒº (0.3-0.7): {mid_r} ({mid_r/len(valid_r_peak)*100:.1f}%)")
            print(f"     é«˜removalåŒº (â‰¥0.7): {high_r} ({high_r/len(valid_r_peak)*100:.1f}%)")
        
        # å…³è”é•¿åº¦å³°å€¼å¤§å°ç»Ÿè®¡ (xi_peak)
        valid_xi_peak = df['xi_peak'].dropna()
        if len(valid_xi_peak) > 0:
            print(f"\nğŸ”ï¸ å…³è”é•¿åº¦å³°å€¼å¤§å°ç»Ÿè®¡ (Î¾_peak):")
            print(f"   æœ€å¤§å…³è”é•¿åº¦: {valid_xi_peak.max():.2f}")
            print(f"   æœ€å°å…³è”é•¿åº¦: {valid_xi_peak.min():.2f}")
            print(f"   å¹³å‡å…³è”é•¿åº¦: {valid_xi_peak.mean():.2f}")
            print(f"   ä¸­ä½å…³è”é•¿åº¦: {valid_xi_peak.median():.2f}")
            
            # å…³è”é•¿åº¦é‡çº§åˆ†å¸ƒ
            small_xi = (valid_xi_peak < 10).sum()
            medium_xi = ((valid_xi_peak >= 10) & (valid_xi_peak < 100)).sum()
            large_xi = (valid_xi_peak >= 100).sum()
            
            print(f"   å…³è”é•¿åº¦é‡çº§åˆ†å¸ƒ:")
            print(f"     å°å°ºåº¦ (<10): {small_xi} ({small_xi/len(valid_xi_peak)*100:.1f}%)")
            print(f"     ä¸­ç­‰å°ºåº¦ (10-100): {medium_xi} ({medium_xi/len(valid_xi_peak)*100:.1f}%)")
            print(f"     å¤§å°ºåº¦ (â‰¥100): {large_xi} ({large_xi/len(valid_xi_peak)*100:.1f}%)")
        else:
            print(f"\nğŸ”ï¸ å…³è”é•¿åº¦å³°å€¼å¤§å°ç»Ÿè®¡: æ— æœ‰æ•ˆæ•°æ®")
        
        # å‚æ•°èŒƒå›´ç»Ÿè®¡ - æŒ‰åˆ†å¸ƒç±»å‹åˆ†ç»„
        print(f"\nâš™ï¸ å‚æ•°èŒƒå›´:")
        
        # åŸºæœ¬å‚æ•°ï¼ˆä¸¤ç§åˆ†å¸ƒéƒ½æœ‰ï¼‰
        for param in ['phi', 'theta']:
            valid_param = df[param].dropna()
            if len(valid_param) > 0:
                print(f"   {param}: [{valid_param.min():.3f}, {valid_param.max():.3f}]")
        
        # åˆ†å¸ƒç‰¹æœ‰å‚æ•°ç»Ÿè®¡
        poisson_df = df[df['distribution_type'] == 'poisson']
        powerlaw_df = df[df['distribution_type'] == 'powerlaw']
        
        if len(poisson_df) > 0:
            print(f"\n   ğŸ”µ æ³Šæ¾åˆ†å¸ƒç‰¹æœ‰å‚æ•°:")
            kappa_poisson = poisson_df['kappa'].dropna()
            if len(kappa_poisson) > 0:
                print(f"      Îº: [{kappa_poisson.min():.0f}, {kappa_poisson.max():.0f}]")
        
        if len(powerlaw_df) > 0:
            print(f"\n   ğŸ”´ å¹‚å¾‹åˆ†å¸ƒç½‘ç»œæ‹“æ‰‘å‚æ•°:")
            gamma_param = powerlaw_df['gamma_pref'].dropna()
            if len(gamma_param) > 0:
                print(f"      Î³ (å¹‚å¾‹æŒ‡æ•°): [{gamma_param.min():.2f}, {gamma_param.max():.2f}]")
            
            for param in ['k_min_pref', 'max_k']:
                valid_param = powerlaw_df[param].dropna()
                if len(valid_param) > 0:
                    param_names = {'k_min_pref': 'k_min (æœ€å°æˆªæ–­åº¦)', 'max_k': 'k_max (æœ€å¤§æˆªæ–­åº¦)'}
                    param_name = param_names.get(param, param)
                    print(f"      {param_name}: [{valid_param.min():.0f}, {valid_param.max():.0f}]")
    
    return df

def find_best_cases(df, top_n=5, min_xi_peak=5.0, min_r2=0.85):
    """
    æ‰¾å‡ºæœ€ä½³case
    
    å‚æ•°:
    - df: ç»“æœDataFrame
    - top_n: è¿”å›å‰å‡ ä¸ªæœ€ä½³ç»“æœ
    - min_xi_peak: æœ€å°å…³è”é•¿åº¦å³°å€¼é˜ˆå€¼ï¼Œä½äºæ­¤å€¼è¢«ç­›é™¤
    - min_r2: æœ€å°æ‹Ÿåˆè´¨é‡é˜ˆå€¼ï¼Œå·¦å³ä¸¤ä¾§RÂ²éƒ½éœ€â‰¥æ­¤å€¼
    
    è¿”å›:
    - best_symmetry: å¯¹ç§°æ€§æœ€å¥½çš„case
    - high_quality: é«˜è´¨é‡æ‹Ÿåˆçš„case (RÂ²â‰¥0.95)
    """
    
    print(f"\nğŸ† å¯»æ‰¾æœ€ä½³æ¡ˆä¾‹...")
    print("=" * 50)
    
    # å…ˆè¿›è¡Œæ‹Ÿåˆè´¨é‡å‰ç½®ç­›é€‰
    r2_mask = (df['r2_left'] >= min_r2) & (df['r2_right'] >= min_r2)
    quality_filtered_df = df[r2_mask].copy()
    
    # ç»Ÿè®¡æ€»ä½“æƒ…å†µ
    total_cases = len(df)
    quality_cases = len(quality_filtered_df)
    valid_xi_cases = len(quality_filtered_df.dropna(subset=['xi_peak']))
    xi_valid_df = quality_filtered_df.dropna(subset=['xi_peak'])
    strong_divergence_cases = len(xi_valid_df[xi_valid_df['xi_peak'] >= min_xi_peak])
    
    print(f"ğŸ“Š æ•°æ®æ¦‚è§ˆ:")
    print(f"   æ€»caseæ•°: {total_cases}")
    print(f"   é«˜è´¨é‡æ‹Ÿåˆ (RÂ²â‰¥{min_r2}): {quality_cases}")
    print(f"   æœ‰Î¾_peakæ•°æ®: {valid_xi_cases}")
    print(f"   å¼ºå‘æ•£case (Î¾â‰¥{min_xi_peak}): {strong_divergence_cases}")
    print(f"   å¼±å‘æ•£è¢«ç­›é™¤: {valid_xi_cases - strong_divergence_cases}")
    
    # === 1. å¯¹ç§°æ€§æœ€å¥½çš„case (æ‹Ÿåˆè´¨é‡ + å…³è”é•¿åº¦åŒé‡ç­›é€‰) ===
    xi_filtered_df = quality_filtered_df.dropna(subset=['asymmetry', 'xi_peak'])
    xi_filtered_df = xi_filtered_df[xi_filtered_df['xi_peak'] >= min_xi_peak]
    
    if len(xi_filtered_df) > 0:
        best_symmetry = xi_filtered_df.nsmallest(top_n, 'asymmetry')
        
        print(f"\nğŸ¯ å¯¹ç§°æ€§æœ€ä½³ (é¢„ç­›é€‰: RÂ²â‰¥{min_r2}, Î¾â‰¥{min_xi_peak}) - {len(best_symmetry)} ä¸ªcase:")
    else:
        # å¦‚æœå¼ºå‘æ•£caseä¸å¤Ÿï¼Œä½¿ç”¨æ‰€æœ‰é«˜è´¨é‡caseä½†ç»™å‡ºè­¦å‘Š
        valid_df = quality_filtered_df.dropna(subset=['asymmetry'])
        best_symmetry = valid_df.nsmallest(top_n, 'asymmetry')
        
        print(f"\nâš ï¸ å¯¹ç§°æ€§æœ€ä½³ (RÂ²â‰¥{min_r2}, æ— å¼ºå‘æ•£case) - {len(best_symmetry)} ä¸ªcase:")
    
    print("-" * 60)
    print(f"{'Rank':<4} {'Ï†':<6} {'Î¸':<6} {'Îº':<4} {'ä¸å¯¹ç§°æ€§':<10} {'Î½_avg':<7} {'RÂ²_avg':<7} {'Î¾_peak':<8}")
    print("-" * 60)
    
    for i, (idx, row) in enumerate(best_symmetry.iterrows()):
        xi_peak = row.get('xi_peak')
        xi_str = f"{xi_peak:.2f}" if xi_peak is not None else "N/A"
        marker = "âš ï¸" if xi_peak is not None and xi_peak < min_xi_peak else ""
        
        print(f"{i+1:<4} {row['phi']:<6.3f} {row['theta']:<6.3f} {row['kappa']:<4.0f} "
              f"{row['asymmetry']:<10.1%} {row['nu_avg']:<7.3f} "
              f"{row['r2_avg']:<7.3f} {xi_str:<8} {marker}")
    
    # === 2. æé«˜è´¨é‡æ‹Ÿåˆçš„case (ä¸¤ä¾§RÂ²éƒ½â‰¥0.95 ä¸”å…³è”é•¿åº¦â‰¥é˜ˆå€¼) ===
    # åœ¨é«˜è´¨é‡æ‹ŸåˆåŸºç¡€ä¸Šï¼Œè¿›ä¸€æ­¥ç­›é€‰RÂ²â‰¥0.95çš„æé«˜è´¨é‡case
    ultra_high_quality_mask = (quality_filtered_df['r2_left'] >= 0.95) & (quality_filtered_df['r2_right'] >= 0.95)
    high_quality_raw = quality_filtered_df[ultra_high_quality_mask].copy()
    
    # åŒæ ·è¿›è¡Œå…³è”é•¿åº¦ç­›é€‰
    high_quality = high_quality_raw.dropna(subset=['xi_peak'])
    high_quality = high_quality[high_quality['xi_peak'] >= min_xi_peak]
    
    if len(high_quality) > 0:
        # æŒ‰å¯¹ç§°æ€§æ’åº
        high_quality = high_quality.sort_values('asymmetry')
        
        print(f"\nğŸ“ æé«˜è´¨é‡æ‹Ÿåˆ (RÂ²â‰¥0.95, Î¾â‰¥{min_xi_peak}) çš„ {len(high_quality)} ä¸ªcase:")
        print("-" * 75)
        print(f"{'Rank':<4} {'Ï†':<6} {'Î¸':<6} {'Îº':<4} {'ä¸å¯¹ç§°æ€§':<10} {'RÂ²_L':<6} {'RÂ²_R':<6} {'Î½_avg':<7} {'Î¾_peak':<8}")
        print("-" * 75)
        
        for i, (idx, row) in enumerate(high_quality.iterrows()):
            xi_peak = row.get('xi_peak')
            xi_str = f"{xi_peak:.2f}" if xi_peak is not None else "N/A"
            
            print(f"{i+1:<4} {row['phi']:<6.3f} {row['theta']:<6.3f} {row['kappa']:<4.0f} "
                  f"{row['asymmetry']:<10.1%} {row['r2_left']:<6.3f} {row['r2_right']:<6.3f} "
                  f"{row['nu_avg']:<7.3f} {xi_str:<8}")
    else:
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        raw_count = len(high_quality_raw)
        print(f"\nğŸ“ æé«˜è´¨é‡æ‹Ÿåˆç»Ÿè®¡:")
        print(f"   åŸºç¡€ç­›é€‰ (RÂ²â‰¥{min_r2}): {quality_cases}")
        print(f"   æé«˜è´¨é‡ (RÂ²â‰¥0.95): {raw_count}")
        print(f"   å¼ºå‘æ•£ç­›é€‰å (Î¾â‰¥{min_xi_peak}): 0")
        print(f"   âš ï¸ æ‰€æœ‰æé«˜è´¨é‡caseéƒ½æ˜¯å¼±å‘æ•£ï¼Œå»ºè®®è°ƒæ•´å‚æ•°èŒƒå›´")
    
    # === 3. ç»¼åˆæœ€ä½³ (å¯¹ç§°æ€§å¥½ä¸”RÂ²é«˜ï¼ŒåŸºäºé«˜è´¨é‡æ‹Ÿåˆ) ===
    valid_quality_df = quality_filtered_df.dropna(subset=['asymmetry', 'r2_avg'])
    if len(valid_quality_df) > 0:
        # ç»¼åˆè¯„åˆ†ï¼šå½’ä¸€åŒ–çš„å¯¹ç§°æ€§ + å½’ä¸€åŒ–çš„RÂ²è´¨é‡
        asymmetry_norm = (valid_quality_df['asymmetry'] - valid_quality_df['asymmetry'].min()) / (valid_quality_df['asymmetry'].max() - valid_quality_df['asymmetry'].min())
        r2_norm = (valid_quality_df['r2_avg'] - valid_quality_df['r2_avg'].min()) / (valid_quality_df['r2_avg'].max() - valid_quality_df['r2_avg'].min())
        
        # ç»¼åˆè¯„åˆ†ï¼šå¯¹ç§°æ€§æƒé‡0.6ï¼ŒRÂ²æƒé‡0.4 (å¯¹ç§°æ€§æ›´é‡è¦)
        valid_quality_df = valid_quality_df.copy()
        valid_quality_df['composite_score'] = 0.6 * (1 - asymmetry_norm) + 0.4 * r2_norm
        
        best_composite = valid_quality_df.nlargest(min(top_n, len(valid_quality_df)), 'composite_score')
        
        print(f"\nğŸ… ç»¼åˆæœ€ä½³ (RÂ²â‰¥{min_r2}, å¯¹ç§°æ€§0.6 + RÂ²è´¨é‡0.4) çš„ {len(best_composite)} ä¸ªcase:")
        print("-" * 70)
        print(f"{'Rank':<4} {'Ï†':<6} {'Î¸':<6} {'Îº':<4} {'ä¸å¯¹ç§°æ€§':<10} {'RÂ²_avg':<7} {'ç»¼åˆåˆ†':<7}")
        print("-" * 70)
        
        for i, (idx, row) in enumerate(best_composite.iterrows()):
            asymmetry = row['asymmetry']
            r2_avg = row['r2_avg']
            score = row['composite_score']
            
            print(f"{i+1:<4} {row['phi']:<6.3f} {row['theta']:<6.3f} {row['kappa']:<4.0f} "
                  f"{asymmetry:<10.1%} {r2_avg:<7.3f} {score:<7.3f}")
    
    return best_symmetry, high_quality

def visualize_best_cases(df, max_plots=3):
    """
    å¯è§†åŒ–3ç§æœ€ä½³caseçš„è¯¦ç»†åˆ†æå›¾
    
    å‚æ•°:
    - df: å®Œæ•´çš„ç»“æœDataFrame
    - max_plots: æœ€å¤šæ˜¾ç¤ºå‡ ä¸ªå›¾
    """
    
    print(f"\nğŸ¨ ç”Ÿæˆ3ç§æœ€ä½³caseçš„å¯è§†åŒ–...")
    print("=" * 50)
    
    import matplotlib.pyplot as plt
    
    cases_to_plot = []
    
    # 1. å¯¹ç§°æ€§æœ€å¥½çš„case
    if len(df) > 0:
        best_symmetry_case = df.loc[df['asymmetry'].idxmin()]
        xi_peak_value = best_symmetry_case.get('xi_peak')
        cases_to_plot.append({
            'type': 'ğŸ¯ æœ€ä½³å¯¹ç§°æ€§',
            'phi': best_symmetry_case['phi'],
            'theta': best_symmetry_case['theta'], 
            'kappa': best_symmetry_case['kappa'],
            'asymmetry': best_symmetry_case['asymmetry'],
            'r2_avg': best_symmetry_case['r2_avg'],
            'xi_peak': xi_peak_value if xi_peak_value is not None else 'N/A'
        })
    
    # 2. æ‹Ÿåˆè´¨é‡æœ€å¥½çš„case (RÂ²æœ€é«˜)
    if len(df) > 0:
        best_r2_case = df.loc[df['r2_avg'].idxmax()]
        
        # æ£€æŸ¥æ˜¯å¦ä¸å¯¹ç§°æ€§caseç›¸åŒ
        if not (best_r2_case['phi'] == cases_to_plot[0]['phi'] and
                best_r2_case['theta'] == cases_to_plot[0]['theta'] and
                best_r2_case['kappa'] == cases_to_plot[0]['kappa']):
            
            xi_peak_value = best_r2_case.get('xi_peak')
            cases_to_plot.append({
                'type': 'ğŸ“ æœ€ä½³æ‹Ÿåˆè´¨é‡',
                'phi': best_r2_case['phi'],
                'theta': best_r2_case['theta'],
                'kappa': best_r2_case['kappa'], 
                'asymmetry': best_r2_case['asymmetry'],
                'r2_avg': best_r2_case['r2_avg'],
                'xi_peak': xi_peak_value if xi_peak_value is not None else 'N/A'
            })
    
    # 3. å…³è”é•¿åº¦æœ€å¤§çš„case (Î¾_peakæœ€å¤§)
    valid_xi_df = df.dropna(subset=['xi_peak'])
    if len(valid_xi_df) > 0:
        best_xi_case = valid_xi_df.loc[valid_xi_df['xi_peak'].idxmax()]
        
        # æ£€æŸ¥æ˜¯å¦ä¸å‰é¢çš„caseé‡å¤
        is_duplicate = False
        for existing_case in cases_to_plot:
            if (best_xi_case['phi'] == existing_case['phi'] and
                best_xi_case['theta'] == existing_case['theta'] and
                best_xi_case['kappa'] == existing_case['kappa']):
                is_duplicate = True
                break
        
        if not is_duplicate:
            cases_to_plot.append({
                'type': 'ğŸ”ï¸ æœ€å¤§å…³è”é•¿åº¦',
                'phi': best_xi_case['phi'],
                'theta': best_xi_case['theta'],
                'kappa': best_xi_case['kappa'],
                'asymmetry': best_xi_case['asymmetry'],
                'r2_avg': best_xi_case['r2_avg'],
                'xi_peak': best_xi_case['xi_peak']
            })
    
    # å¦‚æœå…³è”é•¿åº¦æœ€å¤§çš„caseä¸å­˜åœ¨ï¼Œæ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·
    if len(valid_xi_df) == 0:
        print(f"âš ï¸ è­¦å‘Šï¼šæ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„å…³è”é•¿åº¦æ•°æ®ï¼Œåªèƒ½æ˜¾ç¤ºå¯¹ç§°æ€§å’Œæ‹Ÿåˆè´¨é‡æœ€ä½³çš„case")
    
    # é™åˆ¶ç»˜å›¾æ•°é‡
    cases_to_plot = cases_to_plot[:max_plots]
    
    print(f"å°†å±•ç¤º {len(cases_to_plot)} ä¸ªæœ€ä½³caseçš„è¯¦ç»†åˆ†æå›¾:")
    
    for i, case in enumerate(cases_to_plot):
        print(f"\nğŸ“Š ç»˜åˆ¶ {case['type']} case:")
        
        # æ˜¾ç¤ºå¹‚å¾‹åˆ†å¸ƒçš„å®Œæ•´å‚æ•°
        gamma_pref = case.get('gamma_pref')
        k_min = case.get('k_min_pref')
        k_max = case.get('max_k')
        
        if gamma_pref is not None and k_min is not None and k_max is not None:
            print(f"    å‚æ•°: Ï†={case['phi']:.3f}, Î¸={case['theta']:.3f}, Î³={gamma_pref:.1f}, k_min={k_min:.0f}, k_max={k_max:.0f}")
        else:
            print(f"    å‚æ•°: Ï†={case['phi']:.3f}, Î¸={case['theta']:.3f}, Îº={case['kappa']:.0f} (åŸºç¡€å‚æ•°)")
        print(f"    å¯¹ç§°æ€§: {case['asymmetry']:.1%}")
        print(f"    æ‹Ÿåˆè´¨é‡: RÂ²={case['r2_avg']:.3f}")
        if case['xi_peak'] != 'N/A' and case['xi_peak'] is not None:
            print(f"    å…³è”é•¿åº¦å³°å€¼: Î¾={case['xi_peak']:.2f}")
        else:
            print(f"    å…³è”é•¿åº¦å³°å€¼: æ•°æ®ç¼ºå¤±")
        
        # å°è¯•ä»ç¼“å­˜æ•°æ®ç»˜å›¾ï¼Œå¦‚æœæ²¡æœ‰åˆ™é‡æ–°è®¡ç®—
        try:
            print(f"    ğŸ¨ æ­£åœ¨ç”Ÿæˆå¹‚å¾‹å¯¹ç§°æ€§åˆ†æå›¾...")
            
            # å°è¯•ä»åˆ†å±‚ç¼“å­˜åŠ è½½å…³è”é•¿åº¦æ•°æ®
            success = plot_from_cache_data(case)
            
            if not success:
                print(f"    ğŸ”„ ç¼“å­˜æ•°æ®ä¸è¶³ï¼Œé‡æ–°è®¡ç®—...")
                result = analyze_power_law_symmetry(
                    phi=case['phi'],
                    theta=case['theta'], 
                    kappa=int(case.get('kappa', 120)),
                    gamma_pref=case.get('gamma_pref'),  # å¹‚å¾‹åˆ†å¸ƒå‚æ•°
                    k_min_pref=case.get('k_min_pref', 1),
                    max_k=case.get('max_k', 200),
                    r_range=(0.1, 0.9)  # æ‰©å±•æœç´¢èŒƒå›´
                )
                
                if result:
                    print(f"    âœ… å›¾è¡¨å·²ç”Ÿæˆå¹¶æ˜¾ç¤º")
                else:
                    print(f"    âŒ å›¾è¡¨ç”Ÿæˆå¤±è´¥")
            
        except Exception as e:
            print(f"    âŒ ç»˜å›¾é”™è¯¯: {e}")
    
    print(f"\nâœ¨ å›¾è¡¨ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {len(cases_to_plot)} ä¸ªåˆ†æå›¾ã€‚")

def visualize_top_cases(df, sort_by='symmetry', top_n=10, max_plots=None, min_xi_peak=5.0, min_r2=0.85, 
                       filter_multi_peak=True, min_peak_quality=0.5):
    """
    å¯è§†åŒ–æ’åºé å‰çš„å¤šä¸ªcase
    
    å‚æ•°:
    - df: å®Œæ•´çš„ç»“æœDataFrame
    - sort_by: æ’åºæ ‡å‡† ('symmetry', 'composite', 'xi_peak')
    - top_n: æ˜¾ç¤ºå‰å‡ ä¸ªcase
    - max_plots: æœ€å¤šç»˜åˆ¶å‡ ä¸ªå›¾ (Noneè¡¨ç¤ºå…¨éƒ¨ç»˜åˆ¶)
    - min_xi_peak: æœ€å°å…³è”é•¿åº¦å³°å€¼é˜ˆå€¼ (å¯¹symmetryå’Œcompositeæ’åºæœ‰æ•ˆ)
    - min_r2: æœ€å°æ‹Ÿåˆè´¨é‡é˜ˆå€¼ (å·¦å³ä¸¤ä¾§RÂ²éƒ½éœ€â‰¥æ­¤å€¼)
    - filter_multi_peak: æ˜¯å¦è¿‡æ»¤å¤šå³°case
    - min_peak_quality: æœ€å°å³°å€¼è´¨é‡é˜ˆå€¼
    """
    
    print(f"\nğŸ¨ å¯è§†åŒ–æ’åºå‰{top_n}çš„case (æŒ‰{sort_by}æ’åº)...")
    print("=" * 80)
    
    # æ‰€æœ‰æ’åºéƒ½å…ˆè¿›è¡Œæ‹Ÿåˆè´¨é‡å‰ç½®ç­›é€‰
    r2_mask = (df['r2_left'] >= min_r2) & (df['r2_right'] >= min_r2)
    quality_filtered_df = df[r2_mask].copy()
    
    if len(quality_filtered_df) == 0:
        print(f"âŒ æ²¡æœ‰æ»¡è¶³æ‹Ÿåˆè´¨é‡è¦æ±‚ (RÂ²â‰¥{min_r2}) çš„case")
        return
    
    print(f"ğŸ“Š æ‹Ÿåˆè´¨é‡é¢„ç­›é€‰: {len(df)} â†’ {len(quality_filtered_df)} case (RÂ²â‰¥{min_r2})")
    
    # å®šä¹‰åŸºç¡€ç­›é€‰æè¿°ï¼ˆæ‰€æœ‰æ’åºæ–¹å¼éƒ½ä¼šç”¨åˆ°ï¼‰
    filter_desc = f"RÂ²â‰¥{min_r2}"
    if filter_multi_peak:
        filter_desc += ", å•å³°"
    
    # ğŸ” å¤šå³°æ£€æµ‹å’Œå³°å€¼è´¨é‡ç­›é€‰
    if filter_multi_peak:
        print(f"ğŸ” æ‰§è¡Œå¤šå³°æ£€æµ‹å’Œå³°å€¼è´¨é‡åˆ†æ...")
        
        # æ£€æµ‹æ¯ä¸ªcaseçš„å¤šå³°æ€§è´¨
        single_peak_indices = []
        multi_peak_count = 0
        low_quality_count = 0
        
        for idx, row in quality_filtered_df.iterrows():
            try:
                phi, theta = row['phi'], row['theta']
                kappa = int(row.get('kappa', 120))
                gamma_pref = row.get('gamma_pref')
                k_min_pref = row.get('k_min_pref', 1)
                max_k = row.get('max_k', 200)
                
                # åŠ è½½å…³è”é•¿åº¦æ•°æ®  
                correlation_data = load_correlation_data(
                    phi=phi, theta=theta, kappa=kappa,
                    gamma_pref=gamma_pref, 
                    k_min_pref=k_min_pref,
                    max_k=max_k,
                    use_original_like_dist=row.get('use_original_like_dist', False),
                    r_range=(0.1, 0.9), peak_search_points=100
                )
                
                if correlation_data:
                    r_values = correlation_data.get('r_values')
                    xi_values = correlation_data.get('xi_values')
                    r_peak = correlation_data.get('r_peak')
                    
                    if r_values is not None and xi_values is not None and r_peak is not None:
                        # å¤šå³°æ£€æµ‹ (ä½¿ç”¨æ›´æ•æ„Ÿçš„é˜ˆå€¼0.55)
                        is_multi_peak, peak_info = detect_multiple_peaks(r_values, xi_values, r_peak, prominence_threshold=0.55)
                        quality_score, quality_info = analyze_peak_quality(r_values, xi_values, r_peak)
                        
                        # ç­›é€‰æ¡ä»¶
                        if is_multi_peak:
                            multi_peak_count += 1
                        elif quality_score < min_peak_quality:
                            low_quality_count += 1
                        else:
                            single_peak_indices.append(idx)
                    else:
                        low_quality_count += 1
                else:
                    low_quality_count += 1
                    
            except Exception as e:
                low_quality_count += 1
                continue
        
        # åº”ç”¨å¤šå³°è¿‡æ»¤
        if len(single_peak_indices) > 0:
            peak_filtered_df = quality_filtered_df.loc[single_peak_indices].copy()
            print(f"ğŸ“Š å¤šå³°è¿‡æ»¤ç»“æœ: {len(quality_filtered_df)} â†’ {len(peak_filtered_df)} case")
            print(f"   âš ï¸ å¤šå³°case: {multi_peak_count}")
            print(f"   âš ï¸ ä½è´¨é‡å³°: {low_quality_count}")
            print(f"   âœ… å•å³°é«˜è´¨é‡: {len(peak_filtered_df)}")
        else:
            peak_filtered_df = quality_filtered_df.copy()
            print(f"âš ï¸ å¤šå³°è¿‡æ»¤åæ— ç¬¦åˆæ¡ä»¶çš„caseï¼Œä½¿ç”¨åŸæ•°æ®")
    else:
        peak_filtered_df = quality_filtered_df.copy()
        print(f"ğŸ” è·³è¿‡å¤šå³°æ£€æµ‹ (filter_multi_peak=False)")
    
    # æ ¹æ®æ’åºæ ‡å‡†é€‰æ‹©æ•°æ®å’Œæ’åºæ–¹å¼
    if sort_by == 'symmetry':
        # å¯¹ç§°æ€§æ’åºï¼šæ‹Ÿåˆè´¨é‡ + å¤šå³°è¿‡æ»¤ + å…³è”é•¿åº¦ä¸‰é‡ç­›é€‰
        valid_df = peak_filtered_df.dropna(subset=['asymmetry', 'xi_peak'])
        xi_filtered_df = valid_df[valid_df['xi_peak'] >= min_xi_peak]
        
        if len(xi_filtered_df) >= top_n:
            sorted_df = xi_filtered_df.nsmallest(top_n, 'asymmetry')  # è¶Šå°è¶Šå¥½
            sort_desc = f"å¯¹ç§°æ€§æœ€ä½³ ({filter_desc}, Î¾â‰¥{min_xi_peak})"
        elif len(valid_df) >= top_n:
            # å¦‚æœå¼ºå‘æ•£caseä¸å¤Ÿï¼Œä½¿ç”¨æ‰€æœ‰è¿‡æ»¤åcaseä½†æ ‡æ³¨
            sorted_df = valid_df.nsmallest(top_n, 'asymmetry')
            sort_desc = f"å¯¹ç§°æ€§æœ€ä½³ ({filter_desc}, å«å¼±å‘æ•£âš ï¸)"
        else:
            # caseæ€»æ•°éƒ½ä¸å¤Ÿ
            sorted_df = valid_df.nsmallest(len(valid_df), 'asymmetry')
            sort_desc = f"å¯¹ç§°æ€§æœ€ä½³ ({filter_desc}, ä»…{len(valid_df)}ä¸ªcase)"
        
    elif sort_by == 'composite':
        # ç»¼åˆæ’åºï¼šæ‹Ÿåˆè´¨é‡ + å¤šå³°è¿‡æ»¤ + å…³è”é•¿åº¦ä¸‰é‡ç­›é€‰
        valid_df = peak_filtered_df.dropna(subset=['asymmetry', 'r2_avg', 'xi_peak'])
        xi_filtered_df = valid_df[valid_df['xi_peak'] >= min_xi_peak]
        
        if len(xi_filtered_df) >= top_n:
            # ä½¿ç”¨å¼ºå‘æ•£caseè®¡ç®—ç»¼åˆè¯„åˆ†
            asymmetry_norm = (xi_filtered_df['asymmetry'] - xi_filtered_df['asymmetry'].min()) / (xi_filtered_df['asymmetry'].max() - xi_filtered_df['asymmetry'].min())
            r2_norm = (xi_filtered_df['r2_avg'] - xi_filtered_df['r2_avg'].min()) / (xi_filtered_df['r2_avg'].max() - xi_filtered_df['r2_avg'].min())
            
            xi_filtered_df = xi_filtered_df.copy()
            xi_filtered_df['composite_score'] = 0.6 * (1 - asymmetry_norm) + 0.4 * r2_norm
            sorted_df = xi_filtered_df.nlargest(top_n, 'composite_score')
            sort_desc = f"ç»¼åˆè¯„åˆ†æœ€ä½³ ({filter_desc}, Î¾â‰¥{min_xi_peak})"
        elif len(valid_df) > 0:
            # å¦‚æœå¼ºå‘æ•£caseä¸å¤Ÿï¼Œä½¿ç”¨æ‰€æœ‰è¿‡æ»¤åcaseä½†æ ‡æ³¨
            asymmetry_norm = (valid_df['asymmetry'] - valid_df['asymmetry'].min()) / (valid_df['asymmetry'].max() - valid_df['asymmetry'].min())
            r2_norm = (valid_df['r2_avg'] - valid_df['r2_avg'].min()) / (valid_df['r2_avg'].max() - valid_df['r2_avg'].min())
            
            valid_df = valid_df.copy()
            valid_df['composite_score'] = 0.6 * (1 - asymmetry_norm) + 0.4 * r2_norm
            sorted_df = valid_df.nlargest(top_n, 'composite_score')
            sort_desc = f"ç»¼åˆè¯„åˆ†æœ€ä½³ ({filter_desc}, å«å¼±å‘æ•£âš ï¸)"
        else:
            print(f"âŒ æ²¡æœ‰è¶³å¤Ÿçš„æ•°æ®è®¡ç®—ç»¼åˆè¯„åˆ† (éœ€è¦{filter_desc})")
            return
            
    elif sort_by == 'xi_peak':
        # å…³è”é•¿åº¦å³°å€¼æ’åºï¼šæ‹Ÿåˆè´¨é‡ + å¤šå³°è¿‡æ»¤
        valid_df = peak_filtered_df.dropna(subset=['xi_peak'])
        if len(valid_df) >= top_n:
            sorted_df = valid_df.nlargest(top_n, 'xi_peak')  # è¶Šå¤§è¶Šå¥½
            sort_desc = f"å…³è”é•¿åº¦å³°å€¼æœ€å¤§ ({filter_desc})"
        else:
            sorted_df = valid_df.nlargest(len(valid_df), 'xi_peak')
            sort_desc = f"å…³è”é•¿åº¦å³°å€¼æœ€å¤§ ({filter_desc}, ä»…{len(valid_df)}ä¸ªcase)"
        
    else:
        if sort_by == 'quality':
            print(f"âš ï¸ æ‹Ÿåˆè´¨é‡æ’åºå·²ç§»é™¤ - ç°åœ¨æ˜¯æ‰€æœ‰æ’åºçš„å‰ææ¡ä»¶ (RÂ²â‰¥{min_r2})")
            print(f"ğŸ’¡ å»ºè®®ä½¿ç”¨: 'symmetry', 'xi_peak', æˆ– 'composite'")
        else:
            print(f"âŒ æœªçŸ¥çš„æ’åºæ ‡å‡†: {sort_by}")
            print(f"ğŸ’¡ å¯ç”¨æ’åº: 'symmetry', 'xi_peak', 'composite'")
        return
    
    if len(sorted_df) == 0:
        print(f"âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ•°æ®è¿›è¡Œæ’åº")
        return
    
    # ç¡®å®šå®é™…ç»˜åˆ¶çš„æ•°é‡
    actual_plots = min(len(sorted_df), max_plots if max_plots else len(sorted_df))
    
    print(f"ğŸ“Š {sort_desc} - å‰{len(sorted_df)}ä¸ªcase:")
    print("-" * 115)
    print(f"{'Rank':<4} {'åˆ†å¸ƒ':<6} {'Ï†':<7} {'Î¸':<7} {'å¹‚å¾‹ç½‘ç»œå‚æ•°':<18} {'ä¸å¯¹ç§°æ€§':<10} {'RÂ²_avg':<8} {'Î¾_peak':<8} {'å³°å€¼ç±»å‹':<8} {'æè¿°':<20}")
    print("-" * 115)
    
    # æ˜¾ç¤ºæ’åºè¡¨æ ¼
    for i, (idx, row) in enumerate(sorted_df.iterrows()):
        asymmetry = row['asymmetry'] if not pd.isna(row['asymmetry']) else None
        r2_avg = row['r2_avg'] if not pd.isna(row['r2_avg']) else None
        xi_peak = row['xi_peak'] if not pd.isna(row['xi_peak']) else None
        
        asymmetry_str = f"{asymmetry:.1%}" if asymmetry is not None else "N/A"
        r2_str = f"{r2_avg:.3f}" if r2_avg is not None else "N/A"
        xi_str = f"{xi_peak:.2f}" if xi_peak is not None else "N/A"
        
        # è·å–åˆ†å¸ƒç±»å‹å’Œç½‘ç»œå‚æ•°ä¿¡æ¯
        distribution_type = row.get('distribution_type', 'unknown')
        dist_symbol = "ğŸ”µ" if distribution_type == 'poisson' else "ğŸ”´" if distribution_type == 'powerlaw' else "â“"
        
        # ç½‘ç»œå‚æ•°æ˜¾ç¤º
        if distribution_type == 'poisson':
            kappa = row.get('kappa', 'N/A')
            network_params = f"Îº={kappa:.0f}" if kappa != 'N/A' else "Îº=N/A"
        elif distribution_type == 'powerlaw':
            gamma_pref = row.get('gamma_pref', 'N/A')
            k_min = row.get('k_min_pref', 'N/A')
            k_max = row.get('max_k', 'N/A')
            if gamma_pref != 'N/A' and k_min != 'N/A' and k_max != 'N/A':
                network_params = f"Î³={gamma_pref:.1f},k_min={k_min:.0f},k_max={k_max:.0f}"
            else:
                network_params = "å¹‚å¾‹å‚æ•°ç¼ºå¤±"
        else:
            network_params = "æœªçŸ¥"
        
        # æ£€æµ‹å³°å€¼ç±»å‹
        peak_type = "æœªçŸ¥"
        try:
            phi, theta = row['phi'], row['theta']
            kappa = row.get('kappa', 120)
            gamma_pref = row.get('gamma_pref') if distribution_type == 'powerlaw' else None
            
            correlation_data = load_correlation_data(
                phi=phi, theta=theta, kappa=kappa,
                gamma_pref=gamma_pref, 
                k_min_pref=row.get('k_min_pref', 1) if distribution_type == 'powerlaw' else 1,
                max_k=row.get('max_k', 200) if distribution_type == 'powerlaw' else 200,
                r_range=(0.1, 0.9), peak_search_points=100
            )
            if correlation_data:
                r_values = correlation_data.get('r_values')
                xi_values = correlation_data.get('xi_values')
                r_peak = correlation_data.get('r_peak')
                if r_values is not None and xi_values is not None and r_peak is not None:
                    is_multi_peak, peak_info = detect_multiple_peaks(r_values, xi_values, r_peak)
                    peak_type = "å¤šå³°âš ï¸" if is_multi_peak else "å•å³°âœ“"
        except:
            pass
        
        # æ·»åŠ æè¿°æ ‡ç­¾
        desc_parts = []
        if asymmetry is not None and asymmetry < 0.05:
            desc_parts.append("ä¼˜ç§€å¯¹ç§°")
        if r2_avg is not None and r2_avg >= 0.95:
            desc_parts.append("é«˜è´¨é‡æ‹Ÿåˆ")
        if xi_peak is not None and xi_peak > 10:
            desc_parts.append("å¼ºå‘æ•£")
        elif xi_peak is not None and xi_peak < 3:
            desc_parts.append("å¼±å‘æ•£âš ï¸")
            
        desc = ", ".join(desc_parts) if desc_parts else "æ™®é€š"
        
        marker = "ğŸ†" if i < 3 else f"{i+1:2d}"
        print(f"{marker:<4} {dist_symbol:<6} {row['phi']:<7.3f} {row['theta']:<7.3f} {network_params:<18} "
              f"{asymmetry_str:<10} {r2_str:<8} {xi_str:<8} {peak_type:<8} {desc:<20}")
    
    # å¼€å§‹ç»˜åˆ¶å›¾è¡¨
    print(f"\nğŸ¨ å¼€å§‹ç»˜åˆ¶å‰{actual_plots}ä¸ªcaseçš„è¯¦ç»†åˆ†æå›¾...")
    
    cases_plotted = 0
    for i, (idx, row) in enumerate(sorted_df.iterrows()):
        if cases_plotted >= actual_plots:
            break
            
        print(f"\nğŸ“Š ç»˜åˆ¶ç¬¬{i+1}å case:")
        
        # æ˜¾ç¤ºå¹‚å¾‹åˆ†å¸ƒçš„å®Œæ•´å‚æ•°
        gamma_pref = row.get('gamma_pref', 'N/A')
        k_min = row.get('k_min_pref', 'N/A')
        k_max = row.get('max_k', 'N/A')
        
        if gamma_pref != 'N/A' and k_min != 'N/A' and k_max != 'N/A':
            print(f"    å‚æ•°: Ï†={row['phi']:.3f}, Î¸={row['theta']:.3f}, Î³={gamma_pref:.1f}, k_min={k_min:.0f}, k_max={k_max:.0f}")
        else:
            print(f"    å‚æ•°: Ï†={row['phi']:.3f}, Î¸={row['theta']:.3f}, Îº={row['kappa']:.0f} (å›é€€åˆ°åŸºç¡€å‚æ•°)")
        
        asymmetry = row['asymmetry'] if not pd.isna(row['asymmetry']) else None
        r2_avg = row['r2_avg'] if not pd.isna(row['r2_avg']) else None
        xi_peak = row['xi_peak'] if not pd.isna(row['xi_peak']) else None
        
        if asymmetry is not None:
            print(f"    å¯¹ç§°æ€§: {asymmetry:.1%}")
        if r2_avg is not None:
            print(f"    æ‹Ÿåˆè´¨é‡: RÂ²={r2_avg:.3f}")
        if xi_peak is not None:
            print(f"    å…³è”é•¿åº¦å³°å€¼: Î¾={xi_peak:.2f}")
        
        # æ„é€ caseå­—å…¸ç”¨äºç»˜å›¾ - åŒ…å«å®Œæ•´çš„å¹‚å¾‹åˆ†å¸ƒå‚æ•°
        case = {
            'phi': row['phi'],
            'theta': row['theta'],
            'kappa': row['kappa'],
            'gamma_pref': row.get('gamma_pref'),  # å¹‚å¾‹æŒ‡æ•°
            'k_min_pref': row.get('k_min_pref', 1),  # æœ€å°åº¦æ•°
            'max_k': row.get('max_k', 200),  # æœ€å¤§åº¦æ•°
            'use_original_like_dist': row.get('use_original_like_dist', False),  # æ˜¯å¦ä½¿ç”¨åŸå§‹åˆ†å¸ƒ
            'asymmetry': asymmetry,
            'r2_avg': r2_avg,
            'xi_peak': xi_peak
        }
        
        try:
            print(f"    ğŸ¨ æ­£åœ¨ç”Ÿæˆå¹‚å¾‹å¯¹ç§°æ€§åˆ†æå›¾...")
            
            # å°è¯•ä»åˆ†å±‚ç¼“å­˜åŠ è½½å…³è”é•¿åº¦æ•°æ®
            success = plot_from_cache_data(case)
            
            if not success:
                print(f"    ğŸ”„ ç¼“å­˜æ•°æ®ä¸è¶³ï¼Œé‡æ–°è®¡ç®—...")
                result = analyze_power_law_symmetry(
                    phi=case['phi'],
                    theta=case['theta'], 
                    kappa=int(case.get('kappa', 120)),
                    gamma_pref=case.get('gamma_pref'),  # å¹‚å¾‹åˆ†å¸ƒå‚æ•°
                    k_min_pref=case.get('k_min_pref', 1),
                    max_k=case.get('max_k', 200),
                    use_original_like_dist=case.get('use_original_like_dist', False),
                    r_range=(0.1, 0.9)
                )
                
                if result:
                    print(f"    âœ… å›¾è¡¨å·²ç”Ÿæˆå¹¶æ˜¾ç¤º")
                else:
                    print(f"    âŒ å›¾è¡¨ç”Ÿæˆå¤±è´¥")
            
            cases_plotted += 1
            
        except Exception as e:
            print(f"    âŒ ç»˜å›¾é”™è¯¯: {e}")
    
    print(f"\nâœ¨ å›¾è¡¨ç”Ÿæˆå®Œæˆï¼å…±ç”Ÿæˆ {cases_plotted} ä¸ªåˆ†æå›¾ã€‚")

def show_detailed_case_info(df, case_type="best"):
    """
    å±•ç¤ºè¯¦ç»†çš„caseä¿¡æ¯
    
    å‚æ•°:
    - df: ç»“æœDataFrame
    - case_type: å±•ç¤ºç±»å‹ ("best", "high_quality", "comprehensive")
    """
    
    print(f"\nğŸ“‹ è¯¦ç»†æ¡ˆä¾‹ä¿¡æ¯ ({case_type}):")
    print("=" * 80)
    
    if case_type == "best" and len(df) > 0:
        # æ˜¾ç¤ºå¯¹ç§°æ€§æœ€å¥½çš„å‰3ä¸ª
        best_cases = df.nsmallest(3, 'asymmetry')
        
        for i, (idx, case) in enumerate(best_cases.iterrows()):
            print(f"\nğŸ† æ’å #{i+1} - æœ€ä½³å¯¹ç§°æ€§:")
            
            # æ˜¾ç¤ºå¹‚å¾‹åˆ†å¸ƒçš„å®Œæ•´å‚æ•°
            gamma_pref = case.get('gamma_pref')
            k_min = case.get('k_min_pref')
            k_max = case.get('max_k')
            
            if gamma_pref is not None and k_min is not None and k_max is not None:
                print(f"   å‚æ•°: Ï†={case['phi']:.3f}, Î¸={case['theta']:.3f}, Î³={gamma_pref:.1f}, k_min={k_min:.0f}, k_max={k_max:.0f}")
            else:
                print(f"   å‚æ•°: Ï†={case['phi']:.3f}, Î¸={case['theta']:.3f}, Îº={case['kappa']:.0f} (åŸºç¡€å‚æ•°)")
            print(f"   å¯¹ç§°æ€§: {case['asymmetry']:.1%}")
            print(f"   ä¸´ç•ŒæŒ‡æ•°: Î½_avg={case['nu_avg']:.3f} (å·¦:{case['nu_left']:.3f}, å³:{case['nu_right']:.3f})")
            print(f"   æ‹Ÿåˆè´¨é‡: RÂ²_avg={case['r2_avg']:.3f} (å·¦:{case['r2_left']:.3f}, å³:{case['r2_right']:.3f})")
            print(f"   å…³è”é•¿åº¦å³°å€¼: r_peak={case['r_peak']:.4f}")
            print(f"   æ•°æ®æ¥æº: {case['source']}")
    
    elif case_type == "high_quality":
        # æ˜¾ç¤ºRÂ²â‰¥0.95çš„case
        high_qual_mask = (df['r2_left'] >= 0.95) & (df['r2_right'] >= 0.95)
        high_qual_cases = df[high_qual_mask].sort_values('asymmetry')
        
        if len(high_qual_cases) > 0:
            print(f"\nğŸ“ é«˜è´¨é‡æ‹Ÿåˆæ¡ˆä¾‹ (RÂ²â‰¥0.95):")
            for i, (idx, case) in enumerate(high_qual_cases.iterrows()):
                print(f"\n   æ¡ˆä¾‹ #{i+1}:")
                
                # æ˜¾ç¤ºå¹‚å¾‹åˆ†å¸ƒçš„å®Œæ•´å‚æ•°
                gamma_pref = case.get('gamma_pref')
                k_min = case.get('k_min_pref')
                k_max = case.get('max_k')
                
                if gamma_pref is not None and k_min is not None and k_max is not None:
                    print(f"     å‚æ•°: Ï†={case['phi']:.3f}, Î¸={case['theta']:.3f}, Î³={gamma_pref:.1f}, k_min={k_min:.0f}, k_max={k_max:.0f}")
                else:
                    print(f"     å‚æ•°: Ï†={case['phi']:.3f}, Î¸={case['theta']:.3f}, Îº={case['kappa']:.0f} (åŸºç¡€å‚æ•°)")
                print(f"     å¯¹ç§°æ€§: {case['asymmetry']:.1%}")
                print(f"     RÂ²: å·¦={case['r2_left']:.3f}, å³={case['r2_right']:.3f}")
                print(f"     Î½: {case['nu_avg']:.3f}")
        else:
            print(f"\nğŸ“ æ— é«˜è´¨é‡æ‹Ÿåˆæ¡ˆä¾‹ (RÂ²â‰¥0.95)")



if __name__ == "__main__":
    
    # ğŸš€ æ™ºèƒ½åˆ†å¸ƒç±»å‹è¯†åˆ«
    # æ ¹æ®ç¼“å­˜ç›®å½•åç§°å»ºè®®åˆ†æç±»å‹
    cache_dir_lower = cache_dir.lower()
    suggested_filter = 'powerlaw'  # é»˜è®¤ä¸ºå¹‚å¾‹åˆ†å¸ƒ
    
    if 'powerlaw' in cache_dir_lower or 'power_law' in cache_dir_lower:
        suggested_filter = 'powerlaw'
        print(f"ğŸ” æ ¹æ®ç¼“å­˜ç›®å½•åç§°ï¼Œåˆ†æï¼šğŸ”´ å¹‚å¾‹åˆ†å¸ƒ")
    elif 'poisson' in cache_dir_lower:
        suggested_filter = 'poisson'
        print(f"ğŸ” æ ¹æ®ç¼“å­˜ç›®å½•åç§°ï¼Œåˆ‡æ¢åˆ†æï¼šğŸ”µ æ³Šæ¾åˆ†å¸ƒ")
    else:
        # é»˜è®¤æƒ…å†µä¸‹åˆ†æå¹‚å¾‹åˆ†å¸ƒ
        suggested_filter = 'powerlaw'
        print(f"ğŸ” é»˜è®¤åˆ†æç±»å‹ï¼šğŸ”´ å¹‚å¾‹åˆ†å¸ƒ (å¦‚éœ€æ³Šæ¾åˆ†å¸ƒï¼Œè¯·ä¿®æ”¹ç¼“å­˜ç›®å½•å)")
    
    print("=" * 60)
    
    # 1. æ”¶é›†æ‰€æœ‰ç»“æœ
    results_list = collect_all_scan_results(distribution_filter=suggested_filter)
    
    if len(results_list) == 0:
        if suggested_filter == 'powerlaw':
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¹‚å¾‹åˆ†å¸ƒæ‰«æç»“æœ")
            print(f"ğŸ’¡ æç¤ºï¼šç¡®ä¿ç¼“å­˜ç›®å½•ä¸­æœ‰ gamma_pref â‰  None çš„æ•°æ®æ–‡ä»¶")
        elif suggested_filter == 'poisson':
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ³Šæ¾åˆ†å¸ƒæ‰«æç»“æœ")
            print(f"ğŸ’¡ æç¤ºï¼šç¡®ä¿ç¼“å­˜ç›®å½•ä¸­æœ‰ gamma_pref = None çš„æ•°æ®æ–‡ä»¶")
        else:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ‰«æç»“æœ")
        exit()
    
    # 2. ç»Ÿè®¡åˆ†æ
    df = analyze_results_statistics(results_list)
    
    if len(df) == 0:
        print("âŒ è¿‡æ»¤åæ²¡æœ‰æœ‰æ•ˆçš„ç»“æœæ•°æ®")
        exit()
    
    # 3. æ‰¾å‡ºæœ€ä½³case (ç­›é€‰ï¼šRÂ²â‰¥0.85 + å…³è”é•¿åº¦â‰¥5.0)
    best_symmetry, high_quality = find_best_cases(df, top_n=5, min_xi_peak=5.0, min_r2=0.85)
    
    # 4. å±•ç¤ºè¯¦ç»†æ¡ˆä¾‹ä¿¡æ¯
    show_detailed_case_info(df, "best")
    show_detailed_case_info(df, "high_quality")
    
    # 5. å¯è§†åŒ–æ’åºé å‰çš„æ›´å¤šcase
    print("\n" + "="*80)
    print("ğŸ” è¯¦ç»†å¯è§†åŒ–åˆ†æ - å¤šä¸ªæ’åºé å‰çš„case")
    print("="*80)
    
    # æ–¹å¼1: æŒ‰å¯¹ç§°æ€§æ’åº (é¢„ç­›é€‰: RÂ²â‰¥0.85 + å•å³° + Î¾â‰¥5.0)
    visualize_top_cases(df, sort_by='symmetry', top_n=10, max_plots=8, 
                       min_xi_peak=5.0, min_r2=0.85, filter_multi_peak=True, min_peak_quality=0.5)
    
    # æ–¹å¼2: æŒ‰å…³è”é•¿åº¦å³°å€¼æ’åº (é¢„ç­›é€‰: RÂ²â‰¥0.85 + å•å³°)
    visualize_top_cases(df, sort_by='xi_peak', top_n=8, max_plots=8, 
                       min_r2=0.85, filter_multi_peak=True, min_peak_quality=0.5)
    

