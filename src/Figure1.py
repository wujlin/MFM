#!/usr/bin/env python3
"""
ä¸ºå¤šæ¬¡è®­ç»ƒç»“æœåˆ›å»ºå‘è¡¨çº§å¯è§†åŒ–å›¾è¡¨
æ”¯æŒä¸‰ä¸ªä»»åŠ¡ï¼šemotionã€mainstreamã€wemedia
è‡ªåŠ¨ä»ä¿å­˜çš„ç»“æœæ–‡ä»¶ä¸­è¯»å–æ•°æ®ï¼ŒåŒ…å«è®­ç»ƒæ›²çº¿åˆ†æ
"""
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.gridspec as gridspec
import numpy as np
import seaborn as sns
from scipy import stats
import json
from pathlib import Path
from typing import Dict, Any

# NatureæœŸåˆŠæ ·å¼è®¾ç½®
plt.rcParams.update({
    'font.family': 'Times New Roman',
    'font.size': 12,
    'axes.linewidth': 0.5,
    'axes.spines.top': False,
    'axes.spines.right': False,
    'axes.grid': True,
    'grid.alpha': 0.2,
    'grid.linewidth': 0.5,
    'legend.frameon': False,
    'legend.fontsize': 9,
    'figure.dpi': 400,
    'savefig.dpi': 400,
    'savefig.bbox': 'tight'
})

# Natureå­åˆŠé…è‰²æ–¹æ¡ˆ - ä¸ºä¸‰ä¸ªä»»åŠ¡è®¾è®¡
TASK_COLORS = {
    'emotion': {
        'primary': '#2E86AB',      # æ·±è“
        'secondary': '#A23B72',    # æ·±ç«çº¢
        'light': '#87CEEB',        # å¤©è“
        'name': 'Emotion'
    },
    'mainstream': {
        'primary': '#059669',      # æ·±ç»¿
        'secondary': '#10B981',    # ä¸­ç»¿
        'light': '#6EE7B7',        # æµ…ç»¿
        'name': 'Mainstream'
    },
    'wemedia': {
        'primary': '#DC2626',      # æ·±çº¢
        'secondary': '#F59E0B',    # æ©™é»„
        'light': '#FCA5A5',        # æµ…çº¢
        'name': 'We-media'
    }
}

# é€šç”¨é…è‰²
NATURE_COLORS = {
    'dark': '#343a40',         # æ·±ç°
    'grid': '#E2E8F0',         # æµ…ç°
    'background': '#FAFAFA'    # èƒŒæ™¯è‰²
}

def load_task_training_results(task_dirs):
    """ä»ä¸‰ä¸ªä»»åŠ¡ç›®å½•ä¸­åŠ è½½è®­ç»ƒç»“æœ"""
    
    print("ğŸ” åŠ è½½ä¸‰ä¸ªä»»åŠ¡çš„è®­ç»ƒç»“æœ...")
    
    all_results = {}
    
    for task_name, task_dir in task_dirs.items():
        print(f"\nğŸ“Š å¤„ç†ä»»åŠ¡: {task_name}")
        print(f"   ç›®å½•: {task_dir}")
        
        task_path = Path(task_dir)
        if not task_path.exists():
            print(f"   âŒ ç›®å½•ä¸å­˜åœ¨: {task_path}")
            continue
        
        # å°è¯•è¯»å–CSVæ–‡ä»¶
        csv_path = task_path / "analysis" / "multi_run_results.csv"
        if csv_path.exists():
            print(f"   âœ… æ‰¾åˆ°CSVç»“æœæ–‡ä»¶: {csv_path}")
            df = pd.read_csv(csv_path)
            print(f"   ğŸ“Š åŠ è½½äº† {len(df)} æ¬¡è®­ç»ƒç»“æœ")
            
            # åŠ è½½è®­ç»ƒæ›²çº¿
            training_curves = load_task_training_curves(task_path)
            
            all_results[task_name] = {
                'results_df': df,
                'training_curves': training_curves
            }
        else:
            # å°è¯•ä»JSONæŠ¥å‘Šä¸­æå–
            json_path = task_path / "analysis" / "comprehensive_report.json"
            if json_path.exists():
                print(f"   âœ… æ‰¾åˆ°JSONæŠ¥å‘Šæ–‡ä»¶: {json_path}")
                with open(json_path, 'r') as f:
                    report = json.load(f)
                
                if 'individual_results' in report:
                    df = pd.DataFrame(report['individual_results'])
                    print(f"   ğŸ“Š ä»JSONåŠ è½½äº† {len(df)} æ¬¡è®­ç»ƒç»“æœ")
                    
                    # åŠ è½½è®­ç»ƒæ›²çº¿
                    training_curves = load_task_training_curves(task_path)
                    
                    all_results[task_name] = {
                        'results_df': df,
                        'training_curves': training_curves
                    }
                else:
                    print(f"   âŒ JSONæ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®")
            else:
                print(f"   âŒ æœªæ‰¾åˆ°ç»“æœæ–‡ä»¶")
    
    return all_results

def load_task_training_curves(task_dir):
    """åŠ è½½å•ä¸ªä»»åŠ¡çš„è®­ç»ƒæ›²çº¿æ•°æ®"""
    
    print(f"   ğŸ“ˆ åŠ è½½è®­ç»ƒæ›²çº¿æ•°æ®...")
    training_curves = []
    
    for run_dir in sorted(task_dir.glob("run_*_seed_*")):
        run_info = {
            'run_id': int(run_dir.name.split('_')[1]),
            'seed': int(run_dir.name.split('_')[-1]),
            'training_data': None,
            'validation_data': None
        }
        
        # åŠ è½½è®­ç»ƒæ›²çº¿
        train_file = run_dir / "training_metrics.csv"
        val_file = run_dir / "validation_metrics.csv"
        
        if train_file.exists():
            try:
                train_df = pd.read_csv(train_file)
                run_info['training_data'] = train_df
            except Exception as e:
                print(f"     âŒ åŠ è½½è®­ç»ƒæ›²çº¿å¤±è´¥: {e}")
        
        if val_file.exists():
            try:
                val_df = pd.read_csv(val_file)
                run_info['validation_data'] = val_df
            except Exception as e:
                print(f"     âŒ åŠ è½½éªŒè¯æ›²çº¿å¤±è´¥: {e}")
        
        if run_info['training_data'] is not None or run_info['validation_data'] is not None:
            training_curves.append(run_info)
    
    print(f"     ğŸ“Š åŠ è½½äº† {len(training_curves)} ä¸ªè¿è¡Œçš„è®­ç»ƒæ›²çº¿")
    return training_curves

def aggregate_training_curves(training_curves):
    """èšåˆå¤šæ¬¡è¿è¡Œçš„è®­ç»ƒæ›²çº¿ï¼Œè®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®"""
    
    if not training_curves:
        return None, None
    
    # èšåˆè®­ç»ƒæŸå¤±æ›²çº¿
    train_aggregated = None
    if any(curve['training_data'] is not None for curve in training_curves):
        all_train_data = []
        
        for curve in training_curves:
            if curve['training_data'] is not None:
                df = curve['training_data'].copy()
                df['run_id'] = curve['run_id']
                all_train_data.append(df)
        
        if all_train_data:
            combined_train = pd.concat(all_train_data, ignore_index=True)
            
            # æŒ‰epochèšåˆ
            train_aggregated = combined_train.groupby('epoch').agg({
                'train_loss': ['mean', 'std', 'count'],
                'learning_rate': 'mean'
            }).reset_index()
            
            # å±•å¹³åˆ—å
            train_aggregated.columns = ['epoch', 'train_loss_mean', 'train_loss_std', 'train_loss_count', 'learning_rate']
            train_aggregated['train_loss_sem'] = train_aggregated['train_loss_std'] / np.sqrt(train_aggregated['train_loss_count'])
    
    # èšåˆéªŒè¯å‡†ç¡®ç‡æ›²çº¿
    val_aggregated = None
    if any(curve['validation_data'] is not None for curve in training_curves):
        all_val_data = []
        
        for curve in training_curves:
            if curve['validation_data'] is not None:
                df = curve['validation_data'].copy()
                df['run_id'] = curve['run_id']
                all_val_data.append(df)
        
        if all_val_data:
            combined_val = pd.concat(all_val_data, ignore_index=True)
            
            # æŒ‰epochèšåˆ
            val_aggregated = combined_val.groupby('epoch').agg({
                'eval_accuracy': ['mean', 'std', 'count'],
                'eval_loss': ['mean', 'std', 'count']
            }).reset_index()
            
            # å±•å¹³åˆ—å
            val_aggregated.columns = ['epoch', 'eval_accuracy_mean', 'eval_accuracy_std', 'eval_accuracy_count', 
                                    'eval_loss_mean', 'eval_loss_std', 'eval_loss_count']
            val_aggregated['eval_accuracy_sem'] = val_aggregated['eval_accuracy_std'] / np.sqrt(val_aggregated['eval_accuracy_count'])
            val_aggregated['eval_loss_sem'] = val_aggregated['eval_loss_std'] / np.sqrt(val_aggregated['eval_loss_count'])
    
    return train_aggregated, val_aggregated

def create_results_visualization(task_dirs=None):
    """åŸºäºä¸‰ä¸ªä»»åŠ¡çš„è®­ç»ƒç»“æœåˆ›å»ºå¯è§†åŒ–"""
    
    # é»˜è®¤ä»»åŠ¡ç›®å½•
    if task_dirs is None:
        task_dirs = {
            'emotion': 'multi_run_experiments_emotion',
            'mainstream': 'multi_run_experiments_mainstream', 
            'wemedia': 'multi_run_experiments_wemedia'
        }
    
    # åŠ è½½æ‰€æœ‰ä»»åŠ¡çš„æ•°æ®
    all_results = load_task_training_results(task_dirs)
    
    if not all_results:
        print("âŒ æ— æ³•åŠ è½½ä»»ä½•ä»»åŠ¡çš„è®­ç»ƒç»“æœ")
        return None, None
    
    print(f"\nâœ… æˆåŠŸåŠ è½½ {len(all_results)} ä¸ªä»»åŠ¡çš„æ•°æ®")
    
    # åˆ›å»º2x3å¸ƒå±€çš„å›¾è¡¨ï¼ˆæ¢å¤6ä¸ªå­å›¾ï¼‰
    fig = plt.figure(figsize=(18, 10))
    gs = fig.add_gridspec(2, 3, hspace=0.4, wspace=0.3)
    
    # A. æµ‹è¯•å‡†ç¡®ç‡æ¦‚ç‡å¯†åº¦åˆ†å¸ƒå›¾
    ax1 = fig.add_subplot(gs[0, 0])
    
    for task_name, task_data in all_results.items():
        if 'test_accuracy' in task_data['results_df'].columns:
            accuracy_data = task_data['results_df']['test_accuracy'] * 100
            
            # ä½¿ç”¨KDEç»˜åˆ¶æ¦‚ç‡å¯†åº¦æ›²çº¿
            kde = stats.gaussian_kde(accuracy_data)
            x_range = np.linspace(accuracy_data.min() - 2, accuracy_data.max() + 2, 100)
            density = kde(x_range)
            
            color_config = TASK_COLORS[task_name]
            ax1.fill_between(x_range, density, alpha=0.3, color=color_config['primary'], 
                           label=f"{color_config['name']}")
            ax1.plot(x_range, density, color=color_config['primary'], linewidth=2)
            
            # æ·»åŠ å‡å€¼çº¿
            mean_val = accuracy_data.mean()
            ax1.axvline(mean_val, color=color_config['primary'], linestyle='--', 
                       alpha=0.8, linewidth=1.5)
    
    ax1.set_xlabel('Test Accuracy (%)')
    ax1.set_ylabel('Probability Density')
    # ax1.set_title('a. Test Accuracy Probability Density Distribution', fontweight='bold')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # B. F1-Macroçš„ç®±çº¿å›¾+æ•£ç‚¹å›¾ç»„åˆ
    ax2 = fig.add_subplot(gs[0, 1])
    
    box_data_f1_macro = []
    box_labels = []
    colors = []
    
    for i, (task_name, task_data) in enumerate(all_results.items()):
        if 'test_f1_macro' in task_data['results_df'].columns:
            f1_macro_data = task_data['results_df']['test_f1_macro'] * 100
            box_data_f1_macro.append(f1_macro_data)
            box_labels.append(TASK_COLORS[task_name]['name'])
            colors.append(TASK_COLORS[task_name]['primary'])
    
    if box_data_f1_macro:
        # åˆ›å»ºç®±çº¿å›¾
        box_plot = ax2.boxplot(box_data_f1_macro, positions=range(len(box_data_f1_macro)), 
                              patch_artist=True, showfliers=False)
        
        # è®¾ç½®ç®±çº¿å›¾é¢œè‰²
        for patch, color in zip(box_plot['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        
        # æ·»åŠ æ•£ç‚¹å›¾
        for i, (data, color) in enumerate(zip(box_data_f1_macro, colors)):
            x_positions = np.random.normal(i, 0.04, len(data))
            ax2.scatter(x_positions, data, alpha=0.6, s=20, 
                       color=color, edgecolors='white', linewidth=0.5)
    
    ax2.set_ylabel('F1-Macro Score (%)')
    # ax2.set_title('b. F1-Macro Distribution & Variability Analysis', fontweight='bold')
    ax2.set_xticks(range(len(box_labels)))
    ax2.set_xticklabels(box_labels, rotation=45, ha='right')
    ax2.grid(True, alpha=0.3, axis='y')
    
    # C. F1-Weightedçš„ç®±çº¿å›¾+æ•£ç‚¹å›¾ç»„åˆ
    ax3 = fig.add_subplot(gs[0, 2])
    
    box_data_f1_weighted = []
    box_labels_weighted = []
    colors_weighted = []
    
    for i, (task_name, task_data) in enumerate(all_results.items()):
        if 'test_f1_weighted' in task_data['results_df'].columns:
            f1_weighted_data = task_data['results_df']['test_f1_weighted'] * 100
            box_data_f1_weighted.append(f1_weighted_data)
            box_labels_weighted.append(TASK_COLORS[task_name]['name'])
            colors_weighted.append(TASK_COLORS[task_name]['primary'])
    
    if box_data_f1_weighted:
        # åˆ›å»ºç®±çº¿å›¾
        box_plot = ax3.boxplot(box_data_f1_weighted, positions=range(len(box_data_f1_weighted)), 
                              patch_artist=True, showfliers=False)
        
        # è®¾ç½®ç®±çº¿å›¾é¢œè‰²
        for patch, color in zip(box_plot['boxes'], colors_weighted):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        
        # æ·»åŠ æ•£ç‚¹å›¾
        for i, (data, color) in enumerate(zip(box_data_f1_weighted, colors_weighted)):
            x_positions = np.random.normal(i, 0.04, len(data))
            ax3.scatter(x_positions, data, alpha=0.6, s=20, 
                       color=color, edgecolors='white', linewidth=0.5)
    
    ax3.set_ylabel('F1-Weighted Score (%)')
    # ax3.set_title('c. F1-Weighted Distribution & Variability Analysis', fontweight='bold')
    ax3.set_xticks(range(len(box_labels_weighted)))
    ax3.set_xticklabels(box_labels_weighted, rotation=45, ha='right')
    ax3.grid(True, alpha=0.3, axis='y')
    
    # D. Test Recallæ¦‚ç‡å¯†åº¦åˆ†å¸ƒå›¾
    ax4 = fig.add_subplot(gs[1, 0])
    
    for task_name, task_data in all_results.items():
        # ä»æ¯ä¸ªrunçš„evaluation_results.jsonä¸­æå–recallæ•°æ®
        recall_values = []
        
        # è·å–ä»»åŠ¡ç›®å½•è·¯å¾„
        task_dir = task_dirs[task_name]
        task_path = Path(task_dir)
        
        if task_path.exists():
            # éå†æ‰€æœ‰runç›®å½•
            for run_dir in sorted(task_path.glob("run_*_seed_*")):
                eval_file = run_dir / "evaluation_results.json"
                
                if eval_file.exists():
                    try:
                        with open(eval_file, 'r') as f:
                            eval_data = json.load(f)
                        
                        # æå–recallæ•°æ®ï¼Œä¼˜å…ˆçº§ï¼šmacro avg > weighted avg > æ•´ä½“accuracy
                        if 'classification_report' in eval_data:
                            report = eval_data['classification_report']
                            
                            if 'macro avg' in report and 'recall' in report['macro avg']:
                                recall_val = report['macro avg']['recall'] * 100
                                recall_values.append(recall_val)
                            elif 'weighted avg' in report and 'recall' in report['weighted avg']:
                                recall_val = report['weighted avg']['recall'] * 100
                                recall_values.append(recall_val)
                            elif 'accuracy' in report:
                                # å¯¹äºæŸäº›ä»»åŠ¡ï¼Œaccuracyå¯èƒ½ç­‰åŒäºrecall
                                recall_val = report['accuracy'] * 100
                                recall_values.append(recall_val)
                        
                        # å¤‡é€‰ï¼šä»test_metricsä¸­è·å–
                        elif 'test_metrics' in eval_data and 'eval_accuracy' in eval_data['test_metrics']:
                            recall_val = eval_data['test_metrics']['eval_accuracy'] * 100
                            recall_values.append(recall_val)
                            
                    except Exception as e:
                        print(f"     âš ï¸ è¯»å–{eval_file}å¤±è´¥: {e}")
                        continue
        
        # å¦‚æœæ”¶é›†åˆ°äº†recallæ•°æ®ï¼Œç»˜åˆ¶æ¦‚ç‡å¯†åº¦æ›²çº¿
        if len(recall_values) > 1:  # è‡³å°‘éœ€è¦2ä¸ªæ•°æ®ç‚¹æ‰èƒ½ç»˜åˆ¶KDE
            recall_data = np.array(recall_values)
            
            # ä½¿ç”¨KDEç»˜åˆ¶æ¦‚ç‡å¯†åº¦æ›²çº¿
            kde = stats.gaussian_kde(recall_data)
            x_range = np.linspace(recall_data.min() - 2, recall_data.max() + 2, 100)
            density = kde(x_range)
            
            color_config = TASK_COLORS[task_name]
            ax4.fill_between(x_range, density, alpha=0.3, color=color_config['primary'], 
                           label=f"{color_config['name']}")
            ax4.plot(x_range, density, color=color_config['primary'], linewidth=2)
            
            # æ·»åŠ å‡å€¼çº¿
            mean_recall = recall_data.mean()
            ax4.axvline(mean_recall, color=color_config['primary'], linestyle='--', 
                       alpha=0.8, linewidth=1.5)
            
            print(f"   âœ… {task_name}: æ‰¾åˆ° {len(recall_values)} ä¸ªrecallæ•°æ®ç‚¹ï¼Œå‡å€¼={mean_recall:.1f}%")
        
        elif len(recall_values) == 1:
            # åªæœ‰ä¸€ä¸ªæ•°æ®ç‚¹ï¼Œç»˜åˆ¶å‚ç›´çº¿
            color_config = TASK_COLORS[task_name]
            ax4.axvline(recall_values[0], color=color_config['primary'], 
                       linewidth=3, alpha=0.8, label=f"{color_config['name']}")
            print(f"   âš ï¸ {task_name}: åªæ‰¾åˆ° 1 ä¸ªrecallæ•°æ®ç‚¹: {recall_values[0]:.1f}%")
        
        else:
            print(f"   âŒ {task_name}: æ²¡æœ‰æ‰¾åˆ°recallæ•°æ®")
    
    ax4.set_xlabel('Test Recall (%)')
    ax4.set_ylabel('Probability Density')
    # ax4.set_title('d. Test Recall Probability Density Distribution', fontweight='bold')
    
    # åªæœ‰åœ¨æœ‰å›¾ä¾‹æ•°æ®æ—¶æ‰æ˜¾ç¤ºå›¾ä¾‹
    handles, labels = ax4.get_legend_handles_labels()
    if handles:
        ax4.legend()
    
    ax4.grid(True, alpha=0.3)
    
    # E. æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡å¯¹æ¯”ï¼ˆNatureé£æ ¼æŸ±çŠ¶å›¾ï¼Œç®€æ´ç‰ˆï¼‰
    ax5 = fig.add_subplot(gs[1, 1])
    
    final_val_accs = []
    final_val_acc_labels = []
    final_val_acc_colors = []
    
    for task_name, task_data in all_results.items():
        train_agg, val_agg = aggregate_training_curves(task_data['training_curves'])
        
        if val_agg is not None:
            # è·å–æœ€ç»ˆéªŒè¯å‡†ç¡®ç‡çš„å¹³å‡å€¼
            final_acc_mean = val_agg['eval_accuracy_mean'].iloc[-1] * 100
            
            final_val_accs.append(final_acc_mean)
            final_val_acc_labels.append(TASK_COLORS[task_name]['name'])
            final_val_acc_colors.append(TASK_COLORS[task_name]['primary'])
    
    if final_val_accs:
        # Natureé£æ ¼æŸ±çŠ¶å›¾è®¾è®¡ï¼ˆç®€æ´ç‰ˆï¼Œæ— è¯¯å·®æ£’ï¼‰
        x_pos = np.arange(len(final_val_accs))
        
        # åˆ›å»ºæŸ±çŠ¶å›¾
        bars = ax5.bar(x_pos, final_val_accs, 
                      color=final_val_acc_colors,
                      alpha=0.85,
                      edgecolor='white', 
                      linewidth=2,
                      width=0.6)
        
        # æ·»åŠ é¡¶éƒ¨æ•°å€¼æ ‡ç­¾ï¼ˆNatureé£æ ¼ï¼‰
        for i, (bar, acc) in enumerate(zip(bars, final_val_accs)):
            ax5.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,
                    f'{acc:.1f}%', 
                    ha='center', va='bottom', 
                    fontweight='bold', 
                    fontsize=10,
                    color=NATURE_COLORS['dark'])
            
            # æŸ±å­æ ·å¼
            bar.set_facecolor(final_val_acc_colors[i])
            bar.set_edgecolor('white')
        
        ax5.set_ylabel('Final Test Accuracy (%)', fontweight='600')
        # ax5.set_title('e. Final Performance Comparison', fontweight='bold')
        ax5.set_xticks(x_pos)
        ax5.set_xticklabels([label.replace(' ', '\n') for label in final_val_acc_labels], 
                           rotation=0, ha='center', fontweight='500')
        
        # ç§»é™¤ç½‘æ ¼çº¿
        ax5.grid(False)
        
        # è®¾ç½®yè½´èŒƒå›´ï¼Œçªå‡ºå·®å¼‚
        if len(final_val_accs) > 1:
            y_min = min(final_val_accs) - 2
            y_max = max(final_val_accs) + 4
            ax5.set_ylim(y_min, y_max)
        
        # ç§»é™¤é¡¶éƒ¨å’Œå³ä¾§çš„è¾¹æ¡†
        ax5.spines['top'].set_visible(False)
        ax5.spines['right'].set_visible(False)
    
    # F. è®­ç»ƒç¨³å®šæ€§åˆ†æï¼ˆNatureé£æ ¼æŸ±çŠ¶å›¾ï¼Œæ— ç½‘æ ¼ï¼‰
    ax6 = fig.add_subplot(gs[1, 2])
    
    stability_metrics = []
    stability_labels = []
    stability_colors = []
    
    for task_name, task_data in all_results.items():
        train_agg, val_agg = aggregate_training_curves(task_data['training_curves'])
        
        if val_agg is not None:
            # è®¡ç®—ååŠæœŸçš„ç¨³å®šæ€§ï¼ˆå˜å¼‚ç³»æ•°ï¼‰
            mid_point = len(val_agg) // 2
            late_period_std = val_agg['eval_accuracy_std'].iloc[mid_point:].mean() * 100
            late_period_mean = val_agg['eval_accuracy_mean'].iloc[mid_point:].mean() * 100
            
            if late_period_mean > 0:
                cv = late_period_std / late_period_mean * 100  # å˜å¼‚ç³»æ•°ç™¾åˆ†æ¯”
                stability_metrics.append(cv)
                stability_labels.append(TASK_COLORS[task_name]['name'])
                stability_colors.append(TASK_COLORS[task_name]['primary'])
    
    if stability_metrics:
        # Natureé£æ ¼æŸ±çŠ¶å›¾è®¾è®¡
        x_pos = np.arange(len(stability_metrics))
        
        bars = ax6.bar(x_pos, stability_metrics,
                      color=stability_colors,
                      alpha=0.85,
                      edgecolor='white',
                      linewidth=2,
                      width=0.6)
        
        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (bar, cv) in enumerate(zip(bars, stability_metrics)):
            ax6.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05,
                    f'{cv:.2f}%', 
                    ha='center', va='bottom', 
                    fontweight='bold',
                    fontsize=10,
                    color=NATURE_COLORS['dark'])
        
        ax6.set_ylabel('Coefficient of Variation (%)', fontweight='600')
        # ax6.set_title('f. Training Stability Analysis\n(Lower = More Stable)', fontweight='bold')
        ax6.set_xticks(x_pos)
        ax6.set_xticklabels([label.replace(' ', '\n') for label in stability_labels], 
                           rotation=0, ha='center', fontweight='500')
        
        # ç§»é™¤ç½‘æ ¼çº¿
        ax6.grid(False)
        
        # ç§»é™¤é¡¶éƒ¨å’Œå³ä¾§çš„è¾¹æ¡†
        ax6.spines['top'].set_visible(False)
        ax6.spines['right'].set_visible(False)
        
        # # æ·»åŠ ç¨³å®šæ€§ç­‰çº§æ ‡æ³¨ï¼ˆNatureé£æ ¼ï¼‰
        # max_cv = max(stability_metrics)
        # if max_cv < 1:
        #     stability_note = "Excellent Stability"
        #     note_color = '#2ca02c'  # ç»¿è‰²
        # elif max_cv < 2:
        #     stability_note = "Good Stability"
        #     note_color = '#ff7f0e'  # æ©™è‰²
        # else:
        #     stability_note = "Moderate Stability"
        #     note_color = '#d62728'  # çº¢è‰²
        
        # ax6.text(0.98, 0.95, stability_note, 
        #         transform=ax6.transAxes,
        #         ha='right', va='top', 
        #         fontsize=9, 
        #         fontweight='600',
        #         color=note_color,
        #         bbox=dict(boxstyle='round,pad=0.4', 
        #                  facecolor='white', 
        #                  edgecolor=note_color,
        #                  alpha=0.9,
        #                  linewidth=1.5))
    
    # å»æ‰å¤§æ ‡é¢˜
    # plt.suptitle('Multi-Task Training Analysis: Comprehensive Performance Evaluation\n' + 
    #              f'(Independent Runs Across {len(all_results)} Tasks)', 
    #              fontsize=16, fontweight='bold', y=0.98)
    
    # ä¿å­˜å›¾è¡¨
    output_dir = Path("multi_task_analysis")
    output_dir.mkdir(exist_ok=True)
    
    fig_path = output_dir / 'multi_task_performance_analysis.png'
    plt.savefig(fig_path, dpi=300, bbox_inches='tight')
    plt.savefig(fig_path.with_suffix('.pdf'), dpi=300, bbox_inches='tight')
    
    print(f"\nâœ… å¤šä»»åŠ¡æ€§èƒ½åˆ†æå›¾å·²ä¿å­˜: {fig_path}")
    plt.show()
    
    # æ‰“å°è¯¦ç»†ç»Ÿè®¡æ€»ç»“
    print("\n" + "="*80)
    print("ğŸ“Š MULTI-TASK STATISTICAL SUMMARY FOR MANUSCRIPT")
    print("="*80)
    
    for task_name, task_data in all_results.items():
        df = task_data['results_df']
        color_config = TASK_COLORS[task_name]
        
        print(f"\nğŸ¯ {color_config['name']}:")
        print("-" * 40)
        
        # è®¡ç®—è¯¦ç»†ç»Ÿè®¡
        for metric in ['test_accuracy', 'test_f1_macro', 'test_f1_weighted']:
            if metric in df.columns:
                values = df[metric].values * 100
                mean_val = np.mean(values)
                std_val = np.std(values, ddof=1) if len(values) > 1 else 0
                median_val = np.median(values)
                min_val = np.min(values)
                max_val = np.max(values)
                
                metric_name = metric.replace('test_', '').replace('_', '-').title()
                print(f"  {metric_name}: {mean_val:.2f}% Â± {std_val:.2f}% [{min_val:.2f}%-{max_val:.2f}%]")
        
        # æ·»åŠ è®­ç»ƒæŸå¤±ç»Ÿè®¡
        train_agg, _ = aggregate_training_curves(task_data['training_curves'])
        if train_agg is not None:
            final_losses = []
            for curve in task_data['training_curves']:
                if curve['training_data'] is not None:
                    final_loss = curve['training_data']['train_loss'].iloc[-1]
                    final_losses.append(final_loss)
            
            if final_losses:
                final_losses = np.array(final_losses)
                mean_loss = np.mean(final_losses)
                std_loss = np.std(final_losses, ddof=1) if len(final_losses) > 1 else 0
                print(f"  Final-Training-Loss: {mean_loss:.4f} Â± {std_loss:.4f}")
        
        print(f"  Sample size: n = {len(df)} runs")
    
    print("="*80)
    
    return all_results

if __name__ == "__main__":
    # å¯ä»¥æ‰‹åŠ¨æŒ‡å®šä»»åŠ¡ç›®å½•
    task_directories = {
        'emotion': 'multi_run_experiments_emotion',
        'mainstream': 'multi_run_experiments_mainstream', 
        'wemedia': 'multi_run_experiments_wemedia'
    }
    
    results = create_results_visualization(task_directories) 